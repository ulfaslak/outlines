{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"installation/","title":"Installation","text":"<p>You can install Outlines with <code>pip</code>:</p> <pre><code>pip install outlines\n</code></pre> <p>Outlines supports OpenAI, transformers, Mamba, llama.cpp and exllama2 but you will need to install them manually:</p> <pre><code>pip install openai\npip install transformers datasets accelerate torch\npip install llama-cpp-python\npip install exllamav2 transformers torch\npip install mamba_ssm transformers torch\npip install vllm\n</code></pre> <p>If you encounter any problem using Outlines with these libraries, take a look at their installation instructions. The installation of <code>openai</code> and <code>transformers</code> should be straightforward, but other libraries have specific hardware requirements.</p>"},{"location":"installation/#bleeding-edge","title":"Bleeding edge","text":"<p>You can install the latest version of Outlines on the repository's <code>main</code> branch:</p> <pre><code>pip install git+https://github.com/dottxt-ai/outlines.git@main\n</code></pre> <p>This can be useful, for instance, when a fix has been merged but not yet released.</p>"},{"location":"installation/#installing-for-development","title":"Installing for development","text":"<p>See the contributing documentation for instructions on how to install Outlines for development.</p>"},{"location":"licence/","title":"Licence and citations","text":"<p>Outlines is licenced under the Apache 2.0 licence. To comply with the licence you need to add the following notice at the top every file that uses part of Outlines' code:</p> <pre><code>Copyright 2023- The Outlines developers\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre> <p>If you use Outlines in your work you can use the following citation:</p> <pre><code>@article{willard2023efficient,\n  title={Efficient Guided Generation for LLMs},\n  author={Willard, Brandon T and Louf, R{\\'e}mi},\n  journal={arXiv preprint arXiv:2307.09702},\n  year={2023}\n}\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>After installing Outlines, the fastest way to get to up to speed with the library is to get acquainted with its few core elements. We advise you to take a quick look at this page to see everything Outlines has to offer before diving in the documentation.</p>"},{"location":"quickstart/#core-elements","title":"Core elements","text":""},{"location":"quickstart/#models","title":"Models","text":"<p>The first step when writing a program with Outlines is to initialize a model. Weights will be loaded on the device at this step:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    device=\"cuda\"  # optional device argument, default is cpu\n)\n</code></pre> <p>Outlines supports a wide variety of inference engines and model weight types. More details on different models can be found in the Outlines Models documentation page.</p>"},{"location":"quickstart/#generation","title":"Generation","text":"<p>Once the model is initialized you can build an <code>outlines.generate</code> generator. This generator can be called with a prompt directly.</p> <p>(Outlines Structured Generation Full Documentation)</p> TextStructured <pre><code>generator = outlines.generate.text(model)\n\nresult = generator(\"Question: What's 2+2? Answer:\", max_tokens=100)\nprint(result)\n# The answer is 4\n\n# Outlines also supports streaming output\nstream = generator.stream(\"What's 2+2?\", max_tokens=4)\nfor i in range(5):\n    token = next(stream)\n    print(repr(token))\n# '2'\n# '+'\n# '2'\n# ' equals'\n# '4'\n</code></pre> <p>Along with typical language model generation behavior via, <code>outlines.generate.text</code>, Outlines supports structured generation, which guarantees the tokens generated by the model will follow a predefined structure. Structures can be defined by a regex pattern, JSON schema, python object type, or a Lark grammar defining a parsable language such as SQL or Python.</p> <p>Example: using pydantic to enforce a JSON schema</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr, conint\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: conint(gt=18, lt=99)\n    armor: (Enum('Armor', {'leather': 'leather', 'chainmail': 'chainmail', 'plate': 'plate'}))\n    strength: conint(gt=1, lt=100)\n\ngenerator = outlines.generate.json(model, Character)\n\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# Character(name='Zara', age=25, armor=&lt;Armor.leather: 'leather'&gt;, strength=85)\n</code></pre>"},{"location":"quickstart/#deploy-using-vllm-and-fastapi","title":"Deploy using vLLM and FastAPI","text":"<p>Outlines can be deployed as a LLM service using vLLM and FastAPI. The server supports asynchronous processing of incoming requests, and benefits from the performance of vLLM.</p> <p>First start the server:</p> <pre><code>python -m outlines.serve.serve --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre> <p>Or you can start the server with Outlines' official Docker image:</p> <pre><code>docker run -p 8000:8000 outlinesdev/outlines --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre> <p>This will by default start a server at <code>http://127.0.0.1:8000</code> (check what the console says, though). Without the <code>--model</code> argument set, the OPT-125M model is used.</p> <p>You can then query the model in shell by passing a prompt and a JSON Schema specification for the structure of the output:</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"Question: What is a language model? Answer:\",\n        \"schema\": {\"type\": \"string\"}\n        }'\n</code></pre> <p>Or use the requests library from another python program. You can read the vLLM documentation for more details.</p>"},{"location":"quickstart/#utilities","title":"Utilities","text":""},{"location":"quickstart/#prompt-templates","title":"Prompt templates","text":"<p>Prompting can lead to messy code. Outlines' prompt functions are python functions that contain a template for the prompt in their docstring. We use a powerful templating language to allow you to loop over lists, dictionaries, add conditionals, etc. directly from the prompt. When called, a prompt function returns the rendered template:</p> <pre><code>import outlines\n\n@outlines.prompt\ndef few_shots(instructions, examples, question):\n    \"\"\"{{ instructions }}\n\n    Examples\n    --------\n\n    {% for example in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n\n    {% endfor %}\n    Question\n    --------\n\n    Q: {{ question }}\n    A:\n    \"\"\"\n\ninstructions = \"Please answer the following question following the examples\"\nexamples = [\n    {\"question\": \"2+2=?\", \"answer\":4},\n    {\"question\": \"3+3=?\", \"answer\":6}\n]\nquestion = \"4+4 = ?\"\n\nprompt = few_shots(instructions, examples, question)\nprint(prompt)\n# Please answer the following question following the examples\n\n# Examples\n# --------\n\n# Q: 2+2=?\n# A: 4\n\n# Q: 3+3=?\n# A: 6\n\n# Question\n# --------\n\n# Q: 4+4 = ?\n# A:\n</code></pre>"},{"location":"quickstart/#outlines-functions","title":"Outlines functions","text":"<p>Once you are done experimenting with a prompt and an output structure, it is useful to be able to encapsulate all of these in a single function that can be called from other parts of the program. This is what <code>outlines.Function</code> allows you to do:</p> function.pyCall a functionCall a function stored on GitHub <pre><code>from pydantic import BaseModel\n\nimport outlines\n\n\n@outlines.prompt\ndef tell_a_joke(topic):\n    \"\"\"Tell me a joke about {{ topic }}.\"\"\"\n\nclass Joke(BaseModel):\n    setup: str\n    punchline: str\n\ngenerate_joke = outlines.Function(\n    tell_a_joke,\n    Joke,\n    \"microsoft/Phi-3-mini-4k-instruct\"\n)\n</code></pre> <pre><code>from .function import generate_joke\n\nresponse = generate_joke(\"baseball\")\n\n# haha\n# Joke(setup='Why was the baseball in a bad mood?', punchline='Because it got hit around a lot.')\n</code></pre> <p>You can load a function that is stored on a repository on GitHub directly from Outlines. Say <code>Someone</code> stores a function in <code>joke.py</code> at the root of the <code>TheirRepo</code> repository:</p> <p><pre><code>import outlines\n\njoke = outlines.Function.from_github(\"Someone/TheirRepo/joke\")\nresponse = joke(\"baseball\")\n</code></pre> It make it easier for the community to collaborate on the infinite number of use cases enabled by these models!</p>"},{"location":"quickstart/#going-further","title":"Going further","text":"<p>If you need more inspiration you can take a look at the cookbook or watch Remi Louf's AI Engineer World\u2019s Fair Presentation on Outlines. If you have any question, or requests for documentation please reach out to us on GitHub, Twitter or Discord.</p>"},{"location":"welcome/","title":"Welcome to Outlines!","text":"<p>Outlines is a Python library that allows you to use Large Language Model in a simple and robust way (with structured generation). It is built by .txt, and is already used in production by many companies.</p>"},{"location":"welcome/#what-models-do-you-support","title":"What models do you support?","text":"<p>We support Openai, but the true power of Outlines is unleashed with Open Source models available via the transformers, llama.cpp, exllama2, mlx-lm and vllm models. If you want to build and maintain an integration with another library, get in touch.</p>"},{"location":"welcome/#what-are-the-main-features","title":"What are the main features?","text":"<ul> <li> <p> Make LLMs generate valid JSON</p> <p>No more invalid JSON outputs, 100% guaranteed</p> <p> Generate JSON</p> </li> <li> <p> JSON mode for vLLM</p> <p>Deploy a LLM service using Outlines' JSON structured generation and vLLM</p> <p> Deploy outlines</p> </li> <li> <p> Make LLMs follow a Regex</p> <p>Generate text that parses correctly 100% of the time</p> <p> Guide LLMs</p> </li> <li> <p> Powerful Prompt Templating</p> <p>Better manage your prompts' complexity with prompt templating</p> <p> Learn more</p> </li> </ul>"},{"location":"welcome/#why-use-outlines","title":"Why use Outlines?","text":"<p>Outlines is built at .txt by engineers with decades of experience in software engineering, machine learning (Bayesian Statistics and NLP), and compilers. .txt is a VC-backed company fully focused on the topic of structured generation and is committed to make the community benefit from its experience.</p> <p>We are also open source veterans and have authored/maintained many libraries over the years: the Aesara and Pythological ecosystems, Blackjax and Hy among many others. .</p> <p>Outlines does not use unnecessary abstractions that tend to get in your way. We have a laser focus on reliable text generation with LLMs, a clear roadmap to push the state of the art in this area and a commitment to clean and robust code.</p> <p>And last but not least, unlike alternatives, Outlines' structured generation introduces no overhead during inference.</p>"},{"location":"welcome/#who-is-using-outlines","title":"Who is using Outlines?","text":"<p>Hundreds of organisations and the main LLM serving frameworks (vLLM, TGI, LoRAX, xinference, SGLang) are using Outlines. Some of the prominent companies and organizations that are using Outlines include:</p> <p> <p></p> <p>Organizations are included either because they use Outlines as a dependency in a public repository, or because of direct communication between members of the Outlines team and employees at these organizations.</p> <p>Still not convinced, read what people say about us. And make sure to take a look at what the community is building!</p>"},{"location":"welcome/#philosophy","title":"Philosophy","text":"<p>Outlines  is a library for neural text generation. You can think of it as a more flexible replacement for the <code>generate</code> method in the transformers library.</p> <p>Outlines  helps developers structure text generation to build robust interfaces with external systems. It provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.</p> <p>Outlines  provides robust prompting primitives that separate the prompting from the execution logic and lead to simple implementations of few-shot generations, ReAct, meta-prompting, agents, etc.</p> <p>Outlines  is designed as a library that is meant to be compatible the broader ecosystem, not to replace it. We use as few abstractions as possible, and generation can be interleaved with control flow, conditionals, custom Python functions and calls to other libraries.</p> <p>Outlines  is compatible with every auto-regressive model. It only interfaces with models via the next-token logits distribution.</p>"},{"location":"welcome/#outlines-people","title":"Outlines people","text":"<p>Outlines would not be what it is today without a community of dedicated developers:</p> <p> </p>"},{"location":"welcome/#acknowledgements","title":"Acknowledgements","text":"<p>Outlines was originally developed at @NormalComputing by @remilouf and @BrandonTWillard. It is now maintained by .txt.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/guide/","title":"Guide","text":""},{"location":"api/guide/#outlines.fsm.guide.CFGGuide","title":"<code>CFGGuide</code>","text":"<p>               Bases: <code>Guide</code></p> <p>Guide to generate text that is in the language of a context-free Lark grammar.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class CFGGuide(Guide):\n    \"\"\"Guide to generate text that is in the language of a context-free Lark grammar.\"\"\"\n\n    def __init__(self, cfg_string: str, tokenizer):\n        \"\"\"\n        Construct the PartialLark parser and set the empty initial_state (PartialParserState)\n        \"\"\"\n        warnings.warn(\n            \"Outlines' public *community-contributed* CFG structured generation is experimental. \"\n            \"Please review https://dottxt-ai.github.io/outlines/reference/cfg#disclaimer\"\n        )\n\n        self.cfg_string = cfg_string\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.parser = PartialLark(\n            cfg_string,\n            parser=\"lalr\",\n            import_paths=[grammars.GRAMMAR_PATH],\n        )\n        self.initial_state = CFGState(\n            parser_state=self.parser.parse(\"\"), prev_token=None\n        )\n\n    def get_next_instruction(self, state: CFGState) -&gt; Instruction:\n        \"\"\"Return the next instruction for guided generation.\n\n        Current lazy approach:\n        - For each token in the vocabulary\n          - create a copy of the parsers state\n          - add the tokens to the parsers input text\n          - if valid, add token to returned tokens\n\n        Further refinements are necessary for performant text processing.\n\n        Parameters\n        ----------\n        state\n            The guides current PartialParserState, or None if complete\n\n        Returns\n        -------\n        A `Generate` instance that contains the model and the allowed token ids.\n\n        \"\"\"\n\n        if state.parser_state is None:\n            return Write(torch.tensor([self.eos_token_id]))\n\n        valid_tokens = list(\n            self.iter_valid_token_ids(state, self.tokenizer.vocabulary.values())\n        )\n        if len(valid_tokens) == 1:\n            return Write(torch.tensor(valid_tokens))\n        return Generate(torch.tensor(valid_tokens))\n\n    def iter_valid_token_ids(\n        self, state: CFGState, candidate_token_ids: list\n    ) -&gt; Generator[int, None, None]:\n        \"\"\"\n        Iterate over the given token_ids and yield those that are valid for the current parser state.\n\n        Parameters\n        ----------\n        parser_state\n            The current state of the parser, or None if complete.\n        token_ids\n            The list of token ids to check for validity.\n\n        Yields\n        ------\n        int\n            Valid token ids.\n        \"\"\"\n        if state.parser_state is None:\n            yield self.eos_token_id\n            return\n\n        for token_id in candidate_token_ids:\n            if token_id == self.eos_token_id:\n                if self.can_terminate_state(state):\n                    yield token_id\n            else:\n                try:\n                    self._get_parser_state_token_applied(state, int(token_id))\n                    yield token_id\n                except (\n                    ValueError,\n                    EOFError,\n                    UnexpectedToken,\n                    UnexpectedCharacters,\n                    DedentError,\n                ):\n                    pass\n\n    def get_next_state(self, state: CFGState, token_id: int) -&gt; CFGState:\n        \"\"\"\n        Update the state of the guide.\n        Decode the token_id, and calculate the new parser_state with the token applied.\n\n        Parameters\n        ----------\n        state\n            The guides current PartialParserState, or None if complete\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The guides new PartialParserState\n\n        \"\"\"\n        if state.parser_state is None or token_id == self.eos_token_id:\n            parser_state = None\n        else:\n            parser_state = self._get_parser_state_token_applied(state, int(token_id))\n        return CFGState(parser_state=parser_state, prev_token=token_id)\n\n    def _get_parser_state_token_applied(\n        self, state: CFGState, token_id: int\n    ) -&gt; PartialParserState:\n        \"\"\"\n        Don't mutate `parser_state`, copy to protect\n\n        Get the token string\n          - if first token in generation: tokenizer.decode (no leading whitespace)\n          - else: normalized (with possibly leading whitespace)\n\n        Don't allow empty (\"\") tokens, raise ValueError\n        \"\"\"\n        parser_state = copy.copy(state.parser_state)  # prevent side effects\n\n        # normalize\n        if state.prev_token is None:\n            new_token_str = self.tokenizer.decode([token_id])[0]\n        else:\n            prev_token_str = self.tokenizer.decode([[state.prev_token]])[0]\n            combined_token_str = self.tokenizer.decode([[state.prev_token, token_id]])[\n                0\n            ]\n            new_token_str = combined_token_str[len(prev_token_str) :]\n\n        if new_token_str == \"\":\n            raise ValueError(\"empty next token\")\n\n        # update parser with new token\n        parser_state.lexer.state.text += new_token_str\n        self.parser.parse_from_state(parser_state, is_end=False)\n\n        return parser_state\n\n    def is_final_state(self, state: CFGState) -&gt; bool:\n        # TODO: remove this method, use can_terminate_state and must_terminate_state\n        # here and in RegexGuide per https://github.com/dottxt-ai/outlines/issues/885\n        return self.can_terminate_state(state)\n\n    def can_terminate_state(self, state: CFGState) -&gt; bool:\n        \"\"\"Generation is allowed to terminate\"\"\"\n        if state.parser_state is not None:\n            try:\n                copy.copy(state.parser_state).feed_eof()\n            except UnexpectedToken:\n                return False\n        return True\n\n    def must_terminate_state(self, state: CFGState) -&gt; bool:\n        \"\"\"Generation must terminate, no legal continuations\"\"\"\n        return state.parser_state is None or set(state.parser_state.accepts()).issubset(\n            {\"$END\"}\n        )\n\n    def copy(self) -&gt; \"CFGGuide\":\n        \"\"\"Create a copy of the Guide.\"\"\"\n        return CFGGuide(self.cfg_string, self.tokenizer)\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.__init__","title":"<code>__init__(cfg_string, tokenizer)</code>","text":"<p>Construct the PartialLark parser and set the empty initial_state (PartialParserState)</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def __init__(self, cfg_string: str, tokenizer):\n    \"\"\"\n    Construct the PartialLark parser and set the empty initial_state (PartialParserState)\n    \"\"\"\n    warnings.warn(\n        \"Outlines' public *community-contributed* CFG structured generation is experimental. \"\n        \"Please review https://dottxt-ai.github.io/outlines/reference/cfg#disclaimer\"\n    )\n\n    self.cfg_string = cfg_string\n    self.tokenizer = tokenizer\n    self.eos_token_id = self.tokenizer.eos_token_id\n    self.parser = PartialLark(\n        cfg_string,\n        parser=\"lalr\",\n        import_paths=[grammars.GRAMMAR_PATH],\n    )\n    self.initial_state = CFGState(\n        parser_state=self.parser.parse(\"\"), prev_token=None\n    )\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.can_terminate_state","title":"<code>can_terminate_state(state)</code>","text":"<p>Generation is allowed to terminate</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def can_terminate_state(self, state: CFGState) -&gt; bool:\n    \"\"\"Generation is allowed to terminate\"\"\"\n    if state.parser_state is not None:\n        try:\n            copy.copy(state.parser_state).feed_eof()\n        except UnexpectedToken:\n            return False\n    return True\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.copy","title":"<code>copy()</code>","text":"<p>Create a copy of the Guide.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def copy(self) -&gt; \"CFGGuide\":\n    \"\"\"Create a copy of the Guide.\"\"\"\n    return CFGGuide(self.cfg_string, self.tokenizer)\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_instruction","title":"<code>get_next_instruction(state)</code>","text":"<p>Return the next instruction for guided generation.</p> <p>Current lazy approach: - For each token in the vocabulary   - create a copy of the parsers state   - add the tokens to the parsers input text   - if valid, add token to returned tokens</p> <p>Further refinements are necessary for performant text processing.</p>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_instruction--parameters","title":"Parameters","text":"<p>state     The guides current PartialParserState, or None if complete</p>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_instruction--returns","title":"Returns","text":"<p>A <code>Generate</code> instance that contains the model and the allowed token ids.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def get_next_instruction(self, state: CFGState) -&gt; Instruction:\n    \"\"\"Return the next instruction for guided generation.\n\n    Current lazy approach:\n    - For each token in the vocabulary\n      - create a copy of the parsers state\n      - add the tokens to the parsers input text\n      - if valid, add token to returned tokens\n\n    Further refinements are necessary for performant text processing.\n\n    Parameters\n    ----------\n    state\n        The guides current PartialParserState, or None if complete\n\n    Returns\n    -------\n    A `Generate` instance that contains the model and the allowed token ids.\n\n    \"\"\"\n\n    if state.parser_state is None:\n        return Write(torch.tensor([self.eos_token_id]))\n\n    valid_tokens = list(\n        self.iter_valid_token_ids(state, self.tokenizer.vocabulary.values())\n    )\n    if len(valid_tokens) == 1:\n        return Write(torch.tensor(valid_tokens))\n    return Generate(torch.tensor(valid_tokens))\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_state","title":"<code>get_next_state(state, token_id)</code>","text":"<p>Update the state of the guide. Decode the token_id, and calculate the new parser_state with the token applied.</p>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_state--parameters","title":"Parameters","text":"<p>state     The guides current PartialParserState, or None if complete token_id     The id of the token that was just generated.</p>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.get_next_state--returns","title":"Returns","text":"<p>The guides new PartialParserState</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def get_next_state(self, state: CFGState, token_id: int) -&gt; CFGState:\n    \"\"\"\n    Update the state of the guide.\n    Decode the token_id, and calculate the new parser_state with the token applied.\n\n    Parameters\n    ----------\n    state\n        The guides current PartialParserState, or None if complete\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The guides new PartialParserState\n\n    \"\"\"\n    if state.parser_state is None or token_id == self.eos_token_id:\n        parser_state = None\n    else:\n        parser_state = self._get_parser_state_token_applied(state, int(token_id))\n    return CFGState(parser_state=parser_state, prev_token=token_id)\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.iter_valid_token_ids","title":"<code>iter_valid_token_ids(state, candidate_token_ids)</code>","text":"<p>Iterate over the given token_ids and yield those that are valid for the current parser state.</p>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.iter_valid_token_ids--parameters","title":"Parameters","text":"<p>parser_state     The current state of the parser, or None if complete. token_ids     The list of token ids to check for validity.</p>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.iter_valid_token_ids--yields","title":"Yields","text":"<p>int     Valid token ids.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def iter_valid_token_ids(\n    self, state: CFGState, candidate_token_ids: list\n) -&gt; Generator[int, None, None]:\n    \"\"\"\n    Iterate over the given token_ids and yield those that are valid for the current parser state.\n\n    Parameters\n    ----------\n    parser_state\n        The current state of the parser, or None if complete.\n    token_ids\n        The list of token ids to check for validity.\n\n    Yields\n    ------\n    int\n        Valid token ids.\n    \"\"\"\n    if state.parser_state is None:\n        yield self.eos_token_id\n        return\n\n    for token_id in candidate_token_ids:\n        if token_id == self.eos_token_id:\n            if self.can_terminate_state(state):\n                yield token_id\n        else:\n            try:\n                self._get_parser_state_token_applied(state, int(token_id))\n                yield token_id\n            except (\n                ValueError,\n                EOFError,\n                UnexpectedToken,\n                UnexpectedCharacters,\n                DedentError,\n            ):\n                pass\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.CFGGuide.must_terminate_state","title":"<code>must_terminate_state(state)</code>","text":"<p>Generation must terminate, no legal continuations</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def must_terminate_state(self, state: CFGState) -&gt; bool:\n    \"\"\"Generation must terminate, no legal continuations\"\"\"\n    return state.parser_state is None or set(state.parser_state.accepts()).issubset(\n        {\"$END\"}\n    )\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.Guide","title":"<code>Guide</code>","text":"<p>               Bases: <code>Guide</code></p> <p>Base definition of a generation guide.</p> <p>A generation guide defines the behavior of a finite-state machine that guides a text generation procedure. Unlike the DFAs built from regular expressions guides can also emit a <code>Write</code> instructions which tells the model that it can append a sequence of tokens (or token word) instead of generating it.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class Guide(CoreGuide):\n    \"\"\"Base definition of a generation guide.\n\n    A generation guide defines the behavior of a finite-state machine that guides\n    a text generation procedure. Unlike the DFAs built from regular expressions\n    guides can also emit a `Write` instructions which tells the model that it can\n    append a sequence of tokens (or token word) instead of generating it.\n\n    \"\"\"\n\n    initial_state: Any\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.RegexGuide","title":"<code>RegexGuide</code>","text":"<p>               Bases: <code>RegexGuide</code></p> <p>Guide to generate text in the language of a regular expression. CoreRegexGuide with outlines cache</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class RegexGuide(CoreRegexGuide):\n    \"\"\"\n    Guide to generate text in the language of a regular expression.\n    CoreRegexGuide with outlines cache\n    \"\"\"\n\n    def __init__(self, regex_string: str, tokenizer: \"Tokenizer\"):\n        (\n            self.states_to_token_maps,\n            self.empty_token_ids,\n            fsm_finals,\n        ) = create_states_mapping(regex_string, tokenizer)\n        self.eos_token_id = tokenizer.eos_token_id\n        self.final_states = fsm_finals | {-1}\n        self._cache_state_to_token_tensor()\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.StopAtEOSGuide","title":"<code>StopAtEOSGuide</code>","text":"<p>               Bases: <code>Guide</code></p> <p>Guide to generate tokens until the EOS token has been generated.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>class StopAtEOSGuide(Guide):\n    \"\"\"Guide to generate tokens until the EOS token has been generated.\"\"\"\n\n    final_state = 1\n    start_state = 0  # TODO: remove start_state, use only initial_state\n    initial_state = 0\n\n    def __init__(self, tokenizer: \"Tokenizer\"):\n        \"\"\"Initialize the generation guide.\n\n        model\n            The logit generator used to generate the next token.\n\n        \"\"\"\n        self.eos_token_id = tokenizer.eos_token_id\n        self.vocabulary = tokenizer.vocabulary.values()\n\n    def get_next_instruction(self, state: int) -&gt; Instruction:\n        if self.is_final_state(state):\n            return Write([self.eos_token_id])\n        return Generate(None)\n\n    def get_next_state(self, state: int, token_id: int) -&gt; int:\n        if token_id == self.eos_token_id or state == self.final_state:\n            return self.final_state\n\n        return self.initial_state\n\n    def is_final_state(self, state: int):\n        return state == self.final_state\n\n    def copy(self):\n        return self\n</code></pre>"},{"location":"api/guide/#outlines.fsm.guide.StopAtEOSGuide.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize the generation guide.</p> <p>model     The logit generator used to generate the next token.</p> Source code in <code>outlines/fsm/guide.py</code> <pre><code>def __init__(self, tokenizer: \"Tokenizer\"):\n    \"\"\"Initialize the generation guide.\n\n    model\n        The logit generator used to generate the next token.\n\n    \"\"\"\n    self.eos_token_id = tokenizer.eos_token_id\n    self.vocabulary = tokenizer.vocabulary.values()\n</code></pre>"},{"location":"api/json_schema/","title":"Json schema","text":""},{"location":"api/json_schema/#outlines.fsm.json_schema.build_regex_from_schema","title":"<code>build_regex_from_schema(schema, whitespace_pattern=None)</code>","text":"<p>Turn a JSON schema into a regex that matches any JSON object that follows    this schema.</p> <p>JSON Schema is a declarative language that allows to annotate JSON documents    with types and descriptions. These schemas can be generated from any Python    datastructure that has type annotation: namedtuples, dataclasses, Pydantic    models. And by ensuring that the generation respects the schema we ensure    that the output can be parsed into these objects.    This function parses the provided schema and builds a generation schedule which    mixes deterministic generation (fixed strings), and sampling with constraints.</p> <p>Parameters</p> <p>schema        A string that represents a JSON Schema.    whitespace_pattern        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)        Example: allow only a single space or newline with <code>whitespace_pattern=r\"[ ]?\"</code></p> <p>Returns</p> <p>A generation schedule. A list of strings that represent the JSON    schema's structure and regular expression that define the structure of    the fields.</p> <p>References</p> <p>.. [0] JSON Schema. https://json-schema.org/</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def build_regex_from_schema(schema: str, whitespace_pattern: Optional[str] = None):\n    \"\"\"Turn a JSON schema into a regex that matches any JSON object that follows\n    this schema.\n\n    JSON Schema is a declarative language that allows to annotate JSON documents\n    with types and descriptions. These schemas can be generated from any Python\n    datastructure that has type annotation: namedtuples, dataclasses, Pydantic\n    models. And by ensuring that the generation respects the schema we ensure\n    that the output can be parsed into these objects.\n    This function parses the provided schema and builds a generation schedule which\n    mixes deterministic generation (fixed strings), and sampling with constraints.\n\n    Parameters\n    ----------\n    schema\n        A string that represents a JSON Schema.\n    whitespace_pattern\n        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)\n        Example: allow only a single space or newline with `whitespace_pattern=r\"[\\n ]?\"`\n\n    Returns\n    -------\n    A generation schedule. A list of strings that represent the JSON\n    schema's structure and regular expression that define the structure of\n    the fields.\n\n    References\n    ----------\n    .. [0] JSON Schema. https://json-schema.org/\n\n    \"\"\"\n\n    schema = json.loads(schema)\n    Validator.check_schema(schema)\n\n    # Build reference resolver\n    schema = Resource(contents=schema, specification=DRAFT202012)\n    uri = schema.id() if schema.id() is not None else \"\"\n    registry = Registry().with_resource(uri=uri, resource=schema)\n    resolver = registry.resolver()\n\n    content = schema.contents\n    return to_regex(resolver, content, whitespace_pattern)\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.convert_json_schema_to_str","title":"<code>convert_json_schema_to_str(json_schema)</code>","text":"<p>Convert a JSON schema to a string.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.convert_json_schema_to_str--parameters","title":"Parameters","text":"<p>json_schema     The JSON schema.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.convert_json_schema_to_str--returns","title":"Returns","text":"<p>str     The JSON schema converted to a string.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.convert_json_schema_to_str--raises","title":"Raises","text":"<p>ValueError     If the schema is not a dictionary, a string or a Pydantic class.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def convert_json_schema_to_str(json_schema: Union[dict, str, Type[BaseModel]]) -&gt; str:\n    \"\"\"Convert a JSON schema to a string.\n\n    Parameters\n    ----------\n    json_schema\n        The JSON schema.\n\n    Returns\n    -------\n    str\n        The JSON schema converted to a string.\n\n    Raises\n    ------\n    ValueError\n        If the schema is not a dictionary, a string or a Pydantic class.\n    \"\"\"\n    if isinstance(json_schema, dict):\n        schema_str = json.dumps(json_schema)\n    elif isinstance(json_schema, str):\n        schema_str = json_schema\n    elif issubclass(json_schema, BaseModel):\n        schema_str = json.dumps(json_schema.model_json_schema())\n    else:\n        raise ValueError(\n            f\"Cannot parse schema {json_schema}. The schema must be either \"\n            + \"a Pydantic class, a dictionary or a string that contains the JSON \"\n            + \"schema specification\"\n        )\n    return schema_str\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; str:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    try:\n        fn_name = fn.__name__\n    except Exception as e:\n        fn_name = \"Arguments\"\n        warnings.warn(\n            f\"The function name could not be determined. Using default name 'Arguments' instead. For debugging, here is exact error:\\n{e}\",\n            category=UserWarning,\n        )\n    model = create_model(fn_name, **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.to_regex","title":"<code>to_regex(resolver, instance, whitespace_pattern=None)</code>","text":"<p>Translate a JSON Schema instance into a regex that validates the schema.</p> <p>Note</p> <p>Many features of JSON schema are missing:    - Handle <code>additionalProperties</code> keyword    - Handle types defined as a list    - Handle constraints on numbers    - Handle special patterns: <code>date</code>, <code>uri</code>, etc.</p> <p>This does not support recursive definitions.</p> <p>Parameters</p> <p>resolver        An object that resolves references to other instances within a schema    instance        The instance to translate    whitespace_pattern        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)        Example: allow only a single space or newline with <code>whitespace_pattern=r\"[ ]?\"</code></p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def to_regex(\n    resolver: Resolver, instance: dict, whitespace_pattern: Optional[str] = None\n):\n    \"\"\"Translate a JSON Schema instance into a regex that validates the schema.\n\n    Note\n    ----\n    Many features of JSON schema are missing:\n    - Handle `additionalProperties` keyword\n    - Handle types defined as a list\n    - Handle constraints on numbers\n    - Handle special patterns: `date`, `uri`, etc.\n\n    This does not support recursive definitions.\n\n    Parameters\n    ----------\n    resolver\n        An object that resolves references to other instances within a schema\n    instance\n        The instance to translate\n    whitespace_pattern\n        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)\n        Example: allow only a single space or newline with `whitespace_pattern=r\"[\\n ]?\"`\n    \"\"\"\n\n    # set whitespace pattern\n    if whitespace_pattern is None:\n        whitespace_pattern = WHITESPACE\n\n    if instance == {}:\n        # JSON Schema Spec: Empty object means unconstrained, any json type is legal\n        types = [\n            {\"type\": \"boolean\"},\n            {\"type\": \"null\"},\n            {\"type\": \"number\"},\n            {\"type\": \"integer\"},\n            {\"type\": \"string\"},\n            {\"type\": \"array\"},\n            {\"type\": \"object\"},\n        ]\n        regexes = [to_regex(resolver, t, whitespace_pattern) for t in types]\n        regexes = [rf\"({r})\" for r in regexes]\n        return rf\"{'|'.join(regexes)}\"\n\n    elif \"properties\" in instance:\n        regex = \"\"\n        regex += r\"\\{\"\n        properties = instance[\"properties\"]\n        required_properties = instance.get(\"required\", [])\n        is_required = [item in required_properties for item in properties]\n        # If at least one property is required, we include the one in the lastest position\n        # without any comma.\n        # For each property before it (optional or required), we add with a comma after the property.\n        # For each property after it (optional), we add with a comma before the property.\n        if any(is_required):\n            last_required_pos = max([i for i, value in enumerate(is_required) if value])\n            for i, (name, value) in enumerate(properties.items()):\n                subregex = f'{whitespace_pattern}\"{re.escape(name)}\"{whitespace_pattern}:{whitespace_pattern}'\n                subregex += to_regex(resolver, value, whitespace_pattern)\n                if i &lt; last_required_pos:\n                    subregex = f\"{subregex}{whitespace_pattern},\"\n                elif i &gt; last_required_pos:\n                    subregex = f\"{whitespace_pattern},{subregex}\"\n                regex += subregex if is_required[i] else f\"({subregex})?\"\n        # If no property is required, we have to create a possible pattern for each property in which\n        # it's the last one necessarilly present. Then, we add the others as optional before and after\n        # following the same strategy as described above.\n        # The whole block is made optional to allow the case in which no property is returned.\n        else:\n            property_subregexes = []\n            for i, (name, value) in enumerate(properties.items()):\n                subregex = f'{whitespace_pattern}\"{name}\"{whitespace_pattern}:{whitespace_pattern}'\n                subregex += to_regex(resolver, value, whitespace_pattern)\n                property_subregexes.append(subregex)\n            possible_patterns = []\n            for i in range(len(property_subregexes)):\n                pattern = \"\"\n                for subregex in property_subregexes[:i]:\n                    pattern += f\"({subregex}{whitespace_pattern},)?\"\n                pattern += property_subregexes[i]\n                for subregex in property_subregexes[i + 1 :]:\n                    pattern += f\"({whitespace_pattern},{subregex})?\"\n                possible_patterns.append(pattern)\n            regex += f\"({'|'.join(possible_patterns)})?\"\n\n        regex += f\"{whitespace_pattern}\" + r\"\\}\"\n\n        return regex\n\n    # To validate against allOf, the given data must be valid against all of the\n    # given subschemas.\n    elif \"allOf\" in instance:\n        subregexes = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"allOf\"]\n        ]\n        subregexes_str = [f\"{subregex}\" for subregex in subregexes]\n        return rf\"({''.join(subregexes_str)})\"\n\n    # To validate against `anyOf`, the given data must be valid against\n    # any (one or more) of the given subschemas.\n    elif \"anyOf\" in instance:\n        subregexes = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"anyOf\"]\n        ]\n        return rf\"({'|'.join(subregexes)})\"\n\n    # To validate against oneOf, the given data must be valid against exactly\n    # one of the given subschemas.\n    elif \"oneOf\" in instance:\n        subregexes = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"oneOf\"]\n        ]\n\n        xor_patterns = [f\"(?:{subregex})\" for subregex in subregexes]\n\n        return rf\"({'|'.join(xor_patterns)})\"\n\n    # Create pattern for Tuples, per JSON Schema spec, `prefixItems` determines types at each idx\n    elif \"prefixItems\" in instance:\n        element_patterns = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"prefixItems\"]\n        ]\n        comma_split_pattern = rf\"{whitespace_pattern},{whitespace_pattern}\"\n        tuple_inner = comma_split_pattern.join(element_patterns)\n        return rf\"\\[{whitespace_pattern}{tuple_inner}{whitespace_pattern}\\]\"\n\n    # The enum keyword is used to restrict a value to a fixed set of values. It\n    # must be an array with at least one element, where each element is unique.\n    elif \"enum\" in instance:\n        choices = []\n        for choice in instance[\"enum\"]:\n            if type(choice) in [int, float, bool, type(None), str]:\n                choices.append(re.escape(json.dumps(choice)))\n            else:\n                raise TypeError(f\"Unsupported data type in enum: {type(choice)}\")\n        return f\"({'|'.join(choices)})\"\n\n    elif \"const\" in instance:\n        const = instance[\"const\"]\n        if type(const) in [int, float, bool, type(None), str]:\n            const = re.escape(json.dumps(const))\n        else:\n            raise TypeError(f\"Unsupported data type in const: {type(const)}\")\n        return const\n\n    elif \"$ref\" in instance:\n        path = f\"{instance['$ref']}\"\n        instance = resolver.lookup(path).contents\n        return to_regex(resolver, instance, whitespace_pattern)\n\n    # The type keyword may either be a string or an array:\n    # - If it's a string, it is the name of one of the basic types.\n    # - If it is an array, it must be an array of strings, where each string is\n    # the name of one of the basic types, and each element is unique. In this\n    # case, the JSON snippet is valid if it matches any of the given types.\n    elif \"type\" in instance:\n        instance_type = instance[\"type\"]\n        if instance_type == \"string\":\n            if \"maxLength\" in instance or \"minLength\" in instance:\n                max_items = instance.get(\"maxLength\", \"\")\n                min_items = instance.get(\"minLength\", \"\")\n                try:\n                    if int(max_items) &lt; int(min_items):\n                        raise ValueError(\n                            \"maxLength must be greater than or equal to minLength\"\n                        )  # FIXME this raises an error but is caught right away by the except (meant for int(\"\") I assume)\n                except ValueError:\n                    pass\n                return f'\"{STRING_INNER}{{{min_items},{max_items}}}\"'\n            elif \"pattern\" in instance:\n                pattern = instance[\"pattern\"]\n                if pattern[0] == \"^\" and pattern[-1] == \"$\":\n                    return rf'(\"{pattern[1:-1]}\")'\n                else:\n                    return rf'(\"{pattern}\")'\n            elif \"format\" in instance:\n                format = instance[\"format\"]\n                if format == \"date-time\":\n                    return format_to_regex[\"date-time\"]\n                elif format == \"uuid\":\n                    return format_to_regex[\"uuid\"]\n                elif format == \"date\":\n                    return format_to_regex[\"date\"]\n                elif format == \"time\":\n                    return format_to_regex[\"time\"]\n                else:\n                    raise NotImplementedError(\n                        f\"Format {format} is not supported by Outlines\"\n                    )\n            else:\n                return type_to_regex[\"string\"]\n\n        elif instance_type == \"number\":\n            bounds = {\n                \"minDigitsInteger\",\n                \"maxDigitsInteger\",\n                \"minDigitsFraction\",\n                \"maxDigitsFraction\",\n                \"minDigitsExponent\",\n                \"maxDigitsExponent\",\n            }\n            if bounds.intersection(set(instance.keys())):\n                min_digits_integer, max_digits_integer = validate_quantifiers(\n                    instance.get(\"minDigitsInteger\"),\n                    instance.get(\"maxDigitsInteger\"),\n                    start_offset=1,\n                )\n                min_digits_fraction, max_digits_fraction = validate_quantifiers(\n                    instance.get(\"minDigitsFraction\"), instance.get(\"maxDigitsFraction\")\n                )\n                min_digits_exponent, max_digits_exponent = validate_quantifiers(\n                    instance.get(\"minDigitsExponent\"), instance.get(\"maxDigitsExponent\")\n                )\n                integers_quantifier = (\n                    f\"{{{min_digits_integer},{max_digits_integer}}}\"\n                    if min_digits_integer or max_digits_integer\n                    else \"*\"\n                )\n                fraction_quantifier = (\n                    f\"{{{min_digits_fraction},{max_digits_fraction}}}\"\n                    if min_digits_fraction or max_digits_fraction\n                    else \"+\"\n                )\n                exponent_quantifier = (\n                    f\"{{{min_digits_exponent},{max_digits_exponent}}}\"\n                    if min_digits_exponent or max_digits_exponent\n                    else \"+\"\n                )\n                return rf\"((-)?(0|[1-9][0-9]{integers_quantifier}))(\\.[0-9]{fraction_quantifier})?([eE][+-][0-9]{exponent_quantifier})?\"\n            return type_to_regex[\"number\"]\n\n        elif instance_type == \"integer\":\n            if \"minDigits\" in instance or \"maxDigits\" in instance:\n                min_digits, max_digits = validate_quantifiers(\n                    instance.get(\"minDigits\"), instance.get(\"maxDigits\"), start_offset=1\n                )\n                return rf\"(-)?(0|[1-9][0-9]{{{min_digits},{max_digits}}})\"\n            return type_to_regex[\"integer\"]\n\n        elif instance_type == \"array\":\n            num_repeats = _get_num_items_pattern(\n                instance.get(\"minItems\"), instance.get(\"maxItems\"), whitespace_pattern\n            )\n            if num_repeats is None:\n                return rf\"\\[{whitespace_pattern}\\]\"\n\n            allow_empty = \"?\" if int(instance.get(\"minItems\", 0)) == 0 else \"\"\n\n            if \"items\" in instance:\n                items_regex = to_regex(resolver, instance[\"items\"], whitespace_pattern)\n                return rf\"\\[{whitespace_pattern}(({items_regex})(,{whitespace_pattern}({items_regex})){num_repeats}){allow_empty}{whitespace_pattern}\\]\"\n            else:\n                # Here we need to make the choice to exclude generating list of objects\n                # if the specification of the object is not given, even though a JSON\n                # object that contains an object here would be valid under the specification.\n                legal_types = [\n                    {\"type\": \"boolean\"},\n                    {\"type\": \"null\"},\n                    {\"type\": \"number\"},\n                    {\"type\": \"integer\"},\n                    {\"type\": \"string\"},\n                ]\n                depth = instance.get(\"depth\", 2)\n                if depth &gt; 0:\n                    legal_types.append({\"type\": \"object\", \"depth\": depth - 1})\n                    legal_types.append({\"type\": \"array\", \"depth\": depth - 1})\n\n                regexes = [\n                    to_regex(resolver, t, whitespace_pattern) for t in legal_types\n                ]\n                return rf\"\\[{whitespace_pattern}({'|'.join(regexes)})(,{whitespace_pattern}({'|'.join(regexes)})){num_repeats}{allow_empty}{whitespace_pattern}\\]\"\n\n        elif instance_type == \"object\":\n            # pattern for json object with values defined by instance[\"additionalProperties\"]\n            # enforces value type constraints recursively, \"minProperties\", and \"maxProperties\"\n            # doesn't enforce \"required\", \"dependencies\", \"propertyNames\" \"any/all/on Of\"\n            num_repeats = _get_num_items_pattern(\n                instance.get(\"minProperties\"),\n                instance.get(\"maxProperties\"),\n                whitespace_pattern,\n            )\n            if num_repeats is None:\n                return rf\"\\{{{whitespace_pattern}\\}}\"\n\n            allow_empty = \"?\" if int(instance.get(\"minProperties\", 0)) == 0 else \"\"\n\n            additional_properties = instance.get(\"additionalProperties\")\n\n            if additional_properties is None or additional_properties is True:\n                # JSON Schema behavior: If the additionalProperties of an object is\n                # unset or True, it is unconstrained object.\n                # We handle this by setting additionalProperties to anyOf: {all types}\n\n                legal_types = [\n                    {\"type\": \"string\"},\n                    {\"type\": \"number\"},\n                    {\"type\": \"boolean\"},\n                    {\"type\": \"null\"},\n                ]\n\n                # We set the object depth to 2 to keep the expression finite, but the \"depth\"\n                # key is not a true component of the JSON Schema specification.\n                depth = instance.get(\"depth\", 2)\n                if depth &gt; 0:\n                    legal_types.append({\"type\": \"object\", \"depth\": depth - 1})\n                    legal_types.append({\"type\": \"array\", \"depth\": depth - 1})\n                additional_properties = {\"anyOf\": legal_types}\n\n            value_pattern = to_regex(\n                resolver, additional_properties, whitespace_pattern\n            )\n            key_value_pattern = (\n                f\"{STRING}{whitespace_pattern}:{whitespace_pattern}{value_pattern}\"\n            )\n            key_value_successor_pattern = (\n                f\"{whitespace_pattern},{whitespace_pattern}{key_value_pattern}\"\n            )\n            multiple_key_value_pattern = f\"({key_value_pattern}({key_value_successor_pattern}){num_repeats}){allow_empty}\"\n\n            return (\n                r\"\\{\"\n                + whitespace_pattern\n                + multiple_key_value_pattern\n                + whitespace_pattern\n                + r\"\\}\"\n            )\n\n        elif instance_type == \"boolean\":\n            return type_to_regex[\"boolean\"]\n\n        elif instance_type == \"null\":\n            return type_to_regex[\"null\"]\n\n        elif isinstance(instance_type, list):\n            # Here we need to make the choice to exclude generating an object\n            # if the specification of the object is not give, even though a JSON\n            # object that contains an object here would be valid under the specification.\n            regexes = [\n                to_regex(resolver, {\"type\": t}, whitespace_pattern)\n                for t in instance_type\n                if t != \"object\"\n            ]\n            return rf\"({'|'.join(regexes)})\"\n\n    raise NotImplementedError(\n        f\"\"\"Could not translate the instance {instance} to a\n    regular expression. Make sure it is valid to the JSON Schema specification. If\n    it is, please open an issue on the Outlines repository\"\"\"\n    )\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.validate_quantifiers","title":"<code>validate_quantifiers(min_bound, max_bound, start_offset=0)</code>","text":"<p>Ensures that the bounds of a number are valid. Bounds are used as quantifiers in the regex.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.validate_quantifiers--parameters","title":"Parameters","text":"<p>min_bound     The minimum value that the number can take. max_bound     The maximum value that the number can take. start_offset     Number of elements that are already present in the regex but still need to be counted.     ex: if the regex is already \"(-)?(0|1-9)\", we will always have at least 1 digit, so the start_offset is 1.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.validate_quantifiers--returns","title":"Returns","text":"<p>min_bound     The minimum value that the number can take. max_bound     The maximum value that the number can take.</p>"},{"location":"api/json_schema/#outlines.fsm.json_schema.validate_quantifiers--raises","title":"Raises","text":"<p>ValueError     If the minimum bound is greater than the maximum bound.</p> <p>TypeError or ValueError     If the minimum bound is not an integer or None.     or     If the maximum bound is not an integer or None.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def validate_quantifiers(\n    min_bound: Optional[str], max_bound: Optional[str], start_offset: int = 0\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Ensures that the bounds of a number are valid. Bounds are used as quantifiers in the regex.\n\n    Parameters\n    ----------\n    min_bound\n        The minimum value that the number can take.\n    max_bound\n        The maximum value that the number can take.\n    start_offset\n        Number of elements that are already present in the regex but still need to be counted.\n        ex: if the regex is already \"(-)?(0|[1-9][0-9])\", we will always have at least 1 digit, so the start_offset is 1.\n\n    Returns\n    -------\n    min_bound\n        The minimum value that the number can take.\n    max_bound\n        The maximum value that the number can take.\n\n    Raises\n    ------\n    ValueError\n        If the minimum bound is greater than the maximum bound.\n\n    TypeError or ValueError\n        If the minimum bound is not an integer or None.\n        or\n        If the maximum bound is not an integer or None.\n    \"\"\"\n    min_bound = \"\" if min_bound is None else str(int(min_bound) - start_offset)\n    max_bound = \"\" if max_bound is None else str(int(max_bound) - start_offset)\n    if min_bound and max_bound:\n        if int(max_bound) &lt; int(min_bound):\n            raise ValueError(\"max bound must be greater than or equal to min bound\")\n    return min_bound, max_bound\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api/models/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, tokenizer: \"PreTrainedTokenizer\", **kwargs):\n        self.tokenizer = tokenizer\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[\"torch.LongTensor\", \"torch.LongTensor\"]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: \"torch.LongTensor\") -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            if hasattr(self, \"model_name\") and hasattr(self, \"kwargs\"):\n                return (\n                    other.model_name == self.model_name and other.kwargs == self.kwargs\n                )\n            else:\n                return other.tokenizer == self.tokenizer\n        return NotImplemented\n\n    def __hash__(self):\n        return hash(Hasher.hash(self.tokenizer))\n\n    def __getstate__(self):\n        state = {\"tokenizer\": self.tokenizer}\n        return state\n\n    def __setstate__(self, state):\n        self.__init__(state[\"tokenizer\"])\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers","title":"<code>Transformers</code>","text":"<p>Represents a <code>transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformers:\n    \"\"\"Represents a `transformers` model.\"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        self.model = model\n        self.tokenizer = TransformerTokenizer(tokenizer)\n\n    def forward(\n        self,\n        input_ids: \"torch.LongTensor\",\n        attention_mask: \"torch.LongTensor\",\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; Tuple[\"torch.FloatTensor\", Optional[KVCacheType]]:\n        \"\"\"Compute a forward pass through the transformer model.\n\n        Parameters\n        ----------\n        input_ids\n            The input token ids.  Must be one or two dimensional.\n        attention_mask\n            The attention mask.  Must be one or two dimensional.\n        past_key_values\n            A tuple of tuples containing the cached key and value tensors for each\n            attention head.\n\n        Returns\n        -------\n        The computed logits and the new cached key and value tensors.\n\n        \"\"\"\n        try:\n            import torch\n        except ImportError:\n            ImportError(\n                \"The `torch` library needs to be installed to use `transformers` models.\"\n            )\n        assert 0 &lt; input_ids.ndim &lt; 3\n\n        if past_key_values:\n            input_ids = input_ids[..., -1].unsqueeze(-1)\n\n        with torch.inference_mode():\n            output = self.model(\n                input_ids,\n                attention_mask=attention_mask,\n                return_dict=True,\n                output_attentions=False,\n                output_hidden_states=False,\n                past_key_values=past_key_values,\n            )\n\n        return output.logits, output.past_key_values\n\n    def __call__(\n        self,\n        input_ids: \"torch.LongTensor\",\n        attention_mask: \"torch.LongTensor\",\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; \"torch.FloatTensor\":\n        logits, kv_cache = self.forward(input_ids, attention_mask, past_key_values)\n        next_token_logits = logits[..., -1, :]\n\n        return next_token_logits, kv_cache\n\n    def generate(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; Union[str, List[str], List[List[str]]]:\n        \"\"\"Generate text using `transformers`.\n\n        Arguments\n        ---------\n        prompts\n            A prompt or list of prompts.\n        generation_parameters\n            An instance of `GenerationParameters` that contains the prompt,\n            the maximum number of tokens, stop sequences and seed. All the\n            arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n        logits_processor\n            The logits processor to use when generating text.\n        sampling_parameters\n            An instance of `SamplingParameters`, a dataclass that contains\n            the name of the sampler to use and related parameters as available\n            in Outlines.\n\n        Returns\n        -------\n        The generated text\n        \"\"\"\n        if isinstance(prompts, str):\n            # convert to 2d\n            input_ids, attention_mask = self.tokenizer.encode([prompts])\n        else:\n            input_ids, attention_mask = self.tokenizer.encode(prompts)\n\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": attention_mask.to(self.model.device),\n        }\n        if (\n            \"attention_mask\"\n            not in inspect.signature(self.model.forward).parameters.keys()\n        ):\n            del inputs[\"attention_mask\"]\n\n        generation_kwargs = self._get_generation_kwargs(\n            prompts,\n            generation_parameters,\n            logits_processor,\n            sampling_parameters,\n        )\n        generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n        # if single str input and single sample per input, convert to a 1D output\n        if isinstance(prompts, str):\n            generated_ids = generated_ids.squeeze(0)\n\n        return self._decode_generation(generated_ids)\n\n    def stream(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; Iterator[Union[str, List[str]]]:\n        \"\"\"\n        Temporary stream stand-in which implements stream() signature\n        and equivalent behaviour but isn't yielded until generation completes.\n\n        TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n        \"\"\"\n        if isinstance(prompts, str):\n            # convert to 2d\n            input_ids, attention_mask = self.tokenizer.encode([prompts])\n        else:\n            input_ids, attention_mask = self.tokenizer.encode(prompts)\n        inputs = {\n            \"input_ids\": input_ids.to(self.model.device),\n            \"attention_mask\": attention_mask.to(self.model.device),\n        }\n        if (\n            \"attention_mask\"\n            not in inspect.signature(self.model.forward).parameters.keys()\n        ):\n            del inputs[\"attention_mask\"]\n\n        generation_kwargs = self._get_generation_kwargs(\n            prompts,\n            generation_parameters,\n            logits_processor,\n            sampling_parameters,\n        )\n        generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n        # if single str input and single sample per input, convert to a 1D output\n        if isinstance(prompts, str):\n            generated_ids = generated_ids.squeeze(0)\n\n        for i in range(generated_ids.size(-1)):\n            output_group_ids = generated_ids.select(-1, i).unsqueeze(-1)\n            yield self._decode_generation(output_group_ids)\n\n    def _get_generation_kwargs(\n        self,\n        prompts: Union[str, List[str]],\n        generation_parameters: GenerationParameters,\n        logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n        sampling_parameters: SamplingParameters,\n    ) -&gt; dict:\n        \"\"\"\n        Conert outlines generation parameters into model.generate kwargs\n        \"\"\"\n        from transformers import GenerationConfig, LogitsProcessorList, set_seed\n\n        max_new_tokens, stop_at, seed = dataclasses.astuple(generation_parameters)\n        sampler, num_samples, top_p, top_k, temperature = dataclasses.astuple(\n            sampling_parameters\n        )\n        if max_new_tokens is None:\n            max_new_tokens = int(2**30)\n\n        # global seed, not desirable\n        if seed is not None:\n            set_seed(seed)\n\n        if logits_processor is not None:\n            logits_processor_list = LogitsProcessorList([logits_processor])\n        else:\n            logits_processor_list = None\n\n        generation_config = GenerationConfig(\n            max_new_tokens=max_new_tokens,\n            stop_strings=stop_at,\n            num_return_sequences=(num_samples or 1),\n            top_p=top_p,\n            top_k=top_k,\n            temperature=temperature,\n            do_sample=(sampler == \"multinomial\"),\n            num_beams=(num_samples if sampler == \"beam_search\" else 1),\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.pad_token_id,\n        )\n\n        return dict(\n            logits_processor=logits_processor_list,\n            generation_config=generation_config,\n            tokenizer=self.tokenizer.tokenizer,\n        )\n\n    def _generate_output_seq(\n        self, prompts, inputs, generation_config, **generation_kwargs\n    ):\n        input_ids = inputs[\"input_ids\"]\n        output_ids = self.model.generate(\n            **inputs, generation_config=generation_config, **generation_kwargs\n        )\n\n        # encoder-decoder returns output_ids only, decoder-only returns full seq ids\n        if self.model.config.is_encoder_decoder:\n            generated_ids = output_ids\n        else:\n            generated_ids = output_ids[:, input_ids.shape[1] :]\n\n        # if batch list inputs AND multiple samples per input, convert generated_id to 3D view\n        num_samples = generation_config.num_return_sequences or 1\n\n        if num_samples &gt; 1 and isinstance(prompts, list):\n            batch_size = input_ids.size(0)\n            num_return_sequences = generation_config.num_return_sequences or 1\n            generated_ids = generated_ids.view(batch_size, num_return_sequences, -1)\n\n        return generated_ids\n\n    def _decode_generation(self, generated_ids: \"torch.Tensor\"):\n        if len(generated_ids.shape) == 1:\n            return self.tokenizer.decode([generated_ids])[0]\n        elif len(generated_ids.shape) == 2:\n            return self.tokenizer.decode(generated_ids)\n        elif len(generated_ids.shape) == 3:\n            return [\n                self.tokenizer.decode(generated_ids[i])\n                for i in range(len(generated_ids))\n            ]\n        else:\n            raise TypeError(\n                f\"Generated outputs aren't 1D, 2D or 3D, but instead are {generated_ids.shape}\"\n            )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers.forward","title":"<code>forward(input_ids, attention_mask, past_key_values=None)</code>","text":"<p>Compute a forward pass through the transformer model.</p>"},{"location":"api/models/#outlines.models.transformers.Transformers.forward--parameters","title":"Parameters","text":"<p>input_ids     The input token ids.  Must be one or two dimensional. attention_mask     The attention mask.  Must be one or two dimensional. past_key_values     A tuple of tuples containing the cached key and value tensors for each     attention head.</p>"},{"location":"api/models/#outlines.models.transformers.Transformers.forward--returns","title":"Returns","text":"<p>The computed logits and the new cached key and value tensors.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def forward(\n    self,\n    input_ids: \"torch.LongTensor\",\n    attention_mask: \"torch.LongTensor\",\n    past_key_values: Optional[Tuple] = None,\n) -&gt; Tuple[\"torch.FloatTensor\", Optional[KVCacheType]]:\n    \"\"\"Compute a forward pass through the transformer model.\n\n    Parameters\n    ----------\n    input_ids\n        The input token ids.  Must be one or two dimensional.\n    attention_mask\n        The attention mask.  Must be one or two dimensional.\n    past_key_values\n        A tuple of tuples containing the cached key and value tensors for each\n        attention head.\n\n    Returns\n    -------\n    The computed logits and the new cached key and value tensors.\n\n    \"\"\"\n    try:\n        import torch\n    except ImportError:\n        ImportError(\n            \"The `torch` library needs to be installed to use `transformers` models.\"\n        )\n    assert 0 &lt; input_ids.ndim &lt; 3\n\n    if past_key_values:\n        input_ids = input_ids[..., -1].unsqueeze(-1)\n\n    with torch.inference_mode():\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            output_attentions=False,\n            output_hidden_states=False,\n            past_key_values=past_key_values,\n        )\n\n    return output.logits, output.past_key_values\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers.generate","title":"<code>generate(prompts, generation_parameters, logits_processor, sampling_parameters)</code>","text":"<p>Generate text using <code>transformers</code>.</p>"},{"location":"api/models/#outlines.models.transformers.Transformers.generate--arguments","title":"Arguments","text":"<p>prompts     A prompt or list of prompts. generation_parameters     An instance of <code>GenerationParameters</code> that contains the prompt,     the maximum number of tokens, stop sequences and seed. All the     arguments to <code>SequenceGeneratorAdapter</code>'s <code>__cal__</code> method. logits_processor     The logits processor to use when generating text. sampling_parameters     An instance of <code>SamplingParameters</code>, a dataclass that contains     the name of the sampler to use and related parameters as available     in Outlines.</p>"},{"location":"api/models/#outlines.models.transformers.Transformers.generate--returns","title":"Returns","text":"<p>The generated text</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def generate(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n    sampling_parameters: SamplingParameters,\n) -&gt; Union[str, List[str], List[List[str]]]:\n    \"\"\"Generate text using `transformers`.\n\n    Arguments\n    ---------\n    prompts\n        A prompt or list of prompts.\n    generation_parameters\n        An instance of `GenerationParameters` that contains the prompt,\n        the maximum number of tokens, stop sequences and seed. All the\n        arguments to `SequenceGeneratorAdapter`'s `__cal__` method.\n    logits_processor\n        The logits processor to use when generating text.\n    sampling_parameters\n        An instance of `SamplingParameters`, a dataclass that contains\n        the name of the sampler to use and related parameters as available\n        in Outlines.\n\n    Returns\n    -------\n    The generated text\n    \"\"\"\n    if isinstance(prompts, str):\n        # convert to 2d\n        input_ids, attention_mask = self.tokenizer.encode([prompts])\n    else:\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n\n    inputs = {\n        \"input_ids\": input_ids.to(self.model.device),\n        \"attention_mask\": attention_mask.to(self.model.device),\n    }\n    if (\n        \"attention_mask\"\n        not in inspect.signature(self.model.forward).parameters.keys()\n    ):\n        del inputs[\"attention_mask\"]\n\n    generation_kwargs = self._get_generation_kwargs(\n        prompts,\n        generation_parameters,\n        logits_processor,\n        sampling_parameters,\n    )\n    generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n    # if single str input and single sample per input, convert to a 1D output\n    if isinstance(prompts, str):\n        generated_ids = generated_ids.squeeze(0)\n\n    return self._decode_generation(generated_ids)\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformers.stream","title":"<code>stream(prompts, generation_parameters, logits_processor, sampling_parameters)</code>","text":"<p>Temporary stream stand-in which implements stream() signature and equivalent behaviour but isn't yielded until generation completes.</p> <p>TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def stream(\n    self,\n    prompts: Union[str, List[str]],\n    generation_parameters: GenerationParameters,\n    logits_processor: Optional[\"OutlinesLogitsProcessor\"],\n    sampling_parameters: SamplingParameters,\n) -&gt; Iterator[Union[str, List[str]]]:\n    \"\"\"\n    Temporary stream stand-in which implements stream() signature\n    and equivalent behaviour but isn't yielded until generation completes.\n\n    TODO: implement following completion of https://github.com/huggingface/transformers/issues/30810\n    \"\"\"\n    if isinstance(prompts, str):\n        # convert to 2d\n        input_ids, attention_mask = self.tokenizer.encode([prompts])\n    else:\n        input_ids, attention_mask = self.tokenizer.encode(prompts)\n    inputs = {\n        \"input_ids\": input_ids.to(self.model.device),\n        \"attention_mask\": attention_mask.to(self.model.device),\n    }\n    if (\n        \"attention_mask\"\n        not in inspect.signature(self.model.forward).parameters.keys()\n    ):\n        del inputs[\"attention_mask\"]\n\n    generation_kwargs = self._get_generation_kwargs(\n        prompts,\n        generation_parameters,\n        logits_processor,\n        sampling_parameters,\n    )\n    generated_ids = self._generate_output_seq(prompts, inputs, **generation_kwargs)\n\n    # if single str input and single sample per input, convert to a 1D output\n    if isinstance(prompts, str):\n        generated_ids = generated_ids.squeeze(0)\n\n    for i in range(generated_ids.size(-1)):\n        output_group_ids = generated_ids.select(-1, i).unsqueeze(-1)\n        yield self._decode_generation(output_group_ids)\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.transformers","title":"<code>transformers(model_name, device=None, model_kwargs={}, tokenizer_kwargs={}, model_class=None, tokenizer_class=None)</code>","text":"<p>Instantiate a model from the <code>transformers</code> library and its tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--parameters","title":"Parameters","text":"<p>model_name     The name of the model as listed on Hugging Face's model page. device     The device(s) on which the model should be loaded. This overrides     the <code>device_map</code> entry in <code>model_kwargs</code> when provided. model_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the model. tokenizer_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--returns","title":"Returns","text":"<p>A <code>TransformersModel</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def transformers(\n    model_name: str,\n    device: Optional[str] = None,\n    model_kwargs: dict = {},\n    tokenizer_kwargs: dict = {},\n    model_class=None,\n    tokenizer_class=None,\n):\n    \"\"\"Instantiate a model from the `transformers` library and its tokenizer.\n\n    Parameters\n    ----------\n    model_name\n        The name of the model as listed on Hugging Face's model page.\n    device\n        The device(s) on which the model should be loaded. This overrides\n        the `device_map` entry in `model_kwargs` when provided.\n    model_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the model.\n    tokenizer_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the tokenizer.\n\n    Returns\n    -------\n    A `TransformersModel` model instance.\n\n    \"\"\"\n    if model_class is None or tokenizer_class is None:\n        try:\n            from transformers import AutoModelForCausalLM, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"The `transformers` library needs to be installed in order to use `transformers` models.\"\n            )\n    if model_class is None:\n        model_class = AutoModelForCausalLM\n    if tokenizer_class is None:\n        tokenizer_class = AutoTokenizer\n\n    if device is not None:\n        model_kwargs[\"device_map\"] = device\n\n    model = model_class.from_pretrained(model_name, **model_kwargs)\n\n    tokenizer_kwargs.setdefault(\"padding_side\", \"left\")\n    tokenizer = tokenizer_class.from_pretrained(model_name, **tokenizer_kwargs)\n\n    return Transformers(model, tokenizer)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>An object that represents the OpenAI API.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI:\n    \"\"\"An object that represents the OpenAI API.\"\"\"\n\n    def __init__(\n        self,\n        client,\n        config,\n        system_prompt: Optional[str] = None,\n    ):\n        \"\"\"Create an `OpenAI` instance.\n\n        This class supports the standard OpenAI API, the Azure OpeanAI API as\n        well as compatible APIs that rely on the OpenAI client.\n\n        Parameters\n        ----------\n        client\n            An instance of the API's async client.\n        config\n            An instance of `OpenAIConfig`. Can be useful to specify some\n            parameters that cannot be set by calling this class' methods.\n        \"\"\"\n\n        self.client = client\n        self.config = config\n\n        # We count the total number of prompt and generated tokens as returned\n        # by the OpenAI API, summed over all the requests performed with this\n        # model instance.\n        self.prompt_tokens = 0\n        self.completion_tokens = 0\n\n        self.format_sequence = lambda seq: seq\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        max_tokens: Optional[int] = None,\n        stop_at: Optional[Union[List[str], str]] = None,\n        *,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        samples: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Call the OpenAI API to generate text.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        max_tokens\n            The maximum number of tokens to generate\n        stop_at\n            A string or array of strings which, such that the generation stops\n            when they are generated.\n        system_prompt\n            The content of the system message that precedes the user's prompt.\n        temperature\n            The value of the temperature used to sample tokens\n        samples\n            The number of completions to generate for each prompt\n        stop_at\n            Up to 4 words where the API will stop the completion.\n\n        \"\"\"\n        if max_tokens is None:\n            max_tokens = self.config.max_tokens\n        if stop_at is None:\n            stop_at = self.config.stop\n        if temperature is None:\n            temperature = self.config.temperature\n        if samples is None:\n            samples = self.config.n\n\n        config = replace(self.config, max_tokens=max_tokens, temperature=temperature, n=samples, stop=stop_at)  # type: ignore\n\n        response, prompt_tokens, completion_tokens = generate_chat(\n            prompt, system_prompt, self.client, config\n        )\n        self.prompt_tokens += prompt_tokens\n        self.completion_tokens += completion_tokens\n\n        return self.format_sequence(response)\n\n    def stream(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"Streaming is currently not supported for the OpenAI API\"\n        )\n\n    def new_with_replacements(self, **kwargs):\n        new_instance = copy.copy(self)\n        new_instance.config = replace(new_instance.config, **kwargs)\n        return new_instance\n\n    def __str__(self):\n        return self.__class__.__name__ + \" API\"\n\n    def __repr__(self):\n        return str(self.config)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__","title":"<code>__call__(prompt, max_tokens=None, stop_at=None, *, system_prompt=None, temperature=None, samples=None)</code>","text":"<p>Call the OpenAI API to generate text.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model max_tokens     The maximum number of tokens to generate stop_at     A string or array of strings which, such that the generation stops     when they are generated. system_prompt     The content of the system message that precedes the user's prompt. temperature     The value of the temperature used to sample tokens samples     The number of completions to generate for each prompt stop_at     Up to 4 words where the API will stop the completion.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __call__(\n    self,\n    prompt: Union[str, List[str]],\n    max_tokens: Optional[int] = None,\n    stop_at: Optional[Union[List[str], str]] = None,\n    *,\n    system_prompt: Optional[str] = None,\n    temperature: Optional[float] = None,\n    samples: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Call the OpenAI API to generate text.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    max_tokens\n        The maximum number of tokens to generate\n    stop_at\n        A string or array of strings which, such that the generation stops\n        when they are generated.\n    system_prompt\n        The content of the system message that precedes the user's prompt.\n    temperature\n        The value of the temperature used to sample tokens\n    samples\n        The number of completions to generate for each prompt\n    stop_at\n        Up to 4 words where the API will stop the completion.\n\n    \"\"\"\n    if max_tokens is None:\n        max_tokens = self.config.max_tokens\n    if stop_at is None:\n        stop_at = self.config.stop\n    if temperature is None:\n        temperature = self.config.temperature\n    if samples is None:\n        samples = self.config.n\n\n    config = replace(self.config, max_tokens=max_tokens, temperature=temperature, n=samples, stop=stop_at)  # type: ignore\n\n    response, prompt_tokens, completion_tokens = generate_chat(\n        prompt, system_prompt, self.client, config\n    )\n    self.prompt_tokens += prompt_tokens\n    self.completion_tokens += completion_tokens\n\n    return self.format_sequence(response)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(client, config, system_prompt=None)</code>","text":"<p>Create an <code>OpenAI</code> instance.</p> <p>This class supports the standard OpenAI API, the Azure OpeanAI API as well as compatible APIs that rely on the OpenAI client.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__--parameters","title":"Parameters","text":"<p>client     An instance of the API's async client. config     An instance of <code>OpenAIConfig</code>. Can be useful to specify some     parameters that cannot be set by calling this class' methods.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    client,\n    config,\n    system_prompt: Optional[str] = None,\n):\n    \"\"\"Create an `OpenAI` instance.\n\n    This class supports the standard OpenAI API, the Azure OpeanAI API as\n    well as compatible APIs that rely on the OpenAI client.\n\n    Parameters\n    ----------\n    client\n        An instance of the API's async client.\n    config\n        An instance of `OpenAIConfig`. Can be useful to specify some\n        parameters that cannot be set by calling this class' methods.\n    \"\"\"\n\n    self.client = client\n    self.config = config\n\n    # We count the total number of prompt and generated tokens as returned\n    # by the OpenAI API, summed over all the requests performed with this\n    # model instance.\n    self.prompt_tokens = 0\n    self.completion_tokens = 0\n\n    self.format_sequence = lambda seq: seq\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig","title":"<code>OpenAIConfig</code>  <code>dataclass</code>","text":"<p>Represents the parameters of the OpenAI API.</p> <p>The information was last fetched on 2023/11/20. We document below the properties that are specific to the OpenAI API. Not all these properties are supported by Outlines.</p>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig--properties","title":"Properties","text":"<p>model     The name of the model. Available models can be found on OpenAI's website. frequence_penalty     Number between 2.0 and -2.0. Positive values penalize new tokens based on     their existing frequency in the text, logit_bias     Modifies the likelihood of specified tokens to appear in the completion.     Number between -100 (forbid) and +100 (only allows). n     The number of completions to return for each prompt. presence_penalty     Similar to frequency penalty. response_format     Specifies the format the model must output. <code>{\"type\": \"json_object\"}</code>     enables JSON mode. seed     Two completions with the same <code>seed</code> value should return the same     completion. This is however not guaranteed. stop     Up to 4 words where the API will stop the completion. temperature     Number between 0 and 2. Higher values make the output more random, while     lower values make it more deterministic. top_p     Number between 0 and 1. Parameter for nucleus sampling. user     A unique identifier for the end-user.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@dataclass(frozen=True)\nclass OpenAIConfig:\n    \"\"\"Represents the parameters of the OpenAI API.\n\n    The information was last fetched on 2023/11/20. We document below the\n    properties that are specific to the OpenAI API. Not all these properties are\n    supported by Outlines.\n\n    Properties\n    ----------\n    model\n        The name of the model. Available models can be found on OpenAI's website.\n    frequence_penalty\n        Number between 2.0 and -2.0. Positive values penalize new tokens based on\n        their existing frequency in the text,\n    logit_bias\n        Modifies the likelihood of specified tokens to appear in the completion.\n        Number between -100 (forbid) and +100 (only allows).\n    n\n        The number of completions to return for each prompt.\n    presence_penalty\n        Similar to frequency penalty.\n    response_format\n        Specifies the format the model must output. `{\"type\": \"json_object\"}`\n        enables JSON mode.\n    seed\n        Two completions with the same `seed` value should return the same\n        completion. This is however not guaranteed.\n    stop\n        Up to 4 words where the API will stop the completion.\n    temperature\n        Number between 0 and 2. Higher values make the output more random, while\n        lower values make it more deterministic.\n    top_p\n        Number between 0 and 1. Parameter for nucleus sampling.\n    user\n        A unique identifier for the end-user.\n\n    \"\"\"\n\n    model: str = \"\"\n    frequency_penalty: float = 0\n    logit_bias: Dict[int, int] = field(default_factory=dict)\n    max_tokens: Optional[int] = None\n    n: int = 1\n    presence_penalty: float = 0\n    response_format: Optional[Dict[str, str]] = None\n    seed: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    temperature: float = 1.0\n    top_p: int = 1\n    user: str = field(default_factory=str)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.error_handler","title":"<code>error_handler(api_call_fn)</code>","text":"<p>Handle OpenAI API errors and missing API key.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def error_handler(api_call_fn: Callable) -&gt; Callable:\n    \"\"\"Handle OpenAI API errors and missing API key.\"\"\"\n\n    def call(*args, **kwargs):\n        import openai\n\n        try:\n            return api_call_fn(*args, **kwargs)\n        except (\n            openai.APITimeoutError,\n            openai.InternalServerError,\n            openai.RateLimitError,\n        ) as e:\n            raise OSError(f\"Could not connect to the OpenAI API: {e}\")\n        except (\n            openai.AuthenticationError,\n            openai.BadRequestError,\n            openai.ConflictError,\n            openai.PermissionDeniedError,\n            openai.NotFoundError,\n            openai.UnprocessableEntityError,\n        ) as e:\n            raise e\n\n    return call\n</code></pre>"},{"location":"api/models/#outlines.models.openai.generate_chat","title":"<code>generate_chat(prompt, system_prompt, client, config)</code>  <code>async</code>","text":"<p>Call OpenAI's Chat Completion API.</p>"},{"location":"api/models/#outlines.models.openai.generate_chat--parameters","title":"Parameters","text":"<p>prompt     The prompt we use to start the generation. Passed to the model     with the \"user\" role. system_prompt     The system prompt, passed to the model with the \"system\" role     before the prompt. client     The API client config     An <code>OpenAIConfig</code> instance.</p>"},{"location":"api/models/#outlines.models.openai.generate_chat--returns","title":"Returns","text":"<p>A tuple that contains the model's response(s) and usage statistics.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@functools.partial(vectorize, signature=\"(),(),(),()-&gt;(s),(),()\")\nasync def generate_chat(\n    prompt: str,\n    system_prompt: Union[str, None],\n    client,\n    config: OpenAIConfig,\n) -&gt; Tuple[np.ndarray, int, int]:\n    \"\"\"Call OpenAI's Chat Completion API.\n\n    Parameters\n    ----------\n    prompt\n        The prompt we use to start the generation. Passed to the model\n        with the \"user\" role.\n    system_prompt\n        The system prompt, passed to the model with the \"system\" role\n        before the prompt.\n    client\n        The API client\n    config\n        An `OpenAIConfig` instance.\n\n    Returns\n    -------\n    A tuple that contains the model's response(s) and usage statistics.\n\n    \"\"\"\n\n    @error_handler\n    @cache()\n    async def call_api(prompt, system_prompt, config):\n        responses = await client.chat.completions.create(\n            messages=system_message + user_message,\n            **asdict(config),  # type: ignore\n        )\n        return responses.model_dump()\n\n    system_message = (\n        [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n    )\n    user_message = [{\"role\": \"user\", \"content\": prompt}]\n\n    responses = await call_api(prompt, system_prompt, config)\n\n    results = np.array(\n        [responses[\"choices\"][i][\"message\"][\"content\"] for i in range(config.n)]\n    )\n    usage = responses[\"usage\"]\n\n    return results, usage[\"prompt_tokens\"], usage[\"completion_tokens\"]\n</code></pre>"},{"location":"api/parsing/","title":"Parsing","text":""},{"location":"api/parsing/#outlines.fsm.parsing.PartialIndenter","title":"<code>PartialIndenter</code>","text":"<p>               Bases: <code>Indenter</code></p> <p>An <code>Indenter</code> that doesn't reset its state every time <code>process</code> is called.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialIndenter(Indenter):\n    \"\"\"An `Indenter` that doesn't reset its state every time `process` is called.\"\"\"\n\n    def process(self, stream):\n        return self._process(stream)\n\n    def _process(self, stream):\n        for token in stream:\n            # These were previously *after* the `yield`, but that makes the\n            # state tracking unnecessarily convoluted.\n            if token.type in self.OPEN_PAREN_types:\n                self.paren_level += 1\n            elif token.type in self.CLOSE_PAREN_types:\n                self.paren_level -= 1\n                if self.paren_level &lt; 0:\n                    raise UnexpectedToken(token, [])\n\n            if token.type == self.NL_type:\n                yield from self.handle_NL(token)\n            else:\n                yield token\n\n        # TODO: What do we want to do here?\n        # while len(self.indent_level) &gt; 1:\n        #     self.indent_level.pop()\n        #     yield Token(self.DEDENT_type, \"\")\n\n    def accepts_token_type(self, token_type):\n        if token_type in self.CLOSE_PAREN_types and self.paren_level - 1 &lt; 0:\n            return False\n\n        # TODO:\n        # if token_type == self.NL_type and self.paren_level == 0:\n        #     ...\n        #     return False\n\n        return True\n\n    def __copy__(self):\n        res = type(self)()\n        res.paren_level = self.paren_level\n        res.indent_level = copy(self.indent_level)\n        return res\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(paren_level={self.paren_level!r}, indent_level={self.indent_level!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState","title":"<code>PartialParserState</code>","text":"<p>               Bases: <code>ParserState</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParserState(ParserState):\n    __slots__ = \"use_value_stack\"\n\n    def __init__(\n        self,\n        parse_conf,\n        lexer,\n        state_stack=None,\n        value_stack=None,\n        use_value_stack=False,\n    ):\n        super().__init__(\n            parse_conf, lexer, state_stack=state_stack, value_stack=value_stack\n        )\n        self.use_value_stack = use_value_stack\n\n    def feed_token(self, token, is_end=False):\n        if token.type == \"partial\":\n            # If none of the potential terminals can transition, we need to know now\n            current_state = self.state_stack[-1]\n            current_lexer = get_contextual_lexer(self.lexer).lexers[current_state]\n\n            # We have to feed the token and determine whether or not at least\n            # one terminal is consistent with the stack; otherwise, we'll miss\n            # invalid REDUCE cases.\n            # TODO: We should track separate parses conditional on possible\n            # token/symbol types, then we can coherently reuse the following\n            # results instead of recomputing it later.\n            can_transition = False\n            for terminal_info in token.value.terminals_and_info:\n                if terminal_info.terminal_name not in current_lexer.ignore_types:\n                    test_token = Token.new_borrow_pos(\n                        terminal_info.terminal_name, \"\", token\n                    )\n\n                    stack = copy(self.state_stack)\n                    try:\n                        self.feed_token_no_stack(test_token, is_end=is_end)\n                        can_transition = True\n                        break\n                    except UnexpectedToken:\n                        continue\n                    finally:\n                        self.state_stack = stack\n                else:\n                    can_transition = True\n\n            if not can_transition:\n                expected = {\n                    s\n                    for s in self.parse_conf.states[current_state].keys()\n                    if s.isupper()\n                }\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n        elif self.use_value_stack:\n            super().feed_token(token, is_end=is_end)\n        else:\n            self.feed_token_no_stack(token, is_end=is_end)\n\n    def feed_token_no_stack(self, token, is_end=False):\n        \"\"\"\n        This is a copy of `ParserState.feed_token` with all the value stack\n        steps removed.  Since we're not exactly parsing in order to obtain a\n        CST or anything similar, we can avoid the growing expense of tracking\n        the parse tree.\n        \"\"\"\n        state_stack = self.state_stack\n        states = self.parse_conf.states\n        end_state = self.parse_conf.end_state\n\n        while True:\n            state = state_stack[-1]\n            try:\n                action, arg = states[state][token.type]\n            except KeyError:\n                expected = {s for s in states[state].keys() if s.isupper()}\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n            assert arg != end_state\n\n            if action is Shift:\n                # shift once and return\n                assert not is_end\n                state_stack.append(arg)\n                return\n            else:\n                # reduce+shift as many times as necessary\n                rule = arg\n                size = len(rule.expansion)\n                if size:\n                    del state_stack[-size:]\n\n                _action, new_state = states[state_stack[-1]][rule.origin.name]\n                assert _action is Shift\n                state_stack.append(new_state)\n\n                if is_end and state_stack[-1] == end_state:\n                    return\n\n    def feed_eof(self):\n        last_token = self.lexer.state.last_token\n\n        if last_token is None:\n            eof_token = self.lexer._Token(\"$END\", \"\", 0, 1, 1)\n        else:\n            eof_token = Token.new_borrow_pos(\"$END\", \"\", last_token)\n\n        new_token_is_legal = (\n            last_token is None\n            or last_token.type != \"partial\"\n            or any(ti.is_final for ti in last_token.value.terminals_and_info)\n        )\n        if new_token_is_legal:\n            self.feed_token(eof_token, is_end=True)\n        else:\n            raise UnexpectedToken(eof_token, [], state=self, interactive_parser=None)\n\n    def choices(self):\n        return self.parse_conf.parse_table.states[self.position]\n\n    def accepts(self):\n        \"\"\"\n        Adapted from https://github.com/lark-parser/lark/blob/be542c2ff6d968817df019b8bf03f37b3111c08c/lark/parsers/lalr_interactive_parser.py#L95\n        Returns the set of possible tokens that will advance the parser into a new valid state.\n        \"\"\"\n        accepts = set()\n        conf_no_callbacks = copy(self.parse_conf)\n        # We don't want to call callbacks here since those might have arbitrary side effects\n        # and are unnecessarily slow.\n        conf_no_callbacks.callbacks = {}\n        for t in self.choices():\n            if t.isupper():  # is terminal?\n                new_state = copy(self)\n                new_state.parse_conf = conf_no_callbacks\n                try:\n                    new_state.feed_token(new_state.lexer._Token(t, \"\"))\n                except UnexpectedToken:\n                    pass\n                else:\n                    accepts.add(t)\n        return accepts\n\n    def __copy__(self):\n        return type(self)(\n            self.parse_conf,\n            copy(self.lexer),\n            copy(self.state_stack),\n            deepcopy(self.value_stack),\n            use_value_stack=self.use_value_stack,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(lexer={self.lexer!r}, state_stack={self.state_stack!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState.accepts","title":"<code>accepts()</code>","text":"<p>Adapted from https://github.com/lark-parser/lark/blob/be542c2ff6d968817df019b8bf03f37b3111c08c/lark/parsers/lalr_interactive_parser.py#L95 Returns the set of possible tokens that will advance the parser into a new valid state.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def accepts(self):\n    \"\"\"\n    Adapted from https://github.com/lark-parser/lark/blob/be542c2ff6d968817df019b8bf03f37b3111c08c/lark/parsers/lalr_interactive_parser.py#L95\n    Returns the set of possible tokens that will advance the parser into a new valid state.\n    \"\"\"\n    accepts = set()\n    conf_no_callbacks = copy(self.parse_conf)\n    # We don't want to call callbacks here since those might have arbitrary side effects\n    # and are unnecessarily slow.\n    conf_no_callbacks.callbacks = {}\n    for t in self.choices():\n        if t.isupper():  # is terminal?\n            new_state = copy(self)\n            new_state.parse_conf = conf_no_callbacks\n            try:\n                new_state.feed_token(new_state.lexer._Token(t, \"\"))\n            except UnexpectedToken:\n                pass\n            else:\n                accepts.add(t)\n    return accepts\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState.feed_token_no_stack","title":"<code>feed_token_no_stack(token, is_end=False)</code>","text":"<p>This is a copy of <code>ParserState.feed_token</code> with all the value stack steps removed.  Since we're not exactly parsing in order to obtain a CST or anything similar, we can avoid the growing expense of tracking the parse tree.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def feed_token_no_stack(self, token, is_end=False):\n    \"\"\"\n    This is a copy of `ParserState.feed_token` with all the value stack\n    steps removed.  Since we're not exactly parsing in order to obtain a\n    CST or anything similar, we can avoid the growing expense of tracking\n    the parse tree.\n    \"\"\"\n    state_stack = self.state_stack\n    states = self.parse_conf.states\n    end_state = self.parse_conf.end_state\n\n    while True:\n        state = state_stack[-1]\n        try:\n            action, arg = states[state][token.type]\n        except KeyError:\n            expected = {s for s in states[state].keys() if s.isupper()}\n            raise UnexpectedToken(\n                token, expected, state=self, interactive_parser=None\n            )\n\n        assert arg != end_state\n\n        if action is Shift:\n            # shift once and return\n            assert not is_end\n            state_stack.append(arg)\n            return\n        else:\n            # reduce+shift as many times as necessary\n            rule = arg\n            size = len(rule.expansion)\n            if size:\n                del state_stack[-size:]\n\n            _action, new_state = states[state_stack[-1]][rule.origin.name]\n            assert _action is Shift\n            state_stack.append(new_state)\n\n            if is_end and state_stack[-1] == end_state:\n                return\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParsingFrontend","title":"<code>PartialParsingFrontend</code>","text":"<p>               Bases: <code>ParsingFrontend</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParsingFrontend(ParsingFrontend):\n    def __init__(self, lexer_conf, parser_conf, options, parser=None):\n        assert parser_conf.parser_type == \"lalr\"\n\n        options._plugins[\"LALR_Parser\"] = PartialLALRParser\n        options._plugins[\"BasicLexer\"] = PartialBasicLexer\n        options._plugins[\"ContextualLexer\"] = PartialContextualLexer\n        options._plugins[\"LexerThread\"] = PartialLexerThread\n\n        super().__init__(lexer_conf, parser_conf, options, parser=parser)\n\n        if lexer_conf.postlex:\n            self.lexer = PartialPostLexConnector(self.lexer.lexer, lexer_conf.postlex)\n\n        self._termset_fsm_info = None\n        self._symbols_to_states: Optional[\n            Dict[str, Set[Tuple[ParseStateType, Action]]]\n        ] = None\n        self._reverse_shifts: Optional[\n            Dict[ParseStateType, Dict[str, Set[ParseStateType]]]\n        ] = None\n        # self._state_transition_map: Optional[\n        #     Dict[Tuple[ParseStateType, str], Set[ParseStateType]]\n        # ] = None\n\n    def _compute_maps(\n        self,\n    ):\n        \"\"\"Compute state transition and symbols-to-states maps.\"\"\"\n        self._reverse_shifts = {}\n        self._symbols_to_states = {}\n\n        parse_table = self.parser.parser.parse_table\n\n        for from_state, symbols_to_ops in parse_table.states.items():\n            for symbol, op in symbols_to_ops.items():\n                if op[0] == Shift:\n                    symbols_to_from_states = self._reverse_shifts.setdefault(op[1], {})\n                    symbols_to_from_states.setdefault(symbol, set()).add(from_state)\n                self._symbols_to_states.setdefault(symbol, set()).add((from_state, op))\n\n        # # TODO: This approach is very wasteful.\n        # context_lexer = get_contextual_lexer(self)\n        # self._state_transition_map = {}\n        #\n        # for from_state, transitions in parse_table.states.items():\n        #     for symbol, action in transitions.items():\n        #         # TODO: Filter non-terminals\n        #         if symbol not in context_lexer.root_lexer.terminals_by_name:\n        #             continue\n        #\n        #         if action[0] is Shift:\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(action[1])\n        #             continue\n        #\n        #         antecedent_state_seqs = parse_to_terminal(self, [(from_state,)], symbol)\n        #\n        #         for antecedent_state_seq in antecedent_state_seqs:\n        #             antecedent_state = antecedent_state_seq[-1]\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(antecedent_state)\n\n    def _compute_termset_fsm_info(self):\n        \"\"\"Collect and return information about terminal symbol sets and their FSMs.\n\n        Terminal symbol sets (or \"termsets\") are ordered sequences of terminal\n        symbols that are used by each parser state.  Associated with each is a\n        collection of FSMs for each terminal and a single parse state FSM that is\n        the union of each terminal's FSM.\n\n        This constructs a list of tuples containing the termset, the set of\n        parse states that use the termsets, parse state FSMs, and information\n        mapping the components of the parse state FSMs to their terminal symbol\n        FSMs.\n\n        \"\"\"\n        context_lexer = get_contextual_lexer(self)\n        termsets_to_fsms = {}\n        termsets_to_parse_states: Dict[Tuple[str, ...], Set[ParseStateType]] = {}\n        for parse_state, lexer in context_lexer.lexers.items():\n            scanner = lexer.scanner\n            key = tuple(term.name for term in scanner.terminals)\n            termsets_to_fsms[key] = (scanner.fsm, scanner.fsms_to_trans_finals)\n            termsets_to_parse_states.setdefault(key, set()).add(parse_state)\n\n        self._termset_fsm_info = [\n            (\n                termset,\n                frozenset(termsets_to_parse_states[termset]),\n                fsm,\n                fsms_to_trans_finals,\n            )\n            for termset, (fsm, fsms_to_trans_finals) in termsets_to_fsms.items()\n        ]\n\n    @property\n    def termset_fsm_info(self):\n        if self._termset_fsm_info is None:\n            self._compute_termset_fsm_info()\n        return self._termset_fsm_info\n\n    @property\n    def symbols_to_states(self):\n        if self._symbols_to_states is None:\n            self._compute_maps()\n        return self._symbols_to_states\n\n    @property\n    def reverse_shifts(self):\n        if self._reverse_shifts is None:\n            self._compute_maps()\n        return self._reverse_shifts\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner","title":"<code>PartialScanner</code>","text":"<p>               Bases: <code>Scanner</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialScanner(Scanner):\n    @classmethod\n    @lru_cache\n    def construct_terminal_fsm(cls, terminal):\n        # TODO: This should really be done at the lexer/parser level so that\n        # the lifetime of these objects is tied to the parser itself.\n        regex_str = terminal.pattern.to_regexp()\n        pattern = interegular.parse_pattern(regex_str)\n        fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        return fsm, pattern.prefix_postfix\n\n    def __init__(self, terminals, g_regex_flags, re_, use_bytes, match_whole=False):\n        self.terminals = terminals\n        self.g_regex_flags = g_regex_flags\n        self.use_bytes = use_bytes\n        self.match_whole = match_whole\n        self.allowed_types = {t.name for t in self.terminals}\n        self._mres = None\n\n        fsms = []\n        for t in self.terminals:\n            fsm, prefix_postfix = self.construct_terminal_fsm(t)\n\n            # TODO FIXME: We don't support this right now.\n            assert prefix_postfix == (0, 0)\n\n            fsms.append(fsm)\n\n        self.fsm, self.fsms_to_trans_finals = fsm_union(fsms)\n\n    def get_terminals_info(\n        self, fsm_state_seq\n    ) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n        \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n        terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n            get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n        ):\n            terminal_name = self.terminals[fsm_id].name\n            info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n            terminals_and_info += (info,)\n            if in_final:\n                final_terminals_and_info += (info,)\n\n        return terminals_and_info, final_terminals_and_info\n\n    def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n        \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n        start_pos = pos\n\n        if last_fsm_state_seq:\n            assert len(last_fsm_state_seq) &gt; 1\n            start_pos += len(last_fsm_state_seq) - 1\n            start_state = last_fsm_state_seq[-1]\n        else:\n            start_state = self.fsm.initial\n\n        text_part = text[start_pos:]\n\n        text_transitions = get_token_transition_keys(\n            self.fsm.fsm_info.alphabet_symbol_mapping,\n            self.fsm.fsm_info.alphabet_anything_value,\n            text_part,\n        )\n\n        state_seq = walk_fsm(\n            self.fsm,\n            text_transitions,\n            start_state,\n            full_match=self.match_whole,\n        )\n\n        if not state_seq:\n            return None\n\n        if last_fsm_state_seq:\n            res = last_fsm_state_seq + tuple(state_seq)\n        else:\n            res = (start_state,) + tuple(state_seq)\n\n        return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.get_terminals_info","title":"<code>get_terminals_info(fsm_state_seq)</code>","text":"<p>Get the possible terminal symbols for an FSM state sequence.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def get_terminals_info(\n    self, fsm_state_seq\n) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n    \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n    terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n        get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n    ):\n        terminal_name = self.terminals[fsm_id].name\n        info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n        terminals_and_info += (info,)\n        if in_final:\n            final_terminals_and_info += (info,)\n\n    return terminals_and_info, final_terminals_and_info\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.match","title":"<code>match(text, pos, last_fsm_state_seq=None)</code>","text":"<p>Determine an FSM match over <code>text</code> starting at <code>pos</code> and continuing <code>last_fsm_state_seq</code>.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n    \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n    start_pos = pos\n\n    if last_fsm_state_seq:\n        assert len(last_fsm_state_seq) &gt; 1\n        start_pos += len(last_fsm_state_seq) - 1\n        start_state = last_fsm_state_seq[-1]\n    else:\n        start_state = self.fsm.initial\n\n    text_part = text[start_pos:]\n\n    text_transitions = get_token_transition_keys(\n        self.fsm.fsm_info.alphabet_symbol_mapping,\n        self.fsm.fsm_info.alphabet_anything_value,\n        text_part,\n    )\n\n    state_seq = walk_fsm(\n        self.fsm,\n        text_transitions,\n        start_state,\n        full_match=self.match_whole,\n    )\n\n    if not state_seq:\n        return None\n\n    if last_fsm_state_seq:\n        res = last_fsm_state_seq + tuple(state_seq)\n    else:\n        res = (start_state,) + tuple(state_seq)\n\n    return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.terminals_to_fsms","title":"<code>terminals_to_fsms(lp)</code>","text":"<p>Construct a <code>dict</code> mapping terminal symbol names to their finite state machines.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def terminals_to_fsms(lp: PartialLark) -&gt; Dict[str, FSM]:\n    \"\"\"Construct a ``dict`` mapping terminal symbol names to their finite state machines.\"\"\"\n\n    symbol_names_and_fsms = {}\n    for terminal in lp.terminals:\n        pattern = interegular.parse_pattern(terminal.pattern.to_regexp())\n        # TODO: Use `pyparser.terminals[0].pattern.flags`?\n        try:\n            fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        except Unsupported:\n            fsm = None\n\n        symbol_names_and_fsms[terminal.name] = fsm\n\n    return symbol_names_and_fsms\n</code></pre>"},{"location":"api/prompts/","title":"Prompts","text":""},{"location":"api/prompts/#outlines.prompts.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>Represents a prompt function.</p> <p>We return a <code>Prompt</code> class instead of a simple function so the template defined in prompt functions can be accessed.</p> Source code in <code>outlines/prompts.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"Represents a prompt function.\n\n    We return a `Prompt` class instead of a simple function so the\n    template defined in prompt functions can be accessed.\n\n    \"\"\"\n\n    template: str\n    signature: inspect.Signature\n\n    def __post_init__(self):\n        self.parameters: List[str] = list(self.signature.parameters.keys())\n        self.jinja_environment = create_jinja_template(self.template)\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        The rendered template as a Python ``str``.\n\n        \"\"\"\n        bound_arguments = self.signature.bind(*args, **kwargs)\n        bound_arguments.apply_defaults()\n        return self.jinja_environment.render(**bound_arguments.arguments)\n\n    def __str__(self):\n        return self.template\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.Prompt.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p>"},{"location":"api/prompts/#outlines.prompts.Prompt.__call__--returns","title":"Returns","text":"<p>The rendered template as a Python <code>str</code>.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    The rendered template as a Python ``str``.\n\n    \"\"\"\n    bound_arguments = self.signature.bind(*args, **kwargs)\n    bound_arguments.apply_defaults()\n    return self.jinja_environment.render(**bound_arguments.arguments)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_args","title":"<code>get_fn_args(fn)</code>","text":"<p>Returns the arguments of a function with annotations and default values if provided.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_args(fn: Callable):\n    \"\"\"Returns the arguments of a function with annotations and default values if provided.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `args` filter only applies to callables.\")\n\n    arg_str_list = []\n    signature = inspect.signature(fn)\n    arg_str_list = [str(param) for param in signature.parameters.values()]\n    arg_str = \", \".join(arg_str_list)\n    return arg_str\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/prompts.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/prompts.py</code> <pre><code>@get_schema.register(type(BaseModel))\ndef get_schema_pydantic(model: Type[BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if not type(model) == type(BaseModel):\n        raise TypeError(\"The `schema` filter only applies to Pydantic models.\")\n\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value:\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.prompt","title":"<code>prompt(fn)</code>","text":"<p>Decorate a function that contains a prompt template.</p> <p>This allows to define prompts in the docstring of a function and simplify their manipulation by providing some degree of encapsulation. It uses the <code>render</code> function internally to render templates.</p> <p>import outlines</p> <p>@outlines.prompt def build_prompt(question): ...    \"I have a ${question}\" ... prompt = build_prompt(\"How are you?\")</p> <p>This API can also be helpful in an \"agent\" context where parts of the prompt are set when the agent is initialized and never modified later. In this situation we can partially apply the prompt function at initialization.</p> <p>import outlines import functools as ft ... @outlines.prompt ... def solve_task(name: str, objective: str, task: str): ...     '''Your name is {{name}}. ..      Your overall objective is to {{objective}}. ...     Please solve the following task: {{task}} ...     ''' ... hal = ft.partial(solve_task, \"HAL\", \"Travel to Jupiter\")</p>"},{"location":"api/prompts/#outlines.prompts.prompt--returns","title":"Returns","text":"<p>A <code>Prompt</code> callable class which will render the template when called.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def prompt(fn: Callable) -&gt; Prompt:\n    \"\"\"Decorate a function that contains a prompt template.\n\n    This allows to define prompts in the docstring of a function and simplify their\n    manipulation by providing some degree of encapsulation. It uses the `render`\n    function internally to render templates.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @outlines.prompt\n    &gt;&gt;&gt; def build_prompt(question):\n    ...    \"I have a ${question}\"\n    ...\n    &gt;&gt;&gt; prompt = build_prompt(\"How are you?\")\n\n    This API can also be helpful in an \"agent\" context where parts of the prompt\n    are set when the agent is initialized and never modified later. In this situation\n    we can partially apply the prompt function at initialization.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; import functools as ft\n    ...\n    &gt;&gt;&gt; @outlines.prompt\n    ... def solve_task(name: str, objective: str, task: str):\n    ...     '''Your name is {{name}}.\n    ..      Your overall objective is to {{objective}}.\n    ...     Please solve the following task: {{task}}\n    ...     '''\n    ...\n    &gt;&gt;&gt; hal = ft.partial(solve_task, \"HAL\", \"Travel to Jupiter\")\n\n    Returns\n    -------\n    A `Prompt` callable class which will render the template when called.\n\n    \"\"\"\n\n    signature = inspect.signature(fn)\n\n    # The docstring contains the template that will be rendered to be used\n    # as a prompt to the language model.\n    docstring = fn.__doc__\n    if docstring is None:\n        raise TypeError(\"Could not find a template in the function's docstring.\")\n\n    template = cast(str, docstring)\n\n    return Prompt(template, signature)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.render","title":"<code>render(template, **values)</code>","text":"<p>Parse a Jinaj2 template and translate it into an Outlines graph.</p> <p>This function removes extra whitespaces and linebreaks from templates to allow users to enter prompts more naturally than if they used Python's constructs directly. See the examples for a detailed explanation.</p>"},{"location":"api/prompts/#outlines.prompts.render--examples","title":"Examples","text":"<p>Outlines follow Jinja2's syntax</p> <p>import outlines outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\") I like tomatoes and tennis</p> <p>If the first line of the template is empty, <code>render</code> removes it</p> <p>from outlines import render</p> <p>tpl = ''' ... A new string''' tpl ... '\\nA new string' render(tpl) ... 'a new string'</p> <p>Similarly, <code>render</code> ignores linebreaks introduced by placing the closing quotes underneath the text:</p> <p>tpl = ''' ... A new string ... ''' tpl ... '\\nA new string\\n' render(tpl) ... 'A new string'</p> <p>If you want to insert a linebreak at the end of the rendered template, you will need to leave an empty line at the end of the template:</p> <p>tpl = ''' ... A new string ... ... ''' tpl ... '\\nA new string\\n\\n' render(tpl) ... 'A new string\\n'</p> <p><code>render</code> removes the identation in docstrings. This is particularly important when using prompt functions</p> <p>tpl = ''' ...    a string ...    and another string''' tpl ... '\\n   a string\\n   and another string' render(tpl) ... 'a string\\nand another string'</p> <p>The indentation of the first line is assumed to be the same as the second line's</p> <p>tpl = '''a string ...     and another''' tpl ... 'a string\\n    and another' render(tpl) ... 'a string\\nand another'</p> <p>To get a different indentation for the first and the second line, we can start the prompt on the string's second line:</p> <p>tpl = ''' ... First line ...   Second line''' render(tpl) ... 'First Line\\n  Second Line'</p>"},{"location":"api/prompts/#outlines.prompts.render--parameters","title":"Parameters","text":"<p>template     A string that contains a template written with the Jinja2 syntax. **values     Map from the variables in the template to their value.</p>"},{"location":"api/prompts/#outlines.prompts.render--returns","title":"Returns","text":"<p>A string that contains the rendered template.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def render(template: str, **values: Optional[Dict[str, Any]]) -&gt; str:\n    r\"\"\"Parse a Jinaj2 template and translate it into an Outlines graph.\n\n    This function removes extra whitespaces and linebreaks from templates to\n    allow users to enter prompts more naturally than if they used Python's\n    constructs directly. See the examples for a detailed explanation.\n\n    Examples\n    --------\n\n    Outlines follow Jinja2's syntax\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\")\n    I like tomatoes and tennis\n\n    If the first line of the template is empty, `render` removes it\n\n    &gt;&gt;&gt; from outlines import render\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tpl = '''\n    ... A new string'''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a new string'\n\n    Similarly, `render` ignores linebreaks introduced by placing the closing quotes\n    underneath the text:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string'\n\n    If you want to insert a linebreak at the end of the rendered template, you will\n    need to leave an empty line at the end of the template:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ...\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string\\n'\n\n    `render` removes the identation in docstrings. This is particularly important\n    when using prompt functions\n\n    &gt;&gt;&gt; tpl = '''\n    ...    a string\n    ...    and another string'''\n    &gt;&gt;&gt; tpl\n    ... '\\n   a string\\n   and another string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another string'\n\n    The indentation of the first line is assumed to be the same as the second line's\n\n    &gt;&gt;&gt; tpl = '''a string\n    ...     and another'''\n    &gt;&gt;&gt; tpl\n    ... 'a string\\n    and another'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another'\n\n    To get a different indentation for the first and the second line, we can start the\n    prompt on the string's second line:\n\n    &gt;&gt;&gt; tpl = '''\n    ... First line\n    ...   Second line'''\n    &gt;&gt;&gt; render(tpl)\n    ... 'First Line\\n  Second Line'\n\n    Parameters\n    ----------\n    template\n        A string that contains a template written with the Jinja2 syntax.\n    **values\n        Map from the variables in the template to their value.\n\n    Returns\n    -------\n    A string that contains the rendered template.\n\n    \"\"\"\n    jinja_template = create_jinja_template(template)\n    return jinja_template.render(**values)\n</code></pre>"},{"location":"api/regex/","title":"Regex","text":""},{"location":"api/regex/#outlines.generate.regex.regex","title":"<code>regex(model, regex_str, sampler=multinomial())</code>","text":"<p>Generate structured text in the language of a regular expression.</p>"},{"location":"api/regex/#outlines.generate.regex.regex--parameters","title":"Parameters","text":"<p>model:     An instance of <code>Transformer</code> that represents a model from the     <code>transformers</code> library. regex_str:     The regular expression that the output must follow. sampler:     The sampling algorithm to use to generate token ids from the logits     distribution.</p>"},{"location":"api/regex/#outlines.generate.regex.regex--returns","title":"Returns","text":"<p>A <code>SequenceGeneratorAdapter</code> instance that generates text constrained by the regular expression.</p> Source code in <code>outlines/generate/regex.py</code> <pre><code>@singledispatch\ndef regex(model, regex_str: str, sampler: Sampler = multinomial()):\n    \"\"\"Generate structured text in the language of a regular expression.\n\n    Parameters\n    ----------\n    model:\n        An instance of `Transformer` that represents a model from the\n        `transformers` library.\n    regex_str:\n        The regular expression that the output must follow.\n    sampler:\n        The sampling algorithm to use to generate token ids from the logits\n        distribution.\n\n    Returns\n    -------\n    A `SequenceGeneratorAdapter` instance that generates text constrained by the\n    regular expression.\n\n    \"\"\"\n    from outlines.processors import RegexLogitsProcessor\n\n    logits_processor = RegexLogitsProcessor(regex_str, tokenizer=model.tokenizer)\n    return SequenceGeneratorAdapter(model, logits_processor, sampler)\n</code></pre>"},{"location":"api/samplers/","title":"Samplers","text":""},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler","title":"<code>BeamSearchSampler</code>","text":"<p>Beam Search sampling algorithm.</p>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler--attributes","title":"Attributes","text":"<p>samples     The number of samples taken for each input sequence. Equivalent to the     number of beams.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class BeamSearchSampler:\n    \"\"\"Beam Search sampling algorithm.\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence. Equivalent to the\n        number of beams.\n    \"\"\"\n\n    def __init__(self, beams: int = 1):\n        self.samples = beams\n\n    def __call__(\n        self,\n        next_token_logits: \"torch.DoubleTensor\",\n        sequence_weights: \"torch.DoubleTensor\",\n        _,\n    ) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n        \"\"\"Call the beam search sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        import torch\n\n        logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n        weights = logprobs + sequence_weights.unsqueeze(1).expand_as(next_token_logits)\n\n        # Flatten scores to (n_batch, n_samples * vocab_size)\n        # and find the top-k weights for each batch.\n        batch_size = next_token_logits.shape[0] // self.samples\n        vocab_size = next_token_logits.shape[-1]\n        weights = weights.view(batch_size, self.samples * vocab_size)\n\n        # If the weights are all equal to 0 we are at the beginning of the search\n        # and thus only need to sample from one set of token logits for each\n        # batch.\n        if torch.all(sequence_weights == 0):\n            weights = weights[:, :vocab_size]\n\n        weights, indices = torch.topk(\n            weights, self.samples, dim=1, largest=True, sorted=True\n        )\n\n        ancestors = torch.div(indices, vocab_size, rounding_mode=\"floor\")\n        next_token_ids = indices % vocab_size\n\n        # Re-shape the weights, next_token_ids and ancestors to (n_batch * n_samples, 1)\n        first_batch_idx = torch.arange(\n            0, batch_size * self.samples, self.samples, device=next_token_logits.device\n        ).unsqueeze(1)\n        ancestors = ancestors + first_batch_idx\n\n        ancestors = ancestors.view(self.samples * batch_size)\n        weights = weights.view(self.samples * batch_size)\n        next_token_ids = next_token_ids.view(self.samples * batch_size, 1)\n\n        return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, _)</code>","text":"<p>Call the beam search sampler.</p>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__--parameters","title":"Parameters","text":"<p>next_token_logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. sequence_weights     A tensor of shape <code>(n_seqs,)</code> that represents the cumulative     weight of each sequence. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__--returns","title":"Returns","text":"<p>A tuple with an array that contains the ids of the sampled tokens of shape <code>(n_seqs, 1)</code>, an array that contains the ancestors of each sampled id of shape <code>(n_seqs,)</code> and an array that contains the updated cumulative weights of each sequence of shape <code>(n_seqs,)</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: \"torch.DoubleTensor\",\n    sequence_weights: \"torch.DoubleTensor\",\n    _,\n) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n    \"\"\"Call the beam search sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    import torch\n\n    logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n    weights = logprobs + sequence_weights.unsqueeze(1).expand_as(next_token_logits)\n\n    # Flatten scores to (n_batch, n_samples * vocab_size)\n    # and find the top-k weights for each batch.\n    batch_size = next_token_logits.shape[0] // self.samples\n    vocab_size = next_token_logits.shape[-1]\n    weights = weights.view(batch_size, self.samples * vocab_size)\n\n    # If the weights are all equal to 0 we are at the beginning of the search\n    # and thus only need to sample from one set of token logits for each\n    # batch.\n    if torch.all(sequence_weights == 0):\n        weights = weights[:, :vocab_size]\n\n    weights, indices = torch.topk(\n        weights, self.samples, dim=1, largest=True, sorted=True\n    )\n\n    ancestors = torch.div(indices, vocab_size, rounding_mode=\"floor\")\n    next_token_ids = indices % vocab_size\n\n    # Re-shape the weights, next_token_ids and ancestors to (n_batch * n_samples, 1)\n    first_batch_idx = torch.arange(\n        0, batch_size * self.samples, self.samples, device=next_token_logits.device\n    ).unsqueeze(1)\n    ancestors = ancestors + first_batch_idx\n\n    ancestors = ancestors.view(self.samples * batch_size)\n    weights = weights.view(self.samples * batch_size)\n    next_token_ids = next_token_ids.view(self.samples * batch_size, 1)\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.GreedySampler","title":"<code>GreedySampler</code>","text":"<p>Greedy Sampling algorithm.</p> <p>Greedy sampling consists in choosing the token with the largest likelihood at every step.</p> <p>We don't allow more than one sample. We could attribute this a meaning, for instance the k-th sample represents the k-th most likely token. In which case it would be equivalent to beam search without the sequence weights.</p>"},{"location":"api/samplers/#outlines.samplers.GreedySampler--attributes","title":"Attributes","text":"<p>samples     The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class GreedySampler:\n    \"\"\"Greedy Sampling algorithm.\n\n    Greedy sampling consists in choosing the token with the largest\n    likelihood at every step.\n\n    We don't allow more than one sample. We could attribute this a meaning, for\n    instance the k-th sample represents the k-th most likely token. In which\n    case it would be equivalent to beam search without the sequence weights.\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(self):\n        self.samples = 1\n\n    def __call__(\n        self,\n        next_token_logits: \"torch.DoubleTensor\",\n        sequence_weights: \"torch.DoubleTensor\",\n        _,\n    ) -&gt; \"torch.DoubleTensor\":\n        \"\"\"Call the greedy sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        import torch\n\n        logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n        next_token_ids = torch.argmax(logprobs, dim=-1, keepdim=True)\n\n        ancestors = torch.arange(\n            next_token_logits.shape[0], device=next_token_logits.device\n        )\n        weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n        return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, _)</code>","text":"<p>Call the greedy sampler.</p>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__--parameters","title":"Parameters","text":"<p>next_token_logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. sequence_weights     A tensor of shape <code>(n_seqs,)</code> that represents the cumulative     weight of each sequence. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__--returns","title":"Returns","text":"<p>A tuple with an array that contains the ids of the sampled tokens of shape <code>(n_seqs, 1)</code>, an array that contains the ancestors of each sampled id of shape <code>(n_seqs,)</code> and an array that contains the updated cumulative weights of each sequence of shape <code>(n_seqs,)</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: \"torch.DoubleTensor\",\n    sequence_weights: \"torch.DoubleTensor\",\n    _,\n) -&gt; \"torch.DoubleTensor\":\n    \"\"\"Call the greedy sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    import torch\n\n    logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n    next_token_ids = torch.argmax(logprobs, dim=-1, keepdim=True)\n\n    ancestors = torch.arange(\n        next_token_logits.shape[0], device=next_token_logits.device\n    )\n    weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler","title":"<code>MultinomialSampler</code>","text":"<p>Multinomial sampling algorithm.</p> <p>Multinomial sampling consists in randomly sampling the next token assuming its distribution is a Categorical distribution parametrized by the next-token logits.</p>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler--attributes","title":"Attributes","text":"<p>samples     The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class MultinomialSampler:\n    \"\"\"Multinomial sampling algorithm.\n\n    Multinomial sampling consists in randomly sampling the next token assuming\n    its distribution is a Categorical distribution parametrized by the\n    next-token logits.\n\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        samples: int = 1,\n        *,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        temperature: Optional[float] = None,\n    ):\n        self.samples = samples\n        self.top_k = top_k\n        self.top_p = top_p\n        self.temperature = temperature\n\n        self.logits_processors = []\n        if top_k is not None:\n            self.logits_processors.append(keep_top_k_logits(top_k))\n        elif top_p is not None:\n            self.logits_processors.append(keep_top_p_logits(top_p))\n\n        if temperature is not None:\n            self.logits_processors.append(rescale_logits(temperature))\n\n    def __call__(\n        self,\n        next_token_logits: \"torch.DoubleTensor\",\n        sequence_weights: \"torch.DoubleTensor\",\n        rng: \"torch.Generator\",\n    ) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n        \"\"\"Call the multinomial sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        import torch\n\n        altered_next_token_logits = next_token_logits\n        for logit_processor in self.logits_processors:\n            altered_next_token_logits = logit_processor(next_token_logits)\n\n        probs = torch.nn.functional.softmax(altered_next_token_logits, dim=-1)\n        next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n\n        logprobs = torch.nn.functional.log_softmax(altered_next_token_logits, dim=-1)\n        ancestors = torch.arange(\n            altered_next_token_logits.shape[0], device=next_token_logits.device\n        )\n        weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n        return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, rng)</code>","text":"<p>Call the multinomial sampler.</p>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__--parameters","title":"Parameters","text":"<p>next_token_logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. sequence_weights     A tensor of shape <code>(n_seqs,)</code> that represents the cumulative     weight of each sequence. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__--returns","title":"Returns","text":"<p>A tuple with an array that contains the ids of the sampled tokens of shape <code>(n_seqs, 1)</code>, an array that contains the ancestors of each sampled id of shape <code>(n_seqs,)</code> and an array that contains the updated cumulative weights of each sequence of shape <code>(n_seqs,)</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: \"torch.DoubleTensor\",\n    sequence_weights: \"torch.DoubleTensor\",\n    rng: \"torch.Generator\",\n) -&gt; Tuple[\"torch.DoubleTensor\", \"torch.DoubleTensor\", \"torch.DoubleTensor\"]:\n    \"\"\"Call the multinomial sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    import torch\n\n    altered_next_token_logits = next_token_logits\n    for logit_processor in self.logits_processors:\n        altered_next_token_logits = logit_processor(next_token_logits)\n\n    probs = torch.nn.functional.softmax(altered_next_token_logits, dim=-1)\n    next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n\n    logprobs = torch.nn.functional.log_softmax(altered_next_token_logits, dim=-1)\n    ancestors = torch.arange(\n        altered_next_token_logits.shape[0], device=next_token_logits.device\n    )\n    weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.keep_top_k_logits","title":"<code>keep_top_k_logits(k)</code>","text":"<p>Build a function that masks logits values smaller than the top <code>k</code> ones.</p>"},{"location":"api/samplers/#outlines.samplers.keep_top_k_logits--parameters","title":"Parameters","text":"<p>k     The ranking below which logit values are replaced by <code>-math.inf</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def keep_top_k_logits(k: int) -&gt; Callable[[\"torch.Tensor\"], \"torch.Tensor\"]:\n    \"\"\"Build a function that masks logits values smaller than the top `k` ones.\n\n    Parameters\n    ----------\n    k\n        The ranking below which logit values are replaced by `-math.inf`.\n\n    \"\"\"\n    import torch\n\n    if not isinstance(k, int) or k &lt; 1:\n        raise ValueError(f\"`k` must be a strictly positive integers, got {k} instead.\")\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        num_to_keep = min(k, logits.size(-1))\n        mask_idx = logits &lt; torch.topk(logits, num_to_keep)[0][..., -1, None]\n        return logits.masked_fill(mask_idx, -math.inf)\n\n    return logits_processor\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.keep_top_p_logits","title":"<code>keep_top_p_logits(p)</code>","text":"<p>Build a function that masks the lowest probability tokens whose cumulative probability is below a certain threshold.</p>"},{"location":"api/samplers/#outlines.samplers.keep_top_p_logits--parameters","title":"Parameters","text":"<p>p     The value of the threshold. We keep the highest probability tokens whose     cumulative distribution is greater than or equal to <code>p</code> and mask the     others. Its value must be between 0 (excluded) and 1 (included).</p> Source code in <code>outlines/samplers.py</code> <pre><code>def keep_top_p_logits(p: float) -&gt; Callable[[\"torch.Tensor\"], \"torch.Tensor\"]:\n    \"\"\"Build a function that masks the lowest probability tokens whose\n    cumulative probability is below a certain threshold.\n\n    Parameters\n    ----------\n    p\n        The value of the threshold. We keep the highest probability tokens whose\n        cumulative distribution is greater than or equal to `p` and mask the\n        others. Its value must be between 0 (excluded) and 1 (included).\n\n    \"\"\"\n    import torch\n\n    if p &lt;= 0.0 or p &gt; 1.0:\n        raise ValueError(\n            f\"`p` must be a floating point number between 0 (excluded) and 1 (included), got {p} instead.\"\n        )\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        sorted_logits, sorted_idx = torch.sort(logits, descending=False)\n        cumulative_probabilties = torch.nn.functional.softmax(\n            sorted_logits, dim=-1\n        ).cumsum(dim=-1)\n\n        sorted_masked_idx = cumulative_probabilties &lt;= (1 - p)\n        mask_idx = torch.scatter(sorted_masked_idx, 1, sorted_idx, sorted_masked_idx)\n        return logits.masked_fill(mask_idx, -math.inf)\n\n    return logits_processor\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.rescale_logits","title":"<code>rescale_logits(temperature)</code>","text":"<p>Build a function that rescales the token probabilities exponentially.</p>"},{"location":"api/samplers/#outlines.samplers.rescale_logits--parameters","title":"Parameters","text":"<p>temperature     The value by which we rescale the logits.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def rescale_logits(temperature: float) -&gt; Callable[[\"torch.Tensor\"], \"torch.Tensor\"]:\n    \"\"\"Build a function that rescales the token probabilities exponentially.\n\n    Parameters\n    ----------\n    temperature\n        The value by which we rescale the logits.\n\n    \"\"\"\n\n    if not isinstance(temperature, float) or temperature &lt; 0.0:\n        raise ValueError(\n            f\"`temperature` must be a strictly positive floating point number, got {temperature} instead.\"\n        )\n    elif temperature == 0.0:\n        raise ValueError(\n            \"Please use the greedy sampler instead of setting the temperature to 0.\"\n        )\n\n    def logits_processor(logits: \"torch.Tensor\") -&gt; \"torch.Tensor\":\n        return logits / temperature\n\n    return logits_processor\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/01/10/roadmap-for-2024/","title":"Roadmap for 2024","text":"<p>Outlines is not even one year old and it's already gone a long way! As we just reached 4000 stars, and before laying out the roadmap for the following year, we would like to pause and thank all of you for supporting us, using and contributing to the library!</p> <p></p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#thoughts","title":"Thoughts","text":"<p>Before delving into the detailed roadmap, let me share a few thoughts and explain the general direction of the library. These thoughts are informed with my multiple interactions with users, either on Twitter or in our Discord server.</p> <p>Outlines currently differentiates itself from other libraries with its efficient JSON- and regex- constrained generation. A user-facing interface for grammar-structured generation (it had been hidden in the repository) was also recently added. But there is much more we can do along these lines. In 2024 will we will keep pushing in the direction of more accurate, faster constrained generation.</p> <p>Outlines also supports many models providers: <code>transformers</code>, <code>mamba</code>, <code>llama.cpp</code> and <code>exllama2</code>. Those integrations represent a lot of maintenance, and we will need to simplify them. For instance, <code>transformers</code> now supports quantized models, and we will soon deprecate the support for <code>autoawq</code> and <code>autogptq</code>. Thanks to a refactor of the library, it is now possible to use our constrained generation method by using logits processor with all other libraries, except <code>mamba</code>. We will look for libraries that provide state-space models and allow to pass a logits processor during inference. We will interface with <code>llama.cpp</code> and <code>exllama2</code> using logits processors.</p> <p>We would like expand our work to the whole sampling layer, and add new sampling methods that should make structured generation more accurate. This means we will keep the <code>transformers</code> integration as it is today and will expand our text generation logic around this library.</p> <p>Making workflows re-usable and easy to share is difficult today. That is why we are big believers in outlines functions. We will keep improving the interface and adding examples.</p> <p>Finally, we want to add a CLI tool, <code>outlines serve</code>. This will allows you to either serve an API that does general constrained generation, or to serve Outlines function.</p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#detailed-roadmap","title":"Detailed roadmap","text":"<p>Here is a more detailed roadmap for the next 12 months. Outlines is a community effort, and we invite you to pick either topic and contribute to the library. I will progressively add related issues in the repository.</p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#many-more-examples-and-tutorials","title":"Many more examples and tutorials","text":"<p>Let's be honest, Outlines is lacking clear and thorough examples. We want to change this!</p> <ul> <li>How does Outlines work? What can you do with it?</li> <li>What can you do with Outlines that is harder or impossible to do with other libraries?</li> <li>How you can perform standard LLM workflows, for instance Chain of Thoughts, Tree of Thoughts, etc?</li> <li>How does Oultines integrates with the larger ecosystem, for instance other libraries like LangChain and LlamaIndex?</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#simplify-the-integrations","title":"Simplify the integrations","text":"<p>We want to keep the current integrations but lower the maintenance cost so we can focus on what we bring to the table.</p> <ul> <li>Deprecate every obsolete integration: <code>transformers</code> has recently integrated <code>autoawq</code> and <code>autogptq</code> for instance. (PR)</li> <li>See if we can integrate to a library that provides state-space models via a logit processing function;</li> <li>Integrate with llama.cpp via a logits processor;</li> <li>Integrate with exllamav2 via a logits processor;</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#push-structured-generation-further","title":"Push structured generation further","text":"<p>We're just getting started!</p> <ul> <li>Improve the performance of existing structured generation algorithms;</li> <li>Improve the correctness of structured generation algorithms;</li> <li>Add ready-to-use grammars in the grammars repository or in a submodule in Outlines.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#keep-developing-outlines-functions","title":"Keep developing Outlines functions","text":"<p>Functions are awesome, use them!</p> <ul> <li>Implement a CLI <code>outlines serve</code> that allows to serve Outlines functions locally;</li> <li>Add more functions to the functions repository.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#serve-structured-generation","title":"Serve structured generation","text":"<p>We want to make it easier to serve structured generation and outlines functions.</p> <ul> <li>Implement the outlines serve CLI <code>outlines serve</code></li> <li>Serve local APIs that perform structured generation;</li> <li>Serve Outlines functions.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#improve-the-generation-layer","title":"Improve the generation layer","text":"<ul> <li>Use <code>transformers</code>'s private API to prepare inputs for generation inside the <code>Transformers</code> class;</li> <li>Support successions of model generation and text infilling for methods like Beam Search and SMC;</li> <li>Differentiate by adding new caching methods: attention sink, trie-based caching, etc;</li> <li>Differentiate by implementing SMC;</li> <li>Implement Beam Search;</li> <li>Add token healing.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#a-more-seamless-integration-with-openai","title":"A more seamless integration with OpenAI","text":"<ul> <li>Provide the same user interface for OpenAI and open source models so they are easily interchangeable;</li> <li>Integrate the function calling API.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#last-word","title":"Last word","text":"<p>This roadmap was influenced by the expressed interests of the community. If it doesn't reflect your needs please come and share your experience with us.</p>"},{"location":"community/","title":"Community","text":"<p>Outlines exists for a community of users who believe software doesn't need to be complicated. Who share the same passion for Large Language Models but don't want to compromise on robustness. Together, we are bringing these powerful models back to the world of software.</p>"},{"location":"community/#connect-on-discord","title":"Connect on Discord","text":"<p>The Outlines community lives on our Discord server. There you can ask questions, share ideas or just chat with people like you. Don't be a stranger and join us.</p>"},{"location":"community/contribute/","title":"Contribute","text":""},{"location":"community/contribute/#what-contributions","title":"What contributions?","text":"<ul> <li>Documentation contributions are very valuable to us!</li> <li>Examples. Show us what you did with Outlines :)</li> <li>Bug reports with a minimum working examples in the issue tracker</li> <li>Bug fixes are always a pleasure to review.</li> <li>New features. Please start a new discussion, or come chat with us beforehand!</li> </ul> <p>Note that the issue tracker is only intended for actionable items. In doubt, open a discussion or come talk to us.</p>"},{"location":"community/contribute/#how-to-contribute","title":"How to contribute?","text":""},{"location":"community/contribute/#setup","title":"Setup","text":"<p>First, fork the repository on GitHub and clone the fork locally:</p> <pre><code>git clone git@github.com/YourUserName/outlines.git\ncd outlines\n</code></pre> <p>Create a new virtual environment. If you are using conda:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>If you are using venv:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>Then install the dependencies in editable mode, and install the pre-commit hooks:</p> <pre><code>pip install -e \".[test]\"\npre-commit install\n</code></pre>"},{"location":"community/contribute/#before-pushing-your-code","title":"Before pushing your code","text":"<p>Run the tests:</p> <pre><code>pytest\n</code></pre> <p>And run the code style checks:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"community/contribute/#benchmarking","title":"Benchmarking","text":"<p>Outlines uses asv for automated benchmark testing. Benchmarks are run automatically before pull requests are merged to prevent performance degredation.</p> <p>You can run the benchmark test suite locally with the following command: <pre><code>asv run --config benchmarks/asv.conf.json\n</code></pre></p> <p>Caveats: - If you're on a device with CUDA, you must add the argument <code>--launch-method spawn</code> - Uncommitted code will not be benchmarked, you must first commit your changes.</p>"},{"location":"community/contribute/#run-a-specific-test","title":"Run a specific test:","text":"<pre><code>asv run --config benchmarks/asv.conf.json -b bench_json_schema.JsonSchemaBenchmark.time_json_schema_to_fsm\n</code></pre>"},{"location":"community/contribute/#profile-a-specific-test","title":"Profile a specific test:","text":"<pre><code>asv run --config benchmarks/asv.conf.json --profile -b bench_json_schema.JsonSchemaBenchmark.time_json_schema_to_fsm\n</code></pre>"},{"location":"community/contribute/#compare-to-originmain","title":"Compare to <code>origin/main</code>","text":"<pre><code>get fetch origin\nasv continuous origin/main HEAD --config benchmarks/asv.conf.json\n</code></pre>"},{"location":"community/contribute/#asv-pr-behavior","title":"ASV PR Behavior","text":"<ul> <li>View ASV Benchmark Results: Open the workflow, view <code>BENCHMARK RESULTS</code> section.</li> <li>Merging is blocked unless benchmarks are run for the latest commit.</li> <li>Benchmarks fail if performance degrades by more than 10% for any individual benchmark.</li> <li>The \"Benchmark PR\" workflow runs when its manually dispatched, or if the <code>run_benchmarks</code> label is added to the PR they run for every commit.</li> </ul>"},{"location":"community/contribute/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>To work on the documentation you will need to install the related dependencies:</p> <pre><code>pip install -r requirements-doc.txt\n</code></pre> <p>To build the documentation and serve it locally, run the following command in the repository's root folder:</p> <pre><code>mkdocs serve\n</code></pre> <p>By following the instruction you will be able to view the documentation locally. It will be updated every time you make a change.</p>"},{"location":"community/contribute/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Create a new branch on your fork, commit and push the changes:</p> <pre><code>git checkout -b new-branch\ngit add .\ngit commit -m \"Changes I made\"\ngit push origin new-branch\n</code></pre> <p>Then you can open a pull request on GitHub. It should prompt you to do so. Every subsequent change that you make on your branch will update the pull request.</p> <p>Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback. If you need help, come tell us on Discord.</p>"},{"location":"community/examples/","title":"Community projects and articles","text":"<p>Publishing examples and articles about Outlines are a meaningful way to contrinute to the community. Here is a list of projects we are aware of. Drop us a line if we forgot yours!</p> <p>MMSG is a Python library for generating interleaved text and image content in a structured format you can directly pass to downstream APIs.</p> <p>Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report shows that Structured Generation can outperform finetuning, and maybe even multimodality, in document-image understanding tasks as part of CVPR's 2nd MMFM Challenge.</p> <p>Chess LLM Arena is a HuggingFace Space where you can make LLMs compete in a chess match.</p> <p>LLM Data Gen is a HuggingFace Space that generates synthetic dataset files in JSONLines format.</p> <p>Fast, High-Fidelity LLM Decoding with Regex Constraints  presents an efficient alternative to Outlines's structured generation.</p> <p>gigax is an Open-Source library that allows to create real-time LLM-powered NPCs for video games.</p> <p>Improving Prompt Consistency with Structured Generations shows how structured generation can improve consistency of evaluation runs by reducing sensitivity to changes in prompt format.</p> <p>AskNews is a news curation service processing 300k news articles per day in a structured way, with Outlines.</p>"},{"location":"community/feedback/","title":"Feedback","text":"<p>If Outlines has been helpful to you, let us know on Discord or give us a shoutout on Twitter! It's always heartwarming \u2764\ufe0f</p> <p> <p></p> <p>I am once again reminding you that structured extraction using LLMs is going to transform every single industry in the next 10 years https://t.co/xQ3tcWnrZ8</p>\u2014 Sam Hogan (@0xSamHogan) April 17, 2024 <p>outline's growth is insane, using is an understatement! https://t.co/rHCNWhZdCs</p>\u2014 jason liu (@jxnlco) April 17, 2024 <p>Outlines is an amazing lib and more popular than @remilouf\u2019s modesty will admit. https://t.co/DfHbMPIlX1 https://t.co/mDHIWJrD0C</p>\u2014 Delip Rao e/\u03c3 (@deliprao) April 18, 2024 <p>Impressive implementation of a true regex / json / grammar guided text generation pic.twitter.com/RX5RVYaVIx</p>\u2014 Rohan Paul (@rohanpaul_ai) December 30, 2023 <p>Most underrated Github Repo in AI + LLM JSON guided Generation: https://t.co/lSB8KIet1H</p>\u2014 \ud83c\udf99Jean-Louis Queguiner (@JiliJeanlouis) December 18, 2023 <p>Nice and useful. https://t.co/LX72AE0lgt</p>\u2014 Dan Roy (@roydanroy) August 15, 2023 <p>HUGE dub for open source AI https://t.co/bYKuiEUZ1j</p>\u2014 kenneth \ud83d\udd87 (@k3nnethfrancis) August 15, 2023 <p>This is amazing - glad to see more outp guidance modules! Will try this out soon I'm wondering how they translate from regex automatons to token boundariesAlso why Open Source will succeed. Even today I don't see any guided output functionality from the big providers. https://t.co/Ity2H25Klf</p>\u2014 Hrishi (@hrishioa) August 14, 2023 <p>Outlines - a library to help LLM developers guide text generation in a fast and reliable way.\"Provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.\"Need to check this out. Reliable JSON output is a common use\u2026 pic.twitter.com/Bkbh8vKogN</p>\u2014 elvis (@omarsar0) August 14, 2023 <p>Woah this is cool! Makes open source models more usable.Give any LLM Function Call capability (and more) with Outlines: https://t.co/PtPykR5ZGR https://t.co/RRQjWHnIxv pic.twitter.com/BwNnH8SMwv</p>\u2014 Yohei (@yoheinakajima) August 14, 2023 <p>This is awesome! Being able to guarantee the output's structure unblocks so many applications. This is a great milestone and a fundamental building block for more advanced AI apps. https://t.co/WdwMOc7hE8</p>\u2014 Guilherme Castro (@skastr052) August 15, 2023 <p>Juggling with the unpredictable outputs of ChatGPT API lately while building my product. \ud83d\ude13 Tried prompt engineering to channel its wisdom into a neat JSON, but it's like asking a cat to fetch. \ud83d\udc31Luckily, stumbled upon \"Outlines\" \u2013 looks like a promising way to tame the LLM\u2026 pic.twitter.com/oYQ6q8exAS</p>\u2014 Charlie (@14435635Sun) August 15, 2023 <p>A complex system of LLM input-outputs interacting with non-LLM agents and models benefits immeasurably from structured outputs. The outlines package saves so much time, https://t.co/NhVQ6NpKDR</p>\u2014 Amir Sani (@amirsani) November 26, 2023"},{"location":"community/feedback/#let-us-know","title":"Let us know!","text":"<p>We highly value the insights of our users, and we would love to hear from you. If you are using Outlines for your projects and would like to share your experience with us, let's connect:</p> <ul> <li>What are you building with it?</li> <li>What do you like about it?</li> <li>What challenges are you facing?</li> <li>What do you think could be improved?</li> </ul> <p>To schedule an appointment follow this link. This is exclusively intended to share your experience, please go on Discord or GitHub for support.</p>"},{"location":"community/versioning/","title":"Versioning Guide","text":"<p>The Outlines project follows a structured versioning scheme designed to provide clarity and minimize risk for downstream dependents.</p> <p>Each part of the version number (<code>major.minor.patch</code>) conveys information about the nature and impact of the changes included in the release.</p> <ul> <li>Major Releases includes compatibility-breaking changes to core interfaces, such as <code>LogitsProcessor</code>s and <code>Guides</code>.</li> <li>Minor Releases introduce changes of substance to internal or unexposed functionality. These changes are well tested and intended to maintain compatability with existing use of core interfaces.</li> <li>Patch Releases address bug fixes and incorporate low-risk changes to improve stability and performance.</li> </ul>"},{"location":"community/versioning/#releases","title":"Releases","text":"<p>Releases along with release notes can be found on the Outlines Releases GitHub Page.</p>"},{"location":"community/versioning/#version-pinning-recommendations","title":"Version Pinning Recommendations","text":"<p>Here are our recommendations for managing dependencies on the Outlines package:</p> <p>Small, Risk-Tolerant Projects: Pin to a specific major version.</p> <p>Large, Conservative Projects: Pin to a specific minor version.</p>"},{"location":"cookbook/","title":"Examples","text":"<p>This part of the documentation provides a few cookbooks that you can browse to get acquainted with the library and get some inspiration about what you could do with structured generation. Remember that you can easily change the model that is being used!</p> <ul> <li>Classification: Classify customer requests.</li> <li>Named Entity Extraction: Extract information from pizza orders.</li> <li>Dating Profile: Build dating profiles from descriptions using prompt templating and JSON-structured generation.</li> <li>Chain Of Density: Summarize documents using chain of density prompting and JSON-structured generation.</li> <li>Playing Chess: Make Phi-3 Mini play chess against itself using regex-structured generation.</li> <li>SimToM: Improve LLMs' Theory of Mind capabilities with perspective-taking prompting and JSON-structured generation.</li> <li>Q&amp;A with Citations: Answer questions and provide citations using JSON-structured generation.</li> <li>Knowledge Graph Generation: Generate a Knowledge Graph from unstructured text using JSON-structured generation.</li> <li>Chain Of Thought (CoT): Generate a series of intermediate reasoning steps using regex-structured generation.</li> <li>ReAct Agent: Build an agent with open weights models using regex-structured generation.</li> </ul>"},{"location":"cookbook/chain_of_density/","title":"Summarize documents using Chain of Density prompting","text":"<p>A good summary should be informative, concise and clear. While large language models are generally good at summarizing documents, their summaries tend to be long and contain redundant information; their information density tends to be on the lower end. This is where chain of Density, a new prompting technique, comes in. In this example we will show how one can implement chain of density with a few lines of code using Outlines, leveraging both Outline's prompt templating and its structured generation capabilities.</p> <p>The article we will try to summarize is the first three paragraphs of the Alan Turing page on Wikipedia:</p> <pre><code>article = \"\"\"\nAlan Mathison Turing OBE FRS (/\u02c8tj\u028a\u0259r\u026a\u014b/; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[5] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[6][7][8] He is widely considered to be the father of theoretical computer science and artificial intelligence.[9]\n\nBorn in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes\u2013no questions can never be answered by computation. He defined a Turing machine and proved that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.[10][11]\n\nAfter the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[12] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.[13]\n\"\"\"\n</code></pre>"},{"location":"cookbook/chain_of_density/#how-chain-of-density-works","title":"How Chain Of Density works","text":"<p>Chain Of Density starts with asking the model to generate a first long and non-specific summary. Then it asks the model to generate 4 extra summaries by proceeding in the following way:</p> <ol> <li>Identify 1-3 entities missing in the previous summary;</li> <li>Add all entities marked as missing in the previous step, while not dropping entities;</li> <li>Make the summary more concise;</li> </ol> <p>The prompt also asks the model to return a list of JSON objects that contain the missing entities and the new summary. This is where structured generation will come in handy :) The paper provides the prompt and an example:</p> <p></p> <p>We can now implement the prompt provided in the paper:</p> <pre><code>import outlines\n\n@outlines.prompt\ndef chain_of_density(article):\n    \"\"\"Article: {{ article }}\n\n    You will generate increasingly concise, entity-dense summaries of the above Article.\n\n    Repeat the following 2 steps 5 times.\n\n    Step 1. Identify 1-3 informative Entities (\"; \" delimited) from the Article which are missing from the previously generated summary.\n    Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n\n    A Missing Entity is:\n    - Relevant: to the main story.\n    - Specific: descriptive yet concise (5 words or fewer).\n    - Novel: not in the previous summary.\n    - Faithful: present in the Article.\n    - Anywhere: located anywhere in the Article.\n\n    Guidelines:\n    - The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n    - Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n    - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n    - Missing entities can appear anywhere in the new summary.\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\n    Remember, use the exact same number of words for each summary.\n\n    Answer in JSON. The JSON should be a a dictionary with key \"summaries\" that contains a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n    \"\"\"\n</code></pre> Note <p>Note that we modified the prompt slightly so it returns a JSON object that contains the summaries, instead of a list of summaries.</p>"},{"location":"cookbook/chain_of_density/#outlines-implementation","title":"Outlines implementation","text":"<p>We will use Outline's JSON-structured generation to ensure that the model's output is consistent with the format specified in the prompt. We start with defining the JSON objects that the model is asked to return using Pydantic. One JSON object that contains a list of <code>Summary</code> objects that contain the missing entities and new summary:</p> <pre><code>from pydantic import BaseModel, conlist\n\nclass Summary(BaseModel):\n    missing_entities: str\n    denser_summary: str\n\nclass Summaries(BaseModel):\n    summaries: conlist(Summary, max_length=5, min_length=5)\n</code></pre> <p>We now generate the prompt by passing the article we want to summarize to the template. We load a quantized version of Mistral-7B using the AutoAWQ library, and then use JSON-structured generation to generate the summaries:</p> <pre><code>model = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\")\n\nprompt = chain_of_density(article)\nresult = outlines.generate.json(model, Summaries)(prompt)\n</code></pre> <p>We can now check the results:</p> <pre><code>print(result.model_dump())\n# {'summaries': [\n#     {\n#       'missing_entities': 'English mathematician, cryptanalyst, philosopher',\n#       'denser_summary': 'Alan Mathison Turing was an English mathematician, cryptanalyst, philosopher.'\n#     },\n#     {\n#       'missing_entities': '',\n#       'denser_summary': \"Alan Mathison Turing was an English mathematician who was a crucial figure in WW2's Bletchley Park codebreaking centre and designed one of the first computers.\"\n#     },\n#     {\n#       'missing_entities': 'cryptanalyst, studied, biology, father',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and contributed to mathematical biology.'\n#     },\n#     {\n#       'missing_entities': 'biology, morphogenesis, chemical',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and predicted chemical reactions in morphogenesis.\n#     '},\n#     {\n#       'missing_entities': '',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, developed computer science, and made strides in mathematical biology research.'\n#       }\n# ]}\n</code></pre> <p>Not bad, considering we used a smallish model to generate the summary! Chain of Density seems to be a very effective prompting technique to generate dense summaries, even with small quantized models. Its implementation in Outlines is also very short.</p> <p>Note that this is the first article I tried and it worked out of the box. Try it out on other articles, and please share the results on Twitter, or by opening a new discussion on the Outlines repository!</p>"},{"location":"cookbook/chain_of_thought/","title":"Chain of thought","text":"<p>Chain of thought is a prompting technique introduced in the paper \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" where throught prompting the authors generate a series of intermediate reasoning steps which improves the ability of LLMs to perform complex reasoning.</p> <p>In this guide, we use outlines to apply chain of thought through structured output.</p> <p>We use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/chain_of_thought/#chain-of-thought_1","title":"Chain of thought","text":"<p>We first define our Pydantic class for a reasoning step:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Reasoning_Step(BaseModel):\n    reasoning_step: str = Field(..., description=\"Reasoning step\")\n</code></pre> <p>We then define the Pydantic class for reasoning which will consist on a list of reasoning steps and a conclusion, and we get its JSON schema:</p> <pre><code>from typing import List\n\nclass Reasoning(BaseModel):\n    reasoning: List[Reasoning_Step] = Field(..., description=\"List of reasoning steps\")\n    conclusion: str = Field(..., description=\"Conclusion\")\n\njson_schema = Reasoning.model_json_schema()\n</code></pre> <p>We could generate a response using the json schema but for a change we will use the regex:</p> <pre><code>from outlines.integrations.utils import convert_json_schema_to_str\nfrom outlines.fsm.json_schema import build_regex_from_schema\n\nschema_str = convert_json_schema_to_str(json_schema=json_schema)\nregex_str = build_regex_from_schema(schema_str)\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>def generate_hermes_prompt(user_prompt):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{json_schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\\n\"\n        \"&lt;|im_start|&gt;user\\n\"\n        + user_prompt\n        + \"&lt;|im_end|&gt;\"\n        + \"\\n&lt;|im_start|&gt;assistant\\n\"\n        \"&lt;schema&gt;\"\n    )\n</code></pre> <p>For a given user prompt:</p> <pre><code>user_prompt = \"9.11 and 9.9 -- which is bigger?\"\n</code></pre> <p>we can use <code>generate.regex</code> by passing the Pydantic class we previously defined, and call the generator with the Hermes prompt:</p> <pre><code>generator = generate.regex(model, regex_str)\nprompt = generate_hermes_prompt(user_prompt)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n</code></pre> <p>We obtain a series of intermediate reasoning steps as well as the conclusion:</p> <pre><code>import json\n\njson_response = json.loads(response)\n\nprint(json_response[\"reasoning\"])\nprint(json_response[\"conclusion\"])\n# [{'reasoning_step': 'Both 9.11 and 9.9 are decimal numbers.'},\n#  {'reasoning_step': 'When comparing decimal numbers, we look at the numbers after the decimal point.'},\n#  {'reasoning_step': 'In this case, 9.11 has the number 1 after the decimal point, while 9.9 has the number 9.'},\n#  {'reasoning_step': 'Since 1 is greater than 9, 9.11 is greater than 9.9.'}]\n# '9.11 is bigger.'\n</code></pre> <p>We notice that the 4th reasoning step is wrong ``Since 1 is greater than 9, 9.11 is greater than 9.9.'', so we should probably give the model some examples for this particular task.</p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/classification/","title":"Classification","text":"<p>Classification is a classic problem in NLP and finds many applications: spam detection, sentiment analysis, triaging of incoming requests, etc. We will use the example of a company that wants to sort support requests between those that require immediate attention (<code>URGENT</code>), those that can wait a little (<code>STANDARD</code>). You could easily extend the example by adding new labels.</p> <p>This tutorial shows how one can implement multi-label classification using Outlines. We will use two functionalities of the library: <code>generate.choice</code> and <code>generate.json</code>.</p> <p>As always, we start with initializing the model. Since we are GPU poor we will be using a quantized version of Mistal-7B-v0.1:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre> <p>We will use the following prompt template:</p> <pre><code>@outlines.prompt\ndef customer_support(request):\n    \"\"\"You are an experienced customer success manager.\n\n    Given a request from a client, you need to determine when the\n    request is urgent using the label \"URGENT\" or when it can wait\n    a little with the label \"STANDARD\".\n\n    # Examples\n\n    Request: \"How are you?\"\n    Label: STANDARD\n\n    Request: \"I need this fixed immediately!\"\n    Label: URGENT\n\n    # TASK\n\n    Request: {{ request }}\n    Label: \"\"\"\n</code></pre>"},{"location":"cookbook/classification/#choosing-between-multiple-choices","title":"Choosing between multiple choices","text":"<p>Outlines provides a shortcut to do multi-label classification, using the <code>outlines.generate.choice</code> function to initialize a generator. Outlines uses multinomial sampling by default, here we will use the greedy sampler to get the label with the highest probability:</p> <p><pre><code>from outlines.samplers import greedy\n\ngenerator = outlines.generate.choice(model, [\"URGENT\", \"STANDARD\"], sampler=greedy())\n</code></pre> Outlines supports batched requests, so we will pass two requests to the model:</p> <pre><code>requests = [\n    \"My hair is one fire! Please help me!!!\",\n    \"Just wanted to say hi\"\n]\n\nprompts = [customer_support(request) for request in requests]\n</code></pre> <p>We can now asks the model to classify the requests:</p> <pre><code>labels = generator(prompts)\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre> <p>Now, you might be in a hurry and don't want to wait until the model finishes completion. After all, you only need to see the first letter of the response to know whether the request is urgent or standard. You can instead stream the response:</p> <pre><code>tokens = generator.stream(prompts)\nlabels = [\"URGENT\" if \"U\" in token else \"STANDARD\" for token in next(tokens)]\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre>"},{"location":"cookbook/classification/#using-json-structured-generation","title":"Using JSON-structured generation","text":"<p>Another (convoluted) way to do multi-label classification is to JSON-structured generation in Outlines. We first need to define our Pydantic schema that contains the labels:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\n\nclass Label(str, Enum):\n    urgent = \"URGENT\"\n    standard = \"STANDARD\"\n\n\nclass Classification(BaseModel):\n    label: Label\n</code></pre> <p>and we can use <code>generate.json</code> by passing this Pydantic model we just defined, and call the generator:</p> <pre><code>generator = outlines.generate.json(model, Classification, sampler=greedy())\nlabels = generator(prompts)\nprint(labels)\n# [Classification(label=&lt;Label.urgent: 'URGENT'&gt;), Classification(label=&lt;Label.standard: 'STANDARD'&gt;)]\n</code></pre>"},{"location":"cookbook/dating_profiles/","title":"Generate a synthetic dating profile from a description","text":"<p>In this example we will see how we can use Outlines to generate synthetic data for a dating application. This example was originally contributed by Vibhor Kumar.</p> <pre><code>from dataclasses import dataclass\nfrom enum import Enum\n\nimport torch\nimport transformers\nfrom pydantic import BaseModel, conlist, constr\n\nimport outlines\n</code></pre>"},{"location":"cookbook/dating_profiles/#defining-the-profile-with-pydantic","title":"Defining the profile with Pydantic","text":"<p>Here a dating profile will consist in a biography, a job, a list of interests and two question-answer pairs. The questions are written in advance by the team, and the users are asked to provide an answer:</p> <pre><code>class QuestionChoice(str, Enum):\n    A = \"The key to my heart is\"\n    B = \"The first item on my bucket list is\"\n    C = \"Perks of dating me\"\n    D = \"Message me if you also love\"\n    E = \"People would describe me as\"\n    F = \"I can beat you in a game of\"\n\n@dataclass\nclass QuestionAnswer:\n    question: QuestionChoice\n    answer: str\n</code></pre> <p>Users need to provide a short biography, with a minimum of 10 and a maximum of 300 characters. The application also limits job descriptions to 50 characters. In addition to the question-answer pairs, the user is required to provide a list of between 1 and 5 interests:</p> <pre><code>class DatingProfile(BaseModel):\n    bio: constr(str, min_length=10, max_length=300)\n    job: constr(str, max_lengt=50)\n    interests: conlist(str, min_length=1, max_length=5)  # type: ignore\n    qna1: QuestionAnswer\n    qna2: QuestionAnswer\n</code></pre>"},{"location":"cookbook/dating_profiles/#prompt-template-and-examples","title":"Prompt template and examples","text":"<p>We will ask the model to generate profiles from a high-level description:</p> <pre><code>@dataclass\nclass Example:\n    description: str\n    profile: DatingProfile\n</code></pre> <p>We will use Outlines' prompt templating abilities to generate the prompt for us. This help clearly separate the general prompting logic from what is specific to an example.</p> <pre><code>@outlines.prompt\ndef dating_profile_prompt(description: str, examples: list[Example]):\n    \"\"\"\n    You are a world-renowned matchmaker who understands the modern dating\n    market. Your job is to generate dating app profiles for male clients\n    interested in women based on a provided description. The profiles should be\n    authentic, show off their strengths, and maximize their likelihood of\n    getting matches on dating apps.  Here are some examples of past clients that\n    you have successfully created profiles for:\n\n    {% for example in examples %}\n    Description:\n    {{ example.description }}\n    Profile:\n    {{ example.profile }}\n    {% endfor %}\n\n    Here is the new client who you need to create a profile for:\n    Description: {{ description }}\n    Profile:\n    \"\"\"\n</code></pre> <p>We will provide the model with several few-shot examples:</p> <pre><code>samples: list[Example] = [\n    Example(\n        description=\"I'm an author and former professional soccer player living in Seattle who publishes popular fiction books. A typical day for me starts by hanging out with my cat, drinking a coffee, and reading as much as I can in a few hours. Then, I'll prepare a quick smoothie before starting to write for a few hours, take a break with soccer or running a few miles, and finally meet friends for dinner at a new, hip restaurant in the evening. Sometimes we go axe-throwing afterwards, or play poker, or watch a comedy show, or visit a dive bar. On my vacations, I travel extensively to countries South America, Europe, and Asia, with the goal of visiting them all!\",\n        profile=DatingProfile(\n            bio=\"Adventurer, dreamer, author, and soccer enthusiast. Life\u2019s too short to waste time so I make the most of each day by exploring new places and playing with my friends on the pitch. What\u2019s your favorite way to get out and have fun?\",\n            job=\"Famous Soccer Player -&gt; Famous Author\",\n            interests=[\"Soccer\", \"Travel\", \"Friends\", \"Books\", \"Fluffy Animals\"],\n            qna1=QuestionAnswer(\n                question=QuestionChoice.B, answer=\"swim in all seven oceans!\"\n            ),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.E,\n                answer=\"fun-loving, adventurous, and a little bit crazy\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my company and build houses for a living. I'm a big fan of the outdoors and love to go hiking, camping, and fishing. I don't like video games, but do like to watch movies. My love language is home-cooked food, and I'm looking for someone who isn't afraid to get their hands dirty.\",\n        profile=DatingProfile(\n            bio=\"If you're looking for a Montana man who loves to get outdoors and hunt, and who's in-tune with his masculinity then I'm your guy!\",\n            job=\"House Construction Manager / Entrepreneur\",\n            interests=[\"Hunting\", \"Hiking\", \"The outdoors\", \"Home-cooked food\"],\n            qna1=QuestionAnswer(question=QuestionChoice.A, answer=\"food made at home\"),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.C,\n                answer=\"having a man in your life who can fix anything\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my own Youtube channel with 10M subscribers. I love working with kids, and my audience skews pretty young too. In my free time, I play Fortnite and Roblox. I'm looking for someone who is also a gamer and likes to have fun. I'm learning Japanese in my free time as well as how to cook.\",\n        profile=DatingProfile(\n            bio=\"Easy on the eyes (find me on Youtube!) and great with kids. What more do you need?\",\n            job=\"Youtuber 10M+ subscribers\",\n            interests=[\"Kids\", \"Gaming\", \"Japanese\"],\n            qna1=QuestionAnswer(question=QuestionChoice.D, answer=\"anime and gaming!\"),\n            qna2=QuestionAnswer(question=QuestionChoice.F, answer=\"Fortnite, gg ez\"),\n        ),\n    ),\n]\n</code></pre>"},{"location":"cookbook/dating_profiles/#load-the-model","title":"Load the model","text":"<p>We will use Mosaic's MPT-7B model (requires 13GB of GPU memory) which can fit on a single GPU with a reasonable context window. We initialize it with Outlines:</p> <pre><code>config = transformers.AutoConfig.from_pretrained(\n    \"mosaicml/mpt-7b-8k-instruct\", trust_remote_code=True\n)\nconfig.init_device = \"meta\"\nmodel = outlines.models.transformers(\n    model_name=\"mosaicml/mpt-7b-8k-instruct\",\n    device=\"cuda\",\n    model_kwargs={\n        \"config\": config,\n        \"trust_remote_code\": True,\n        \"torch_dtype\": torch.bfloat16,\n        \"device_map\": {\"\": 0},\n    },\n)\n</code></pre>"},{"location":"cookbook/dating_profiles/#json-structured-generation-of-profiles","title":"JSON-structured generation of profiles","text":"<p>We will now generate a dating profile from a textual description of oneself:</p> <pre><code>new_description = \"\"\"I'm a laid-back lawyer who spends a lot of his free-time\ngaming. I work in a corporate office, but ended up here after the start-up  I\ncofounded got acquired, so still play ping pong with my cool coworkers every\nday.  I have a bar at home where I make cocktails, which is great for\nentertaining  friends. I secretly like to wear suits and get a new one tailored\nevery few  months. I also like weddings because I get to wear those suits, and\nit's  a good excuse for a date. I watch the latest series because I'm paying,\nwith my hard-earned money, for every streaming service.\"\"\"\n\nprompt = dating_profile_prompt(new_description, samples)\nprofile = outlines.generate.json(model, DatingProfile)(prompt)\nparsed_profile = DatingProfile.model_validate_json(profile)\n</code></pre>"},{"location":"cookbook/dating_profiles/#results","title":"Results","text":"<p>Here are a couple of results:</p> <pre><code>{\n    \"bio\": \"\"\"I'm an ambitious lawyer with a casual and fashionable style. I love\n    games and sports, but my true passion is preparing refreshing cocktails at\n    home and dressing to the nines at weddings. I'm currently looking for a woman\n    to show a good time to and get a kiss on the opulent suit I just had made.\n    Send resume to this inbox.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Stylish guys\",\n        \"Gaming\",\n        \"Ping pong\",\n        \"Cocktails\",\n        \"Weddings\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"be married and have a family.\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"charming, stylish, and funny.\"\n    }\n}\n</code></pre> <pre><code>{\n    \"bio\": \"\"\"I\u2019m a sexy lawyer with time on my hands. I love to game and\n    play ping pong, but the real reason you should swipe to the right\n    is because I look great in a suit. Who doesn\u2019t love a man in a\n    suit? Just saying. Send me a message if you think it\u2019s time to take\n    your dating life to the next level.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Gaming\",\n        \"Ping Pong\",\n        \"Tailored Suits\",\n        \"Weddings\",\n        \"Streaming Services\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"simulate space but stay alive for as long as possible\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"easy-going, a little nerdy but with a mature essence\"\n    }\n}\n</code></pre>"},{"location":"cookbook/deploy-using-bentoml/","title":"Run Outlines using BentoML","text":"<p>BentoML is an open-source model serving library for building performant and scalable AI applications with Python. It comes with tools that you need for serving optimization, model packaging, and production deployment.</p> <p>In this guide, we will show you how to use BentoML to run programs written with Outlines on GPU locally and in BentoCloud, an AI Inference Platform for enterprise AI teams. The example source code in this guide is also available in the examples/bentoml/ directory.</p>"},{"location":"cookbook/deploy-using-bentoml/#import-a-model","title":"Import a model","text":"<p>First we need to download an LLM (Mistral-7B-v0.1 in this example and you can use any other LLM) and import the model into BentoML's Model Store. Let's install BentoML and other dependencies from PyPi (preferably in a virtual environment):</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Then save the code snippet below as <code>import_model.py</code> and run <code>python import_model.py</code>.</p> <p>Note: You need to accept related conditions on Hugging Face first to gain access to Mistral-7B-v0.1.</p> <pre><code>import bentoml\n\nMODEL_ID = \"mistralai/Mistral-7B-v0.1\"\nBENTO_MODEL_TAG = MODEL_ID.lower().replace(\"/\", \"--\")\n\ndef import_model(model_id, bento_model_tag):\n\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    )\n\n    with bentoml.models.create(bento_model_tag) as bento_model_ref:\n        tokenizer.save_pretrained(bento_model_ref.path)\n        model.save_pretrained(bento_model_ref.path)\n\n\nif __name__ == \"__main__\":\n    import_model(MODEL_ID, BENTO_MODEL_TAG)\n</code></pre> <p>You can verify the download is successful by running:</p> <pre><code>$ bentoml models list\n\nTag                                          Module  Size        Creation Time\nmistralai--mistral-7b-v0.1:m7lmf5ac2cmubnnz          13.49 GiB   2024-04-25 06:52:39\n</code></pre>"},{"location":"cookbook/deploy-using-bentoml/#define-a-bentoml-service","title":"Define a BentoML Service","text":"<p>As the model is ready, we can define a BentoML Service to wrap the capabilities of the model.</p> <p>We will run the JSON-structured generation example in the README, with the following schema:</p> <pre><code>DEFAULT_SCHEMA = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n</code></pre> <p>First, we need to define a BentoML service by decorating an ordinary class (<code>Outlines</code> here) with <code>@bentoml.service</code> decorator. We pass to this decorator some configuration and GPU on which we want this service to run in BentoCloud (here an L4 with 24GB memory):</p> <pre><code>import typing as t\nimport bentoml\n\nfrom import_model import BENTO_MODEL_TAG\n\n@bentoml.service(\n    traffic={\n        \"timeout\": 300,\n    },\n    resources={\n        \"gpu\": 1,\n        \"gpu_type\": \"nvidia-l4\",\n    },\n)\nclass Outlines:\n\n    bento_model_ref = bentoml.models.get(BENTO_MODEL_TAG)\n\n    def __init__(self) -&gt; None:\n\n        import outlines\n        import torch\n        self.model = outlines.models.transformers(\n            self.bento_model_ref.path,\n            device=\"cuda\",\n            model_kwargs={\"torch_dtype\": torch.float16},\n        )\n\n    ...\n</code></pre> <p>We then need to define an HTTP endpoint using <code>@bentoml.api</code> to decorate the method <code>generate</code> of <code>Outlines</code> class:</p> <pre><code>    ...\n\n    @bentoml.api\n    async def generate(\n        self,\n        prompt: str = \"Give me a character description.\",\n        json_schema: t.Optional[str] = DEFAULT_SCHEMA,\n    ) -&gt; t.Dict[str, t.Any]:\n\n        import outlines\n\n        generator = outlines.generate.json(self.model, json_schema)\n        character = generator(prompt)\n\n        return character\n</code></pre> <p>Here <code>@bentoml.api</code> decorator defines <code>generate</code> as an HTTP endpoint that accepts a JSON request body with two fields: <code>prompt</code> and <code>json_schema</code> (optional, which allows HTTP clients to provide their own JSON schema). The type hints in the function signature will be used to validate incoming JSON requests. You can define as many HTTP endpoints as you want by using <code>@bentoml.api</code> to decorate other methods of <code>Outlines</code> class.</p> <p>Now you can save the above code to <code>service.py</code> (or use this implementation), and run the code using the BentoML CLI.</p>"},{"location":"cookbook/deploy-using-bentoml/#run-locally-for-testing-and-debugging","title":"Run locally for testing and debugging","text":"<p>Then you can run a server locally by:</p> <pre><code>bentoml serve .\n</code></pre> <p>The server is now active at http://localhost:3000. You can interact with it using the Swagger UI or in other different ways:</p> CURL <pre><code>curl -X 'POST' \\\n  'http://localhost:3000/generate' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"prompt\": \"Give me a character description.\"\n}'\n</code></pre> Python client <pre><code>import bentoml\n\nwith bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\n    response = client.generate(\n        prompt=\"Give me a character description\"\n    )\n    print(response)\n</code></pre> <p>Expected output:</p> <pre><code>{\n  \"name\": \"Aura\",\n  \"age\": 15,\n  \"armor\": \"plate\",\n  \"weapon\": \"sword\",\n  \"strength\": 20\n}\n</code></pre>"},{"location":"cookbook/deploy-using-bentoml/#deploy-to-bentocloud","title":"Deploy to BentoCloud","text":"<p>After the Service is ready, you can deploy it to BentoCloud for better management and scalability. Sign up if you haven't got a BentoCloud account.</p> <p>Make sure you have logged in to BentoCloud, then run the following command to deploy it.</p> <pre><code>bentoml deploy .\n</code></pre> <p>Once the application is up and running on BentoCloud, you can access it via the exposed URL.</p> <p>Note: For custom deployment in your own infrastructure, use BentoML to generate an OCI-compliant image.</p>"},{"location":"cookbook/deploy-using-cerebrium/","title":"Run Outlines using Cerebrium","text":"<p>Cerebrium is a serverless AI infrastructure platform that makes it easier for companies to build and deploy AI based applications. They offer Serverless GPU's\u00a0with low cold start times with over 12 varieties of GPU chips that auto scale and you only pay for the compute you use.</p> <p>In this guide we will show you how you can use Cerebrium to run programs written with Outlines on GPUs in the cloud.</p>"},{"location":"cookbook/deploy-using-cerebrium/#setup-cerebrium","title":"Setup Cerebrium","text":"<p>First, we install Cerebrium and login to get authenticated.</p> <pre><code>pip install cerebrium\ncerebrium login\n</code></pre> <p>Then let us create our first project</p> <pre><code>cerebrium init outlines-project\n</code></pre>"},{"location":"cookbook/deploy-using-cerebrium/#setup-environment-and-hardware","title":"Setup Environment and Hardware","text":"<p>You set up your environment and hardware in the cerebrium.toml file that was created using the init function above.</p> <pre><code>[cerebrium.deployment]\ndocker_base_image_url = \"nvidia/cuda:12.1.1-runtime-ubuntu22.04\"\n\n[cerebrium.hardware]\ncpu = 2\nmemory = 14.0\ngpu = \"AMPERE A10\"\ngpu_count = 1\nprovider = \"aws\"\nregion = \"us-east-1\"\n\n[cerebrium.dependencies.pip]\noutline = \"==0.0.37\"\ntransformers = \"==4.38.2\"\ndatasets = \"==2.18.0\"\naccelerate = \"==0.27.2\"\n</code></pre>"},{"location":"cookbook/deploy-using-cerebrium/#setup-inference","title":"Setup inference","text":"<p>Running code in Cerebrium is like writing normal python with no special syntax. In a <code>main.py</code> file specify the following:</p> <pre><code>import outlines\n\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\nschema = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n\ngenerator = outlines.generate.json(model, schema)\n</code></pre> <p>On first deploy, it will download the model and store it on disk therefore for subsequent calls it will load the model from disk.</p> <p>Every function in Cerebrium is callable through an API endpoint. Code at the top most layer (ie: not in a function) is instantiated only when the container is spun up the first time so for subsequent calls, it will simply run the code defined in the function you call.</p> <p>To deploy an API that creates a new character when called with a prompt you can add the following code to <code>main.py</code>:</p> <pre><code>def generate(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n\n    character = generator(\n        f\"&lt;s&gt;[INST]Give me a character description. Describe {prompt}.[/INST]\"\n    )\n\n    return character\n</code></pre>"},{"location":"cookbook/deploy-using-cerebrium/#run-on-the-cloud","title":"Run on the cloud","text":"<pre><code>cerebrium deploy\n</code></pre> <p>You will see your application deploy, install pip packages and download the model. Once completed it will output a CURL request you can use to call your endpoint. Just remember to end the url with the function you would like to call - in this case /generate. You should see your response returned!</p>"},{"location":"cookbook/deploy-using-modal/","title":"Run Outlines using Modal","text":"<p>Modal is a serverless platform that allows you to easily run code on the cloud, including GPUs. It can come very handy for those of us who don't have a monster GPU at home and want to be able to quickly and easily provision, configure and orchestrate cloud infrastructure.</p> <p>In this guide we will show you how you can use Modal to run programs written with Outlines on GPU in the cloud.</p>"},{"location":"cookbook/deploy-using-modal/#requirements","title":"Requirements","text":"<p>We recommend installing <code>modal</code> and <code>outlines</code> in a virtual environment. You can create one with:</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Then install the required packages:</p> <pre><code>pip install modal outlines\n</code></pre>"},{"location":"cookbook/deploy-using-modal/#build-the-image","title":"Build the image","text":"<p>First we need to define our container image. If you need to access a gated model, you will need to provide an access token. See the <code>.env</code> call below for how to provide a HuggingFace token.</p> <p>Setting a token is best done by setting an environment variable <code>HF_TOKEN</code> with your token. If you do not wish to do this, we provide a commented-out line in the code to set the token directly in the code.</p> <pre><code>from modal import Image, App, gpu\nimport os\n\n# This creates a modal App object. Here we set the name to \"outlines-app\".\n# There are other optional parameters like modal secrets, schedules, etc.\n# See the documentation here: https://modal.com/docs/reference/modal.App\napp = App(name=\"outlines-app\")\n\n# Specify a language model to use.\n# Another good model to use is \"NousResearch/Hermes-2-Pro-Mistral-7B\"\nlanguage_model = \"mistral-community/Mistral-7B-v0.2\"\n\n# Please set an environment variable HF_TOKEN with your Hugging Face API token.\n# The code below (the .env({...}) part) will copy the token from your local\n# environment to the container.\n# More info on Image here: https://modal.com/docs/reference/modal.Image\noutlines_image = Image.debian_slim(python_version=\"3.11\").pip_install(\n    \"outlines\",\n    \"transformers\",\n    \"datasets\",\n    \"accelerate\",\n    \"sentencepiece\",\n).env({\n    # This will pull in your HF_TOKEN environment variable if you have one.\n    'HF_TOKEN':os.environ['HF_TOKEN']\n\n    # To set the token directly in the code, uncomment the line below and replace\n    # 'YOUR_TOKEN' with the HuggingFace access token.\n    # 'HF_TOKEN':'YOUR_TOKEN'\n})\n</code></pre>"},{"location":"cookbook/deploy-using-modal/#setting-the-container-up","title":"Setting the container up","text":"<p>When running longer Modal apps, it's recommended to download your language model when the container starts, rather than when the function is called. This will cache the model for future runs.</p> <pre><code># This function imports the model from Hugging Face. The modal container\n# will call this function when it starts up. This is useful for\n# downloading models, setting up environment variables, etc.\ndef import_model():\n    import outlines\n    outlines.models.transformers(language_model)\n\n# This line tells the container to run the import_model function when it starts.\noutlines_image = outlines_image.run_function(import_model)\n</code></pre>"},{"location":"cookbook/deploy-using-modal/#define-a-schema","title":"Define a schema","text":"<p>We will run the JSON-structured generation example in the README, with the following schema:</p> <pre><code># Specify a schema for the character description. In this case,\n# we want to generate a character with a name, age, armor, weapon, and strength.\nschema = \"\"\"{\n    \"title\": \"Character\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"title\": \"Name\",\n            \"maxLength\": 10,\n            \"type\": \"string\"\n        },\n        \"age\": {\n            \"title\": \"Age\",\n            \"type\": \"integer\"\n        },\n        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n        \"strength\": {\n            \"title\": \"Strength\",\n            \"type\": \"integer\"\n        }\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"weapon\", \"strength\"],\n    \"definitions\": {\n        \"Armor\": {\n            \"title\": \"Armor\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"type\": \"string\"\n        },\n        \"Weapon\": {\n            \"title\": \"Weapon\",\n            \"description\": \"An enumeration.\",\n            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n            \"type\": \"string\"\n        }\n    }\n}\"\"\"\n</code></pre> <p>To make the inference work on Modal we need to wrap the corresponding function in a <code>@app.function</code> decorator. We pass to this decorator the image and GPU on which we want this function to run.</p> <p>Let's choose an A100 with 80GB memory. Valid GPUs can be found here.</p> <pre><code># Define a function that uses the image we chose, and specify the GPU\n# and memory we want to use.\n@app.function(image=outlines_image, gpu=gpu.A100(size='80GB'))\ndef generate(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n    # Remember, this function is being executed in the container,\n    # so we need to import the necessary libraries here. You should\n    # do this with any other libraries you might need.\n    import outlines\n\n    # Load the model into memory. The import_model function above\n    # should have already downloaded the model, so this call\n    # only loads the model into GPU memory.\n    model = outlines.models.transformers(\n        language_model, device=\"cuda\"\n    )\n\n    # Generate a character description based on the prompt.\n    # We use the .json generation method -- we provide the\n    # - model: the model we loaded above\n    # - schema: the JSON schema we defined above\n    generator = outlines.generate.json(model, schema)\n\n    # Make sure you wrap your prompt in instruction tags ([INST] and [/INST])\n    # to indicate that the prompt is an instruction. Instruction tags can vary\n    # by models, so make sure to check the model's documentation.\n    character = generator(\n        f\"&lt;s&gt;[INST]Give me a character description. Describe {prompt}.[/INST]\"\n    )\n\n    # Print out the generated character.\n    print(character)\n</code></pre> <p>We then need to define a <code>local_entrypoint</code> to call our function <code>generate</code> remotely.</p> <pre><code>@app.local_entrypoint()\ndef main(\n    prompt: str = \"Amiri, a 53 year old warrior woman with a sword and leather armor.\",\n):\n    # We use the \"generate\" function defined above -- note too that we are calling\n    # .remote() on the function. This tells modal to run the function in our cloud\n    # machine. If you want to run the function locally, you can call .local() instead,\n    # though this will require additional setup.\n    generate.remote(prompt)\n</code></pre> <p>Here <code>@app.local_entrypoint()</code> decorator defines <code>main</code> as the function to start from locally when using the Modal CLI. You can save above code to <code>example.py</code> (or use this implementation). Let's now see how to run the code on the cloud using the Modal CLI.</p>"},{"location":"cookbook/deploy-using-modal/#run-on-the-cloud","title":"Run on the cloud","text":"<p>First install the Modal client from PyPi, if you have not already:</p> <pre><code>pip install modal\n</code></pre> <p>You then need to obtain a token from Modal. Run the following command:</p> <pre><code>modal setup\n</code></pre> <p>Once that is set you can run inference on the cloud using:</p> <pre><code>modal run example.py\n</code></pre> <p>You should see the Modal app initialize, and soon after see the result of the <code>print</code> function in your terminal. That's it!</p>"},{"location":"cookbook/extraction/","title":"Named entity extraction","text":"<p>Named Entity Extraction is a fundamental problem in NLP. It involves identifying and categorizing named entities within a document: people, organization, dates, places, etc. It is usually the first step in a more complex NLP worklow. Here we will use the example of a pizza restaurant that receives orders via their website and need to identify the number and types of pizzas that are being ordered.</p> <p>Getting LLMs to output the extracted entities in a structured format can be challenging. In this tutorial we will see how we can use Outlines' JSON-structured generation to extract entities from a document and return them in a valid JSON data structure 100% of the time.</p> <p>As always, we start with initializing the model. We will be using a quantized version of Mistal-7B-v0.1 (we're GPU poor):</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre> <p>And we will be using the following prompt template:</p> <pre><code>@outlines.prompt\ndef take_order(order):\n    \"\"\"You are the owner of a pizza parlor. Customers \\\n    send you orders from which you need to extract:\n\n    1. The pizza that is ordered\n    2. The number of pizzas\n\n    # EXAMPLE\n\n    ORDER: I would like one Margherita pizza\n    RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n\n    # OUTPUT INSTRUCTIONS\n\n    Answer in valid JSON. Here are the different objects relevant for the output:\n\n    Order:\n        pizza (str): name of the pizza\n        number (int): number of pizzas\n\n    Return a valid JSON of type \"Order\"\n\n    # OUTPUT\n\n    ORDER: {{ order }}\n    RESULT: \"\"\"\n</code></pre> <p>We now define our data model using Pydantic:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\nclass Pizza(str, Enum):\n    margherita = \"Margherita\"\n    pepperonni = \"Pepperoni\"\n    calzone = \"Calzone\"\n\nclass Order(BaseModel):\n    pizza: Pizza\n    number: int\n</code></pre> <p>We can now define our generator and call it on several incoming orders:</p> <pre><code>orders = [\n    \"Hi! I would like to order two pepperonni pizzas and would like them in 30mins.\",\n    \"Is it possible to get 12 margheritas?\"\n]\nprompts = [take_order(order) for order in orders]\n\ngenerator = outlines.generate.json(model, Order)\n\nresults = generator(prompts)\nprint(results)\n# [Order(pizza=&lt;Pizza.pepperonni: 'Pepperoni'&gt;, number=2),\n#  Order(pizza=&lt;Pizza.margherita: 'Margherita'&gt;, number=12)]\n</code></pre> <p>There are several ways you could improve this example:</p> <ul> <li>Clients may order several types of pizzas.</li> <li>Clients may order drinks as well.</li> <li>If the pizza place has a delivery service we need to extract the client's address and phone number</li> <li>Clients may specify the time for which they want the pizza. We could then check against a queuing system and reply to them with the estimated delivery time.</li> </ul> <p>How would you change the Pydantic model to account for these use cases?</p>"},{"location":"cookbook/knowledge_graph_extraction/","title":"Knowledge Graph Extraction","text":"<p>In this guide, we use outlines to extract a knowledge graph from unstructured text.</p> <p>We will use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/knowledge_graph_extraction/#knowledge-graph-extraction_1","title":"Knowledge Graph Extraction","text":"<p>We first need to define our Pydantic class for each node and each edge of the knowledge graph:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Node(BaseModel):\n    \"\"\"Node of the Knowledge Graph\"\"\"\n\n    id: int = Field(..., description=\"Unique identifier of the node\")\n    label: str = Field(..., description=\"Label of the node\")\n    property: str = Field(..., description=\"Property of the node\")\n\n\nclass Edge(BaseModel):\n    \"\"\"Edge of the Knowledge Graph\"\"\"\n\n    source: int = Field(..., description=\"Unique source of the edge\")\n    target: int = Field(..., description=\"Unique target of the edge\")\n    label: str = Field(..., description=\"Label of the edge\")\n    property: str = Field(..., description=\"Property of the edge\")\n</code></pre> <p>We then define the Pydantic class for the knowledge graph and get its JSON schema:</p> <pre><code>from typing import List\n\nclass KnowledgeGraph(BaseModel):\n    \"\"\"Generated Knowledge Graph\"\"\"\n\n    nodes: List[Node] = Field(..., description=\"List of nodes of the knowledge graph\")\n    edges: List[Edge] = Field(..., description=\"List of edges of the knowledge graph\")\n\nschema = KnowledgeGraph.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>def generate_hermes_prompt(user_prompt):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\\n\"\n        \"&lt;|im_start|&gt;user\\n\"\n        + user_prompt\n        + \"&lt;|im_end|&gt;\"\n        + \"\\n&lt;|im_start|&gt;assistant\\n\"\n        \"&lt;schema&gt;\"\n    )\n</code></pre> <p>For a given user prompt, for example:</p> <pre><code>user_prompt = \"Alice loves Bob and she hates Charlie.\"\n</code></pre> <p>We can use <code>generate.json</code> by passing the Pydantic class we previously defined, and call the generator with the Hermes prompt:</p> <pre><code>from outlines import generate, models\n\nmodel = models.LlamaCpp(llm)\ngenerator = generate.json(model, KnowledgeGraph)\nprompt = generate_hermes_prompt(user_prompt)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\n</code></pre> <p>We obtain the nodes and edges of the knowledge graph:</p> <pre><code>print(response.nodes)\nprint(response.edges)\n# [Node(id=1, label='Alice', property='Person'),\n# Node(id=2, label='Bob', property='Person'),\n# Node(id=3, label='Charlie', property='Person')]\n# [Edge(source=1, target=2, label='love', property='Relationship'),\n# Edge(source=1, target=3, label='hate', property='Relationship')]\n</code></pre>"},{"location":"cookbook/knowledge_graph_extraction/#optional-visualizing-the-knowledge-graph","title":"(Optional) Visualizing the Knowledge Graph","text":"<p>We can use the Graphviz library to visualize the generated knowledge graph. For detailed installation instructions, see here.</p> <pre><code>from graphviz import Digraph\n\ndot = Digraph()\nfor node in response.nodes:\n    dot.node(str(node.id), node.label, shape='circle', width='1', height='1')\nfor edge in response.edges:\n    dot.edge(str(edge.source), str(edge.target), label=edge.label)\n\ndot.render('knowledge-graph.gv', view=True)\n</code></pre> <p></p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/models_playing_chess/","title":"Large language models playing chess","text":"<p>In this example we will make a Phi-2 model play chess against itself. On its own the model easily generates invalid moves, so we will give it a little help. At each step we will generate a regex that only matches valid move, and use it to help the model only generating valid moves.</p>"},{"location":"cookbook/models_playing_chess/#the-chessboard","title":"The chessboard","text":"<p>The game will be played on a standard checkboard. We will use the <code>chess</code> library to track the opponents' moves, and check that the moves are valid.</p> <pre><code>%pip install outlines -q\n%pip install chess -q\n%pip install transformers accelerate einops -q\n\nimport chess\n\nboard = chess.Board(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\")\n</code></pre>"},{"location":"cookbook/models_playing_chess/#the-opponents","title":"The opponents","text":"<p>Phi-2 will be playing against itself:</p> <pre><code>from outlines import models\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n</code></pre>"},{"location":"cookbook/models_playing_chess/#a-little-help-for-the-language-model","title":"A little help for the language model","text":"<p>To make sure Phi-2 generates valid chess moves we will use Outline's regex-structured generation. We define a function that takes the current state of the board and returns a regex that matches all possible legal moves:</p> <pre><code>import re\n\ndef legal_moves_regex(board):\n    \"\"\"Build a regex that only matches valid moves.\"\"\"\n    legal_moves = list(board.legal_moves)\n    legal_modes_str = [board.san(move) for move in legal_moves]\n    legal_modes_str = [re.sub(r\"[+#]\", \"\", move) for move in legal_modes_str]\n    regex_pattern = \"|\".join(re.escape(move) for move in legal_modes_str)\n    regex_pattern = f\"{regex_pattern}\"\n    return regex_pattern\n</code></pre>"},{"location":"cookbook/models_playing_chess/#prompting-the-language-model","title":"Prompting the language model","text":"<p>The prompt corresponds to the current state of the board, so we start with:</p> <pre><code>prompt = \"Let's play Chess. Moves: \"\n</code></pre> <p>We update the prompt at each step so it reflects the state of the board after the previous move.</p>"},{"location":"cookbook/models_playing_chess/#lets-play","title":"Let's play","text":"<pre><code>from outlines import generate\n\nboard_state = \" \"\nturn_number = 0\nwhile not board.is_game_over():\n    regex_pattern = legal_moves_regex(board)\n    structured = generate.regex(model, regex_pattern)(prompt + board_state)\n    move = board.parse_san(structured)\n\n    if turn_number % 2 == 0 :  # It's White's turn\n        board_state += board.san(move) + \" \"\n    else:\n        board_state += board.san(move) + \" \" + str(turn_number) + \".\"\n\n    turn_number += 1\n\n    board.push(move)\n\n    print(board_state)\n</code></pre> <p>Interestingly enough, Phi-2 hates capturing.</p> <pre><code> e4 e5 1.Nf3 Ne7 3.b4 Nf5 5.Nc3 Ne7 7.Bb5 a6 9.Na4 b6 11.c3 Nec6 13.c4 a5 15.d4 Qg5 17.Nd2 Bb7 19.dxe5\n</code></pre> <p>This example was originally authored by @903124S in this gist.</p>"},{"location":"cookbook/qa-with-citations/","title":"Generate Synthetic Data and Q&amp;A with Citations","text":"<p>This tutorial is adapted from the instructor-ollama notebook. We start with a simple example to generate synthetic data and then we approach the problem of question answering by providing citations.</p> <p>We will use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/qa-with-citations/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>We first need to define our Pydantic class for a user:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier\") # so the model keeps track of the number of users\n    first_name: str\n    last_name: str\n    age: int\n</code></pre> <p>We then define a Pydantic class for a list of users:</p> <pre><code>from typing import List\n\nclass Users(BaseModel):\n    users: List[UserDetail]\n</code></pre> <p>We can use a <code>generate.json</code> by passing this Pydantic class we just defined, and call the generator:</p> <pre><code>model = models.LlamaCpp(llm)\ngenerator = generate.json(model, Users)\nresponse = generator(\"Create 5 fake users\", max_tokens=1024, temperature=0, seed=42)\nprint(response.users)\n# [UserDetail(id=1, first_name='John', last_name='Doe', age=25),\n# UserDetail(id=2, first_name='Jane', last_name='Doe', age=30),\n# UserDetail(id=3, first_name='Bob', last_name='Smith', age=40),\n# UserDetail(id=4, first_name='Alice', last_name='Smith', age=35),\n# UserDetail(id=5, first_name='John', last_name='Smith', age=20)]\n</code></pre> <pre><code>for user in response.users:\n    print(user.first_name)\n    print(user.last_name)\n    print(user.age)\n    print(#####)\n# John\n# Doe\n# 25\n# #####\n# Jane\n# Doe\n# 30\n# #####\n# Bob\n# Smith\n# 40\n# #####\n# Alice\n# Smith\n# 35\n# #####\n# John\n# Smith\n# 20\n# #####\n</code></pre>"},{"location":"cookbook/qa-with-citations/#qa-with-citations","title":"QA with Citations","text":"<p>We first need to define our Pydantic class for QA with citations:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n    citations: List[str]\n\nschema = QuestionAnswer.model_json_schema()\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema:</p> <pre><code>def generate_hermes_prompt(question, context, schema=schema):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON with correct and exact citations \"\n        \"extracted from the `Context`. \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\\n\"\n        \"&lt;|im_start|&gt;user\\n\"\n        + \"`Context`: \"\n        + context\n        + \"\\n`Question`: \"\n        + question + \"&lt;|im_end|&gt;\"\n        + \"\\n&lt;|im_start|&gt;assistant\\n\"\n        \"&lt;schema&gt;\"\n    )\n</code></pre> <p>We can use <code>generate.json</code> by passing the Pydantic class we previously defined, and call the generator with Hermes prompt:</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\ngenerator = generate.json(model, QuestionAnswer)\nprompt = generate_hermes_prompt(question, context)\nresponse = generator(prompt, max_tokens=1024, temperature=0, seed=42)\nprint(response)\n# QuestionAnswer(question='What did the author do during college?', answer='The author studied Computational Mathematics and physics in university and was also involved in starting the Data Science club, serving as its president for 2 years.', citations=['I went to an arts high school but in university I studied Computational Mathematics and physics.', 'I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.'])\n</code></pre> <p>We can do the same for a list of question-context pairs:</p> <pre><code>question1 = \"Where was John born?\"\ncontext1 = \"\"\"\nJohn Doe is a software engineer who was born in New York, USA.\nHe studied Computer Science at the Massachusetts Institute of Technology.\nDuring his studies, he interned at Google and Microsoft.\nHe also founded the Artificial Intelligence club at his university and served as its president for three years.\n\"\"\"\n\nquestion2 = \"What did Emily study in university?\"\ncontext2 = \"\"\"\nEmily Smith is a data scientist from London, England.\nShe attended the University of Cambridge where she studied Statistics and Machine Learning.\nShe interned at IBM and Amazon during her summer breaks.\nEmily was also the head of the Women in Tech society at her university.\n\"\"\"\n\nquestion3 = \"Which companies did Robert intern at?\"\ncontext3 = \"\"\"\nRobert Johnson, originally from Sydney, Australia, is a renowned cybersecurity expert.\nHe studied Information Systems at the University of Melbourne.\nRobert interned at several cybersecurity firms including NortonLifeLock and McAfee.\nHe was also the leader of the Cybersecurity club at his university.\n\"\"\"\n\nquestion4 = \"What club did Alice start at her university?\"\ncontext4 = \"\"\"\nAlice Williams, a native of Dublin, Ireland, is a successful web developer.\nShe studied Software Engineering at Trinity College Dublin.\nAlice interned at several tech companies including Shopify and Squarespace.\nShe started the Web Development club at her university and was its president for two years.\n\"\"\"\n\nquestion5 = \"What did Michael study in high school?\"\ncontext5 = \"\"\"\nMichael Brown is a game developer from Tokyo, Japan.\nHe attended a specialized high school where he studied Game Design.\nHe later attended the University of Tokyo where he studied Computer Science.\nMichael interned at Sony and Nintendo during his university years.\nHe also started the Game Developers club at his university.\n\"\"\"\n\nfor question, context in [\n    (question1, context1),\n    (question2, context2),\n    (question3, context3),\n    (question4, context4),\n    (question5, context5),\n]:\n    final_prompt = my_final_prompt(question, context)\n    generator = generate.json(model, QuestionAnswer)\n    response = generator(final_prompt, max_tokens=1024, temperature=0, seed=42)\n    display(question)\n    display(response.answer)\n    display(response.citations)\n    print(\"\\n\\n\")\n\n# 'Where was John born?'\n# 'John Doe was born in New York, USA.'\n# ['John Doe is a software engineer who was born in New York, USA.']\n#\n#\n# 'What did Emily study in university?'\n# 'Emily studied Statistics and Machine Learning in university.'\n# ['She attended the University of Cambridge where she studied Statistics and Machine Learning.']\n#\n#\n# 'Which companies did Robert intern at?'\n# 'Robert interned at NortonLifeLock and McAfee.'\n# ['Robert Johnson, originally from Sydney, Australia, is a renowned cybersecurity expert. He interned at several cybersecurity firms including NortonLifeLock and McAfee.']\n#\n#\n# 'What club did Alice start at her university?'\n# 'Alice started the Web Development club at her university.'\n# ['Alice Williams, a native of Dublin, Ireland, is a successful web developer. She started the Web Development club at her university and was its president for two years.']\n#\n#\n# 'What did Michael study in high school?'\n# 'Michael studied Game Design in high school.'\n# ['Michael Brown is a game developer from Tokyo, Japan. He attended a specialized high school where he studied Game Design.']\n</code></pre> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/react_agent/","title":"ReAct Agent","text":"<p>This example shows how to use outlines to build your own agent with open weights local models and structured outputs. It is inspired by the blog post A simple Python implementation of the ReAct pattern for LLMs by Simon Willison.</p> <p>The ReAct pattern (for Reason+Act) is described in the paper ReAct: Synergizing Reasoning and Acting in Language Models. It's a pattern where you implement additional actions that an LLM can take - searching Wikipedia or running calculations for example - and then teach it how to request the execution of those actions, and then feed their results back into the LLM.</p> <p>Additionally, we give the LLM the possibility of using a scratchpad described in the paper Show Your Work: Scratchpads for Intermediate Computation with Language Models which improves the ability of LLMs to perform multi-step computations.</p> <p>We use llama.cpp using the llama-cpp-python library. Outlines supports llama-cpp-python, but we need to install it ourselves:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>We download the model weights by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern): <pre><code>import llama_cpp\nfrom outlines import generate, models\n\nmodel = models.llamacpp(\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n            \"Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n            tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n            \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n            ),\n            n_gpu_layers=-1,\n            flash_attn=True,\n            n_ctx=8192,\n            verbose=False)\n</code></pre></p> (Optional) Store the model weights in a custom folder <p>By default the model weights are downloaded to the hub cache but if we want so store the weights in a custom folder, we pull a quantized GGUF model Hermes-2-Pro-Llama-3-8B by NousResearch from HuggingFace:</p> <pre><code>wget https://hf.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\n</code></pre> <p>We initialize the model:</p> <pre><code>import llama_cpp\nfrom llama_cpp import Llama\nfrom outlines import generate, models\n\nllm = Llama(\n    \"/path/to/model/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf\",\n    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n        \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n    ),\n    n_gpu_layers=-1,\n    flash_attn=True,\n    n_ctx=8192,\n    verbose=False\n)\n</code></pre>"},{"location":"cookbook/react_agent/#build-a-react-agent","title":"Build a ReAct agent","text":"<p>In this example, we use two tools:</p> <ul> <li>wikipedia: \\&lt;search term&gt; - search Wikipedia and returns the snippet of the first result</li> <li>calculate: \\&lt;expression&gt; - evaluate an expression using Python's eval() function</li> </ul> <pre><code>import httpx\n\ndef wikipedia(q):\n    return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n        \"action\": \"query\",\n        \"list\": \"search\",\n        \"srsearch\": q,\n        \"format\": \"json\"\n    }).json()[\"query\"][\"search\"][0][\"snippet\"]\n\n\ndef calculate(numexp):\n    return eval(numexp)\n</code></pre> <p>We define the logic of the agent through a Pydantic class. First, we want the LLM to decide only between the two previously defined tools:</p> <pre><code>from enum import Enum\n\nclass Action(str, Enum):\n    wikipedia = \"wikipedia\"\n    calculate = \"calculate\"\n</code></pre> <p>Our agent will loop through Thought and Action. We explicitly give the Action Input field so it doesn't forget to add the arguments of the Action. We also add a scratchpad (optional).</p> <pre><code>from pydantic import BaseModel, Field\n\nclass Reason_and_Act(BaseModel):\n    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n    Thought: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n    Action: Action\n    Action_Input: str = Field(..., description=\"The arguments of the Action.\")\n</code></pre> <p>Our agent will reach a Final Answer. We also add a scratchpad (optional).</p> <pre><code>class Final_Answer(BaseModel):\n    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n    Final_Answer: str = Field(..., description=\"Answer to the question grounded on the Observation\")\n</code></pre> <p>Our agent will decide when it has reached a Final Answer and therefore to stop the loop of Thought and Action.</p> <pre><code>from typing import Union\n\nclass Decision(BaseModel):\n    Decision: Union[Reason_and_Act, Final_Answer]\n</code></pre> <p>We could generate a response using the json schema but we will use the regex and check that everything is working as expected:</p> <pre><code>from outlines.integrations.utils import convert_json_schema_to_str\nfrom outlines.fsm.json_schema import build_regex_from_schema\n\njson_schema = Decision.model_json_schema()\nschema_str = convert_json_schema_to_str(json_schema=json_schema)\nregex_str = build_regex_from_schema(schema_str)\nprint(regex_str)\n# '\\\\{[ ]?\"Decision\"[ ]?:[ ]?(\\\\{[ ]?\"Scratchpad\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?,[ ]?\"Thought\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?,[ ]?\"Action\"[ ]?:[ ]?(\"wikipedia\"|\"calculate\")[ ]?,[ ]?\"Action_Input\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?\\\\}|\\\\{[ ]?\"Scratchpad\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?,[ ]?\"Final_Answer\"[ ]?:[ ]?\"([^\"\\\\\\\\\\\\x00-\\\\x1F\\\\x7F-\\\\x9F]|\\\\\\\\[\"\\\\\\\\])*\"[ ]?\\\\})[ ]?\\\\}'\n</code></pre> <p>We then need to adapt our prompt to the Hermes prompt format for JSON schema and explain the agent logic:</p> <pre><code>import datetime\n\ndef generate_hermes_prompt(question, schema=\"\"):\n    return (\n        \"&lt;|im_start|&gt;system\\n\"\n        \"You are a world class AI model who answers questions in JSON with correct Pydantic schema. \"\n        f\"Here's the json schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;\\n\"\n        \"Today is \" + datetime.datetime.today().strftime('%Y-%m-%d') + \".\\n\" +\n        \"You run in a loop of Scratchpad, Thought, Action, Action Input, PAUSE, Observation. \"\n        \"At the end of the loop you output a Final Answer. \"\n        \"Use Scratchpad to store the information from the Observation useful to answer the question \"\n        \"Use Thought to describe your thoughts about the question you have been asked \"\n        \"and reflect carefully about the Observation if it exists. \"\n        \"Use Action to run one of the actions available to you. \"\n        \"Use Action Input to input the arguments of the selected action - then return PAUSE. \"\n        \"Observation will be the result of running those actions. \"\n        \"Your available actions are:\\n\"\n        \"calculate:\\n\"\n        \"e.g. calulate: 4**2 / 3\\n\"\n        \"Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\"\n        \"wikipedia:\\n\"\n        \"e.g. wikipedia: Django\\n\"\n        \"Returns a summary from searching Wikipedia\\n\"\n        \"DO NOT TRY TO GUESS THE ANSWER. Begin! &lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;user\\n\" + question + \"&lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;assistant\\n\"\n    )\n</code></pre> <p>We define a ChatBot class</p> <pre><code>class ChatBot:\n    def __init__(self, prompt=\"\"):\n        self.prompt = prompt\n\n    def __call__(self, user_prompt):\n        self.prompt += user_prompt\n        result = self.execute()\n        return result\n\n    def execute(self):\n        generator = generate.regex(model, regex_str)\n        result = generator(self.prompt, max_tokens=1024, temperature=0, seed=42)\n        return result\n</code></pre> <p>We define a query function:</p> <pre><code>import json\n\ndef query(question, max_turns=5):\n    i = 0\n    next_prompt = (\n        \"\\n&lt;|im_start|&gt;user\\n\" + question + \"&lt;|im_end|&gt;\"\n        \"\\n&lt;|im_start|&gt;assistant\\n\"\n    )\n    previous_actions = []\n    while i &lt; max_turns:\n        i += 1\n        prompt = generate_hermes_prompt(question=question, schema=Decision.model_json_schema())\n        bot = ChatBot(prompt=prompt)\n        result = bot(next_prompt)\n        json_result = json.loads(result)['Decision']\n        if \"Final_Answer\" not in list(json_result.keys()):\n            scratchpad = json_result['Scratchpad'] if i == 0 else \"\"\n            thought = json_result['Thought']\n            action = json_result['Action']\n            action_input = json_result['Action_Input']\n            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n            print(f\"\\x1b[34m Thought: {thought} \\x1b[0m\")\n            print(f\"\\x1b[36m  -- running {action}: {str(action_input)}\\x1b[0m\")\n            if action + \": \" + str(action_input) in previous_actions:\n                observation = \"You already run that action. **TRY A DIFFERENT ACTION INPUT.**\"\n            else:\n                if action==\"calculate\":\n                    try:\n                        observation = eval(str(action_input))\n                    except Exception as e:\n                        observation = f\"{e}\"\n                elif action==\"wikipedia\":\n                    try:\n                        observation = wikipedia(str(action_input))\n                    except Exception as e:\n                        observation = f\"{e}\"\n            print()\n            print(f\"\\x1b[33m Observation: {observation} \\x1b[0m\")\n            print()\n            previous_actions.append(action + \": \" + str(action_input))\n            next_prompt += (\n                \"\\nScratchpad: \" + scratchpad +\n                \"\\nThought: \" + thought +\n                \"\\nAction: \" + action  +\n                \"\\nAction Input: \" + action_input +\n                \"\\nObservation: \" + str(observation)\n            )\n        else:\n            scratchpad = json_result[\"Scratchpad\"]\n            final_answer = json_result[\"Final_Answer\"]\n            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n            print(f\"\\x1b[34m Final Answer: {final_answer} \\x1b[0m\")\n            return final_answer\n    print(f\"\\nFinal Answer: I am sorry, but I am unable to answer your question. Please provide more information or a different question.\")\n    return \"No answer found\"\n</code></pre> <p>We can now test our ReAct agent:</p> <pre><code>print(query(\"What's 2 to the power of 10?\"))\n# Scratchpad:\n# Thought: I need to perform a mathematical calculation to find the result of 2 to the power of 10.\n#  -- running calculate: 2**10\n#\n# Observation: 1024\n#\n# Scratchpad: 2 to the power of 10 is 1024.\n# Final Answer: 2 to the power of 10 is 1024.\n# 2 to the power of 10 is 1024.\n</code></pre> <pre><code>print(query(\"What does England share borders with?\"))\n# Scratchpad:\n# Thought: To answer this question, I will use the 'wikipedia' action to gather information about England's geographical location and its borders.\n#  -- running wikipedia: England borders\n#\n# Observation: Anglo-Scottish &lt;span class=\"searchmatch\"&gt;border&lt;/span&gt; (Scottish Gaelic: Cr\u00ecochan Anglo-Albannach) is an internal &lt;span class=\"searchmatch\"&gt;border&lt;/span&gt; of the United Kingdom separating Scotland and &lt;span class=\"searchmatch\"&gt;England&lt;/span&gt; which runs for\n#\n# Scratchpad: Anglo-Scottish border (Scottish Gaelic: Cr\u00ecochan Anglo-Albannach) is an internal border of the United Kingdom separating Scotland and England which runs for\n# Final Answer: England shares a border with Scotland.\n# England shares a border with Scotland.\n</code></pre> <p>As mentioned in Simon's blog post, this is not a very robust implementation at all and there's a ton of room for improvement. But it is lovely how simple it is with a few lines of Python to make these extra capabilities available to the LLM. And now you can run it locally with an open weights LLM.</p> <p>This example was originally contributed by Alonso Silva.</p>"},{"location":"cookbook/simtom/","title":"Build perspective-taking agents with SimToM","text":"<p>Prompting strategies like Chain-of-Thought (CoT) can improve LLMs' reasoning capabilities. However, they underwhelm in tasks that require keeping track of inconsistent world states. SimToM proposes a simple, two-stage prompting framework for LLMs inspired by Simulation Theory. The authors showed that this approach outperforms zero-shot prompting and CoT on ToMI and BigToM, two benchmarks with Theory of Mind questions.</p> <p>In this example, we will implement SimToM with a few lines of code using Outlines' prompt templating and structured generation capabilities.</p>"},{"location":"cookbook/simtom/#how-simtom-works","title":"How SimToM works","text":"<p>SimToM calls an LLM with two consecutive prompts:</p> <ol> <li>Perspective-taking: The first prompt receives a <code>story</code> and a <code>character</code>. The goal is to understand the situation based on the character's point of view and filter out the rest of the story.</li> <li>Question-Answering: The second prompt receives the character's point of view from the previous step and tasks the LLM to answer a question using that context.</li> </ol> <p></p>"},{"location":"cookbook/simtom/#outlines-implementation","title":"Outlines implementation","text":"<p>To implement SimToM with Outlines, we will need to:</p> <ol> <li>Write the prompts with prompt functions.</li> <li>Define the JSON object each prompt will return using Pydantic.</li> <li>Generate responses with a Mistral model using the transformers integration.</li> </ol> <p>Let's dive into it!</p>"},{"location":"cookbook/simtom/#using-prompt-functions","title":"Using Prompt Functions","text":"<p>With Outlines, you can write your prompts as Python functions by adding the <code>@outlines.prompt</code> decorator. The prompt template is contained in their docstring, and their arguments correspond to variables used in the prompt.</p> <p>The authors have shared their code, prompts and data in this GitHub repository. Below, we define in Outlines the prompts they used for the ToMI dataset:</p> <pre><code>import outlines\n\n\n@outlines.prompt\ndef perspective_taking(story: str, character: str) -&gt; None:\n    \"\"\"&lt;s&gt;[INST] The following is a sequence of events about some characters, that takes place in multiple locations.\n    Your job is to output only the events that the specified character, {{character}}, knows about.\n\n    Here are a few rules:\n    1. A character knows about all events that they do.\n    2. If a character is in a certain room/location, that character knows about all other events that happens in the room. This includes other characters leaving or exiting the location, the locations of objects in that location, and whether somebody moves an object to another place.\n    3. If a character leaves a location, and is NOT in that location, they no longer know about any events that happen within that location. However, they can re-enter the location.\n\n    Story: {{story}}\n    What events does {{character}} know about? Only output the events according to the above rules, do not provide an explanation. [/INST]\"\"\" # noqa\n\n@outlines.prompt\ndef simulation(events: list, name: str, question: str) -&gt; None:\n    \"\"\"&lt;s&gt;[INST] {% for event in events %}\n    {{event}}\n    {% endfor %}\n    You are {{name}}.\n    Based on the above information, answer the following question:\n    {{question}}\n    You must choose one of the above choices, do not say there is not enough information. Answer with a single word, do not output anything else. [/INST]\"\"\" # noqa\n</code></pre>"},{"location":"cookbook/simtom/#json-structured-generation","title":"JSON Structured Generation","text":"<p>Outlines guarantees that the LLM will return a valid JSON object, which we can specify as a Pydantic model.</p> <p>We will need two Pydantic models for SimToM, one for each prompt:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass PerspectiveTaking(BaseModel):\n    \"\"\"This is for the first prompt.\"\"\"\n    character: str = Field(description=\"The character we extract the events for.\")\n    events: List[str] = Field(description=\"All events that the character knows about.\")\n\n\nclass Simulation(BaseModel):\n    \"\"\"This is for the second prompt.\"\"\"\n    answer: str\n</code></pre>"},{"location":"cookbook/simtom/#calling-an-llm","title":"Calling an LLM","text":"<p>Let's try SimToM with an example from the ToMI dataset:</p> <pre><code>story = \"\"\"\n1 Aria entered the front_yard.\n2 Aiden entered the front_yard.\n3 The grapefruit is in the green_bucket.\n4 Aria moved the grapefruit to the blue_container.\n5 Aiden exited the front_yard.\n6 Noah entered the playroom.\n\"\"\"\nquestion = \"7 Where was the grapefruit at the beginning?\"\ncharacter = \"Aria\"\n</code></pre> <p>We load <code>Mistral-7B-Instruct-v0.3</code>, create the prompt using the template we defined earlier, and generate a structured response. As a reminder, the goal of the first call is to get all the events a character, <code>Aria</code>, knows about.</p> <pre><code># Load an LLM from Hugging Face\nMODEL_NAME = \"mistral-community/Mistral-7B-Instruct-v0.3\"\nmodel = outlines.models.transformers(MODEL_NAME, device=\"cuda\")\n\nperspective_prompt = perspective_taking(story=story, character=character)\n\n# Call Mistral 7B with the first prompt\ngenerator = outlines.generate.json(model, PerspectiveTaking)\nperspective = generator(perspective_prompt)\n\nprint(perspective.model_dump())\n# {'character': 'Aria', 'events': ['1 Aria entered the front_yard.', '3 The grapefruit is in the green_bucket.', '4 Aria moved the grapefruit to the blue_container.']}\n</code></pre> <p>Not bad! We will now generate the second prompt with those events.</p> <pre><code>sim_prompt = simulation(events=perspective.events, name=character, question=question)\n\n# Call Mistral 7B with the second prompt\ngenerator = outlines.generate.json(model, Simulation)\nresult = generator(sim_prompt)\n\nprint(result.model_dump())\n# {'answer': 'green_bucket'}\n</code></pre> <p>And this is it! SimToM could be useful in agentic workflows, where agents must act based on what they know, not all available information. One caveat of SimToM is that the perspective-taking step may remove important information, leading to wrong results. As the authors note in their paper, it can feature as a simple and effective baseline for evaluating LLMs on Theory of Mind reasoning tasks.</p>"},{"location":"cookbook/structured_generation_workflow/","title":"Structured Generation Workflow: Generating Synthetic Phone Numbers","text":"<p>This is a condensed version of Coding for Structured Generation with LLMs.</p> <p>For this example we're going to be building an LLM program to generate synthetic data in the form of realistic looking phone numbers for Washington State. Using an LLM for this task is a bit overkill since we could just as easily accomplish this with a tool like Faker, but this example still serves as a useful way to demonstrate a workflow for using structured generation.</p>"},{"location":"cookbook/structured_generation_workflow/#unstructured-approach","title":"Unstructured approach","text":"<p>Before diving into how to use structure generation for this task let's start with an unstructured example. We begin by loading our model:</p> <pre><code>import outlines\n\nmodel_name = 'microsoft/Phi-3-mini-4k-instruct'\nmodel = outlines.models.transformers(model_name)\n</code></pre> <p>Next we need a prompt for this model. Since we're focusing on structured generation, we won't be engaging in any form of \"prompt hacking\" and will be leaving this prompt untouched for the rest of this example.</p> <pre><code>tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmessages_phone = [\n            {\"role\": \"user\", \"content\": \"\"\"\n            Please generate a realistic phone number for Washington State in the following format\n\n            (555) 555-5555\n\n            \"\"\"}\n]\n\n# This allows us to properly format our prompt for\n# Phi-3 Mini's 'Instruct' interface.\nprompt_phone = tokenizer.apply_chat_template(messages_phone, tokenize=False)\n</code></pre> <p>With our prompt ready we can now generate 10 example phone numbers</p> <pre><code>phone_generator_unstruct = outlines.generate.text(model)\nfor _ in range(10):\n    print(phone_generator_unstruct(prompt_phone,max_tokens=12))\n</code></pre> <p>I'd be happy to help you generate a realistic phone\\ I cannot generate a real phone number as I'm just\\ I'm an AI and don't have the ability\\ Sure! Here is a randomly generated phone number in the format\\ Here's a phone number that fits the format for a\\ In Washington State, phone numbers typically have a three-dig\\ Here are a few examples of phone numbers that could be considered\\ I'd be happy to help generate a realistic phone number\\ I'd be happy to help you generate a random phone\\ Based on the format you provided, a realistic phone number for\\</p> <p>As we can see, none of these outputs are even phone numbers!</p> <p>Let's see  if we can improve this using structured generation.</p>"},{"location":"cookbook/structured_generation_workflow/#the-structured-generation-workflow","title":"The Structured Generation Workflow","text":"<p>In order to solve this problem we're going to introduce a Structured Generation Workflow outlined in this image:</p> <p></p> <p>Let's step through this:</p>"},{"location":"cookbook/structured_generation_workflow/#real-example","title":"Real example","text":"<p>We start with a real example phone number, in this case for the Seattle Public Library, that we can use to verify the structure we are creating.</p> <pre><code>phone_number = \"(206) 386-4636\"\n</code></pre> <p>For a simple example like this, we'll just be using a single phone number, for more complex examples it can be helpful to have more examples.</p>"},{"location":"cookbook/structured_generation_workflow/#draft-structure","title":"Draft Structure","text":"<p>The next step in the process is for use to define a simple regex that we feel correctly models our real data.</p> <pre><code>phone_regex_1 = r'\\([0-9]{3}\\) [0-9]{3}-[0-9]{4}'\n</code></pre> <p>Next we need to validate this regex against our real data.</p>"},{"location":"cookbook/structured_generation_workflow/#validate-by-matching-examples","title":"Validate by matching examples","text":"<p>Whenever writing non-trivial code with structured generation it is essential that you first validate the code against your real data example(s).</p> <p>We'll start with a simple method of validation: just checking that our regex matches the data.</p> <pre><code>import re\nre.match(phone_regex_1, phone_number)\n\n# &lt;re.Match object; span=(0, 14), match='(206) 386-4636'&gt;\n</code></pre> <p>Now that we have a match, we can move on to generating structured output!</p>"},{"location":"cookbook/structured_generation_workflow/#generate-structure","title":"Generate Structure","text":"<p>We're ready to see if structured generation can make an improvement over our initial unstructured approach:</p> <pre><code>phone_generator_v1 = outlines.generate.regex(model, phone_regex_1)\nfor _ in range(10):\n    print(phone_generator_v1(prompt_phone))\n</code></pre> <p>(206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234\\ (206) 123-4567\\ (206) 555-1234\\ (206) 555-1234\\ (206) 555-1234</p> <p>At least we have phone numbers! But I think we can do better!</p>"},{"location":"cookbook/structured_generation_workflow/#inspect-output","title":"Inspect output","text":"<p>In this case the model did create phone numbers and, impressively, got the area code correct. So using structured generation did improve things. However these numbers are pretty boring. Let's improve that structure!</p>"},{"location":"cookbook/structured_generation_workflow/#iteration","title":"Iteration","text":"<p>We've walked through the loop once, so we can go quickly now through each iteration.</p> <p>We start by improving our structure:</p> <pre><code>phone_regex_2 = r'\\([0-9]{3}\\) [2-46-9]{3}-[02-9]{4}'\n</code></pre> <p>Before rushing to another round of generation, let's validate this new regex. We'll add just a bit more sophistication over our last check:</p> <p><pre><code>re.match(phone_regex_2, phone_number)[0] == phone_number\n# True\n</code></pre> Now that we've validated, let's generate with this new regex!</p> <pre><code>phone_generator_v2 = outlines.generate.regex(model,\n                                             phone_regex_2)\nfor _ in range(10):\n    print(phone_generator_v2(prompt_phone))\n</code></pre> <p>(206) 867-5309\\ (206) 666-7777\\ (206) 444-3333\\ (206) 444-3333\\ (206) 943-2222\\ (206) 323-6789\\ (206) 444-3333\\ (206) 867-5309\\ (206) 466-2255\\ (206) 222-3333</p> <p>Better, but I don't like those repeated sequences. Like good software developers, let's iterate again!</p>"},{"location":"cookbook/structured_generation_workflow/#reiteration-with-debugging","title":"Reiteration - with debugging","text":"<p>Here's a fancier regex that should give us more interesting results:</p> <pre><code>phone_regex_3_error = r'\\([0-9]{3}\\) [2-4][7-9][4-6]-[3-6][2-8][1-4]'\n</code></pre> <p>This looks good to me, but there's a subtle bug, that's why we always need to validate our structure against real data. This time we'll make our validator do a bit more work to verify the correct string is matched:</p> <p><pre><code>if not re.match(phone_regex_3_error, phone_number):\n    print(\"Regex fails match\")\nelse:\n    matched_string = re.match(phone_regex_3_error, phone_number)[0]\n    if matched_string == phone_number:\n    print(\"Successful match\")\n    else:\n    print(f\"Error {matched_string} != {phone_number}\")\n</code></pre> This prints out:</p> <p>Error (206) 386-463 != (206) 386-4636</p> <p>Ah! We were missing the last digit, let's fix that and regenerate:</p> <pre><code>phone_regex_3_fixed = r'\\([0-9]{3}\\) [2-4][7-9][4-6]-[3-6][2-8][1-4][6-9]'\nphone_generator_v3 = outlines.generate.regex(model,\n                                             phone_regex_3_fixed)\nfor _ in range(10):\n    print(phone_generator_v3(prompt_phone))\n</code></pre> <p>(206) 494-3216\\ (206) 374-6218\\ (206) 494-3337\\ (206) 476-3216\\ (206) 484-3548\\ (206) 495-3218\\ (206) 494-5517\\ (206) 375-4636\\ (206) 384-6216\\ (206) 385-6218</p> <p>Much better!</p> <p>Now you've seen a quick example of the structured generation workflow that can be used at the basis for building and iteration on much larger structured generation tasks!</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#structured-generation","title":"Structured generation","text":"<p>While LLM capabilities are increasingly impressive, we can make their output more reliable by steering the generation. Outlines thus offers mechanisms to specify high level constraints on text completions by generative language models.</p> <p>Stopping sequence By default, language models stop generating tokens after and  token was generated, or after a set maximum number of tokens. Their output can be verbose, and for practical purposes it is often necessary to stop the generation after a given sequence has been found instead. You can use the stop_at keyword argument when calling the model with a prompt: <pre><code>import outlines.models as models\n\ncomplete = models.openai(\"gpt-4o-mini\")\nexpert = complete(\"Name an expert in quantum gravity.\", stop_at=[\"\\n\", \".\"])\n</code></pre>"},{"location":"reference/functions/","title":"Outlines functions","text":""},{"location":"reference/prompting/","title":"Prompt templating","text":"<p>Outlines provides a powerful domain-specific language to write and manage prompts, via what we call prompt functions.  Prompt functions are Python functions that contain a template for the prompt in their docstring, and their arguments correspond to the variables used in the prompt. When called, a prompt function returns the template rendered with the values of the arguments.</p> <p>The aim of prompt functions is to solve several recurrent problems with prompting:</p> <ol> <li>Building complex prompts quickly leads to messy code. This problem has    already been solved in the web development community by using templating, so    why not use it here?</li> <li>Composing prompts is difficult. Why not just compose functions?</li> <li>Separating prompts from code. Encapsulation in functions allows a clean    separation between prompts and code. Moreover, like any function, prompt    functions can be imported from other modules.</li> </ol> <p>Outlines uses the Jinja templating engine to render prompts, which allows to easily compose complex prompts.</p> <p>Prompt rendering</p> <p>Prompt functions are opinionated when it comes to prompt rendering. These opinions are meant to avoid common prompting errors, but can have unintended consequences if you are doing something unusual. We advise to always print the prompt before using it. You can also read the reference section if you want to know more.</p>"},{"location":"reference/prompting/#your-first-prompt","title":"Your first prompt","text":"<p>The following snippet showcases a very simple prompt. The variables between curly brackets <code>{{  }}</code> are placeholders for the values of the arguments you will pass to the prompt function.</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef greetings(name, question):\n    \"\"\"Hello, {{ name }}!\n    {{ question }}\n    \"\"\"\n\nprompt = greetings(\"user\", \"How are you?\")\nprint(prompt)\n</code></pre> <pre><code>Hello, user!\nHow are you?\n</code></pre> <p>If a variable is missing in the function's arguments, Jinja2 will throw an <code>UndefinedError</code> exception:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef greetings(name):\n    \"\"\"Hello, {{ surname }}!\"\"\"\n\nprompt = greetings(\"user\")\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 9, in &lt;module&gt;\n  File \"/home/remi/projects/normal/outlines/outlines/prompts.py\", line 38, in __call__\n      return render(self.template, **bound_arguments.arguments)\n  File \"/home/remi/projects/normal/outlines/outlines/prompts.py\", line 213, in render\n      return jinja_template.render(**values)\n  File \"/home/remi/micromamba/envs/outlines/lib/python3.9/site-packages/jinja2/environment.py\", line 1301, in render\n      self.environment.handle_exception()\n  File \"/home/remi/micromamba/envs/outlines/lib/python3.9/site-packages/jinja2/environment.py\", line 936, in handle_exception\n      raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 1, in top-level template code\n  jinja2.exceptions.UndefinedError: 'surname' is undefined\n</code></pre>"},{"location":"reference/prompting/#importing-prompt-functions","title":"Importing prompt functions","text":"<p>Prompt functions are functions, and thus can be imported from other modules:</p> prompts.pygenerate.pyOutput <pre><code>import outlines\n\n@outlines.prompt\ndef greetings(name, question):\n    \"\"\"Hello, {{ name }}!\n    {{ question }}\n    \"\"\"\n</code></pre> <pre><code>from .prompts import greetings\n\nprompt = greetings(\"John Doe\", \"How are you today?\")\n</code></pre> <pre><code>Hello, John Doe!\nHow are you today?\n</code></pre>"},{"location":"reference/prompting/#few-shot-prompting","title":"Few-shot prompting","text":"<p>Few-shot prompting can lead to messy code. Prompt functions allow you to loop over lists or dictionaries from the template. In the following example we demonstrate how we can generate a prompt by passing a list of dictionaries with keys <code>question</code> and <code>answer</code> to the prompt function:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef few_shots(instructions, examples, question):\n    \"\"\"{{ instructions }}\n\n    Examples\n    --------\n\n    {% for example in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n\n    {% endfor %}\n    Question\n    --------\n\n    Q: {{ question }}\n    A:\n    \"\"\"\n\ninstructions = \"Please answer the following question following the examples\"\nexamples = [\n    {\"question\": \"2+2=?\", \"answer\":4},\n    {\"question\": \"3+3=?\", \"answer\":6}\n]\nquestion = \"4+4 = ?\"\n\nprompt = few_shots(instructions, examples, question)\nprint(prompt)\n</code></pre> <pre><code>Please answer the following question following the examples\n\nExamples\n--------\n\nQ: 2+2=?\nA: 4\n\nQ: 3+3=?\nA: 6\n\nQuestion\n--------\n\nQ: 4+4 = ?\nA:\n</code></pre>"},{"location":"reference/prompting/#conditionals-filters-etc","title":"Conditionals, filters, etc.","text":"<p>Jinja2 has many features beyond looping that are not described here: conditionals, filtering, formatting, etc. Please refer to the Jinja documentation for more information about the syntax of the templating language. The Jinja syntax is powerful, and we recommend you take some time to read their documentation if you are building complex prompts.</p>"},{"location":"reference/prompting/#tools","title":"Tools","text":"<p>Several projects (e.g.Toolformer, ViperGPT, AutoGPT, etc.) have shown that we can \"teach\" language models to use external functions by describing what these functions do in the prompt. In these projects the same information is often repeated twice: the function implementation, name, docstring, or arguments are copy-pasted in the prompt. This is cumbersome and error prone; you can directly pull this information from within an Outlines prompt function:</p> CodeOutput <pre><code>import outlines\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n\n@outlines.prompt\ndef tool_prompt(question, tool):\n    \"\"\"{{ question }}\n\n    COMMANDS\n    1. {{ tool | name }}: {{ tool | description }}, args: {{ tool | args }}\n\n    {{ tool | source }}\n    \"\"\"\n\nprompt = tool_prompt(\"Can you do something?\", my_tool)\nprint(prompt)\n</code></pre> <pre><code>Can you do something?\n\nCOMMANDS\n1. my_tool: Tool description., args: arg1: str, arg2: int\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompting/#json-response-format","title":"JSON response format","text":"<p>To build reliable chains with language models we often need to instruct them the format in which we would like them to return their response.</p> <p>Without prompt templating, the information is repeated twice between creating the parsing function (e.g. a Pydantic model), and writing the desired schema in the prompt. This can lead to errors that are hard to debug.</p> <p>Outlines allows you to directly pull the JSON schema of a pydantic model, or pretty print a dictionary from within an Outlines prompt function</p> CodeOutput <pre><code>from pydantic import BaseModel, Field\n\nimport outlines\n\nclass MyResponse(BaseModel):\n    field1: int = Field(description=\"an int\")\n    field2: str\n\n@outlines.prompt\ndef my_prompt(response_model):\n    \"\"\"{{ response_model | schema }}\"\"\"\n\nprompt = my_prompt(MyResponse)\nprint(prompt)\n# {\n#   \"field1\": \"an int\",\n#   \"field2\": \"&lt;field2&gt;\"\n# }\n</code></pre> <pre><code>response = {\n    \"field1\": \"&lt;field1&gt;\",\n    \"field2\": \"a string\"\n}\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"&lt;field1&gt;\",\n#   \"field2\": \"a string\"\n# }\n</code></pre>"},{"location":"reference/prompting/#formatting-conventions","title":"Formatting conventions","text":"<p>Prompt functions are opinionated when it comes to rendering, and these opinions are meant to avoid prompting mistakes and help with formatting.</p>"},{"location":"reference/prompting/#whitespaces","title":"Whitespaces","text":"<p>If you have experience working with strings between triple quotes you know that indenting has an influence on the string's formatting. Prompt functions adopt a few conventions so you don't have to think about indents when writing prompt.</p> <p>First, whether you start the prompt right after the triple quotes or on the line below does not matter for formatting:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef prompt1():\n    \"\"\"My prompt\n    \"\"\"\n\n@outlines.prompt\ndef prompt2():\n    \"\"\"\n    My prompt\n    \"\"\"\n\nprint(prompt1())\nprint(prompt2())\n</code></pre> <pre><code>My prompt\nMy prompt\n</code></pre> <p>Indentation is relative to the second line of the docstring, and leading spaces are removed:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef example1():\n    \"\"\"First line\n    Second line\n    \"\"\"\n\n@outlines.prompt\ndef example2():\n    \"\"\"\n      Second line\n      Third line\n    \"\"\"\n\n@outlines.prompt\ndef example3():\n    \"\"\"\n      Second line\n        Third line\n    \"\"\"\n\nprint(example1())\nprint(example2())\nprint(example3())\n</code></pre> <pre><code>First line\nSecond line\n\nSecond line\nThird line\n\nSecond line\n  Third line\n</code></pre> <p>Trailing whitespaces are not removed, unless they follow a linebreak symbol <code>\\</code> (see linebreaks).</p>"},{"location":"reference/prompting/#linebreaks","title":"Linebreaks","text":"<p>You can use the backslash <code>\\</code> to break a long line of text. It will render as a single line:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef example():\n   \"\"\"\n   Break in \\\n   several lines \\\n   But respect the indentation\n       on line breaks.\n   And after everything \\\n   Goes back to normal\n   \"\"\"\n\nprint(example())\n</code></pre> <pre><code>Break in several lines But respect the indentation\n    on line breaks.\nAnd after everything Goes back to normal\n</code></pre>"},{"location":"reference/samplers/","title":"Samplers","text":"<p>Outlines offers different sequence sampling algorithms, and we will integrate more in the future. You can read this blog post for an overview of the different sampling algorithm.</p> <p>Samplers provide control over the sampling process, allowing you to influence the output of the model. This can include controlling randomness (temperature), biasing towards certain tokens (top-k, top-p), or sequence generation (beam search).</p>"},{"location":"reference/samplers/#multinomial-sampling","title":"Multinomial sampling","text":"<p>Multinomial sampling is the default sampling algorithm in Outlines.</p> <p>As an example, suppose we have only two possible tokens: \"H\" and \"T\". For a fixed prompt such as \"Flip a coin, did you get heads or tails?\" The language model calculates probability for each token:</p> Token Probability \"H\" 0.5 \"T\" 0.5 <p>You'd expect to receive \"H\" 50% of the time and \"T\" 50% of the time.</p>"},{"location":"reference/samplers/#parameters","title":"Parameters","text":"<ul> <li><code>samples</code>: Number of samples to generate (default: 1)</li> <li><code>top_k</code>: Only consider the top k tokens (optional)</li> <li><code>top_p</code>: Only consider the top tokens with cumulative probability &gt;= p (optional)</li> <li><code>temperature</code>: Controls randomness of sampling (optional)</li> </ul>"},{"location":"reference/samplers/#default-behavior","title":"Default behavior","text":"<p>Outlines defaults to the multinomial sampler without top-p or top-k sampling, and temperature equal to 1.</p> <p>Not specifying a sampler is equivalent to:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial()\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre>"},{"location":"reference/samplers/#batching","title":"Batching","text":"<p>You can ask the generator to take multiple samples by passing the number of samples when initializing the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial(3)\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# [4, 4, 4]\n</code></pre> <p>If you ask multiple samples for a batch of prompts the returned array will be of shape <code>(num_samples, num_batches)</code>:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial(3)\n\ngenerator = generate.text(model, sampler)\nanswer = generator([\"What is 2+2?\", \"What is 3+3?\"])\n\nprint(answer)\n# [[4, 4, 4], [6, 6, 6]]\n</code></pre>"},{"location":"reference/samplers/#temperature","title":"Temperature","text":"<p>You can control the temperature with</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.multinomial(3, temperature=0.5)\n\ngenerator = generate.text(model, sampler)\nanswer = generator([\"What is 2+2?\", \"What is 3+3?\"])\n\nprint(answer)\n</code></pre> <p>If you would like to use <code>temperature=0.0</code>, please use <code>sampler=samplers.greedy()</code> instead.</p>"},{"location":"reference/samplers/#top-k-sampling","title":"Top-k sampling","text":"<p>You can ask Outlines to only consider the top-k logits at each step by specifying the value of the <code>top-k</code> keyword argument when initializing the sampler.</p> <pre><code>sampler = samplers.multinomial(3, top_k=10)\n</code></pre>"},{"location":"reference/samplers/#top-p-sampling","title":"Top-p sampling","text":"<p>You can ask Outlines to only consider the highest probability tokens such that their cumulative probability is greater than a threshold <code>p</code>. Specify the <code>top_p</code> keyword argument when initializing the sampler:</p> <pre><code>sampler = samplers.multinomial(3, top_p=0.95)\n</code></pre>"},{"location":"reference/samplers/#greedy-sampler","title":"Greedy sampler","text":"<p>Greedy sampling selects the token with the highest probability at each step. It's deterministic and always produces the same output for a given input.</p> <p>To use the greedy sampler, initialize the generator with the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.greedy()\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre> <p>You cannot ask for multiple samples with the greedy sampler since it does not clear what the result should be. Only the most likely token can be returned.</p>"},{"location":"reference/samplers/#beam-search","title":"Beam Search","text":"<p>Beam search maintains multiple candidate sequences at each step, potentially finding better overall sequences than greedy or multinomial sampling.</p> <p>To use Beam Search, initialize the generator with the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\nsampler = samplers.beam_search(beams=5)\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre> <p>Compatibility</p> <p>Only models from the <code>transformers</code>  and <code>exllamav2</code> libraries are compatible with Beam Search.</p>"},{"location":"reference/samplers/#parameters_1","title":"Parameters","text":"<ul> <li><code>beams</code>: Number of beams to use (default: 1)</li> </ul>"},{"location":"reference/samplers/#sampler-comparison","title":"Sampler Comparison","text":"<p>Here's a table comparing the different samplers:</p> Sampler Pros Cons Use Cases Greedy Deterministic, fast May produce repetitive text When you need consistent, predictable output Multinomial Balances exploration and exploitation Results may vary between runs General-purpose text generation, creative tasks Beam Search Can find globally better sequences More computationally expensive When sequence quality is critical, e.g., translation <p>For most use cases, we recommend using the default multinomial sampler.</p>"},{"location":"reference/text/","title":"Text generation","text":"<p>Outlines provides a unified interface to generate text with many language models, API-based and local. The same pattern is used throughout the library:</p> <ol> <li>Instantiate a generator by calling <code>outlines.generate.text</code> with the model to be used.</li> <li>Call the generator with the prompt and (optionally) some generation parameters.</li> </ol> <pre><code>from outlines import models, generate\n\nmodel = models.openai(\"gpt-4o-mini\")\ngenerator = generate.text(model)\nanswer = generator(\"What is 2+2?\")\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\nanswer = generator(\"What is 2+2?\")\n</code></pre> <p>By default Outlines uses the multinomial sampler with <code>temperature=1</code>. See this section to learn how to use different samplers.</p>"},{"location":"reference/text/#streaming","title":"Streaming","text":"<p>Outlines allows you to stream the model's response by calling the <code>.stream</code> method of the generator with the prompt:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\ntokens = generator.stream(\"What is 2+2?\")\nfor token in tokens:\n    print(token)\n</code></pre>"},{"location":"reference/text/#parameters","title":"Parameters","text":""},{"location":"reference/text/#limit-the-number-of-tokens-generated","title":"Limit the number of tokens generated","text":"<p>To limit the number of tokens generated you can pass the <code>max_tokens</code> positional argument to the generator:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\nanswer = generator(\"What is 2+2?\", 5)\nanswer = generator(\"What is 2+2?\", max_tokens=5)\n</code></pre>"},{"location":"reference/text/#stop-after-a-given-string-is-generated","title":"Stop after a given string is generated","text":"<p>You can also ask the model to stop generating text after a given string has been generated, for instance a period or a line break. You can pass a string or a line of string for the <code>stop_at</code> argument:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\nanswer = generator(\"What is 2+2?\", stop_at=\".\")\nanswer = generator(\"What is 2+2?\", stop_at=[\".\", \"\\n\"])\n</code></pre> <p>The stopping string will be included in the response.</p>"},{"location":"reference/text/#seed-the-generation","title":"Seed the generation","text":"<p>It can be useful to seed the generation in order to get reproducible results:</p> <pre><code>import torch\nfrom outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\nseed = 789001\n\nanswer = generator(\"What is 2+2?\", seed=seed)\n</code></pre>"},{"location":"reference/generation/cfg/","title":"Grammar-structured generation","text":"<p>You can pass any context-free grammar in the EBNF format and Outlines will generate an output that is valid to this grammar:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = \"\"\"\n    ?start: expression\n\n    ?expression: term ((\"+\" | \"-\") term)*\n\n    ?term: factor ((\"*\" | \"/\") factor)*\n\n    ?factor: NUMBER\n           | \"-\" factor\n           | \"(\" expression \")\"\n\n    %import common.NUMBER\n\"\"\"\n\nmodel = models.transformers(\"WizardLM/WizardMath-7B-V1.1\")\ngenerator = generate.cfg(model, arithmetic_grammar)\nsequence = generator(\n  \"Alice had 4 apples and Bob ate 2. \"\n  + \"Write an expression for Alice's apples:\"\n)\n\nprint(sequence)\n# (8-2)\n</code></pre>"},{"location":"reference/generation/cfg/#disclaimer","title":"Disclaimer","text":"<p>Experimental</p> <p>Outlines current community-contributed implementation of CFG-structured generation is experimental. This does not reflect the performance of .txt's product, where we have optimized grammar-structured generation to be as fast as regex-structured generation. Additionally, it does not fully align with the approach described in our technical report, aside from its use of incremental/partial parsing. This feature is still a work in progress, requiring performance enhancements and bug fixes for an ideal implementation. For more details, please see our grammar-related open issues on GitHub.</p> <p>Greedy</p> <p>To mitigate performance issues, CFG-structured generation will use rejection sampling and iterate over the candidate tokens highest logit first,, completing once a single valid token ID is selected. This is effectively greedy generation.</p>"},{"location":"reference/generation/cfg/#ready-to-use-grammars","title":"Ready-to-use grammars","text":"<p>Outlines contains a (small) library of grammars that can be imported and use directly. We can rewrite the previous example as:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = outlines.grammars.arithmetic\n\nmodel = models.transformers(\"WizardLM/WizardMath-7B-V1.1\")\ngenerator = generate.cfg(model, arithmetic_grammar)\nsequence = generator(\n  \"Alice had 4 apples and Bob ate 2. \"\n  + \"Write an expression for Alice's apples:\"\n)\n\nprint(sequence)\n# (8-2)\n</code></pre> <p>The following grammars are currently available:</p> <ul> <li>Arithmetic grammar via <code>outlines.grammars.arithmetic</code></li> <li>JSON grammar via <code>outlines.grammars.json</code></li> </ul> <p>If you would like more grammars to be added to the repository, please open an issue or a pull request.</p>"},{"location":"reference/generation/cfg/#grammar-guide","title":"Grammar guide","text":"<p>A grammar is a list of rules and terminals that define a language:</p> <ul> <li>Terminals define the vocabulary of the language; they may be a string, regular expression or combination of these and other terminals.</li> <li>Rules define the structure of that language; they are a list of terminals and rules.</li> </ul> <p>Outlines uses the Lark library to make Large Language Models generate text in a language of a grammar, it thus uses grammars defined in a format that Lark understands, based on the EBNF syntax. Read the Lark documentation for more details on grammar, the following is a small primer that should help get your started.</p> <p>In the following we will define a LOGO-like toy language for python's turtle library.</p>"},{"location":"reference/generation/cfg/#terminals","title":"Terminals","text":"<p>A turtle can take 4 different <code>MOVEMENT</code> move instructions: forward (<code>f</code>), backward (<code>b</code>), turn right (<code>r</code>) and turn left (<code>l</code>). It can take <code>NUMBER</code> number of steps in each direction, and draw lines in a specified <code>COLOR</code>. These define the vocabulary of our language:</p> <pre><code>MOVEMENT: \"f\"|\"b\"|\"r\"|\"l\"\nCOLOR: LETTER+\n\n%import common.LETTER\n%import common.INT -&gt; NUMBER\n%import common.WS\n%ignore WS\n</code></pre> <p>The lines that start with <code>%</code> are called \"directive\". They allow to import pre-defined terminals and rules, such as <code>LETTER</code> and <code>NUMBER</code>. <code>LETTER+</code> is a regular expressions, and indicates that a <code>COLOR</code> is made of at least one <code>LETTER</code>. The last two lines specify that we will ignore white spaces (<code>WS</code>) in the grammar.</p>"},{"location":"reference/generation/cfg/#rules","title":"Rules","text":"<p>We now need to define our rules, by decomposing instructions we can send to the turtle via our python program. At each line of the program, we can either choose a direction and execute a given number of steps, change the color used to draw the pattern. We can also choose to start filling, make a series of moves, and stop filling. We can also choose to repeat a series of move.</p> <p>We can easily write the first two rules:</p> <pre><code>instruction: MOVEMENT NUMBER   -&gt; movement\n           | \"c\" COLOR [COLOR] -&gt; change_color\n</code></pre> <p>where <code>movement</code> and <code>change_color</code> represent aliases for the rules. A whitespace implied concatenating the elements, and <code>|</code> choosing either of the elements. The <code>fill</code> and <code>repeat</code> rules are slightly more complex, since they apply to a code block, which is made of instructions. We thus define a new <code>code_block</code>  rule that refers to <code>instruction</code> and finish implementing our rules:</p> <pre><code>instruction: MOVEMENT NUMBER            -&gt; movement\n           | \"c\" COLOR [COLOR]          -&gt; change_color\n           | \"fill\" code_block          -&gt; fill\n           | \"repeat\" NUMBER code_block -&gt; repeat\n\ncode_block: \"{\" instruction \"}\"\n</code></pre> <p>We can now write the full grammar:</p> <pre><code>start: instruction+\n\ninstruction: MOVEMENT NUMBER            -&gt; movement\n            | \"c\" COLOR [COLOR]          -&gt; change_color\n            | \"fill\" code_block          -&gt; fill\n            | \"repeat\" NUMBER code_block -&gt; repeat\n\ncode_block: \"{\" instruction+ \"}\"\n\nMOVEMENT: \"f\"|\"b\"|\"l\"|\"r\"\nCOLOR: LETTER+\n\n%import common.LETTER\n%import common.INT -&gt; NUMBER\n%import common.WS\n%ignore WS\n</code></pre> <p>Notice the <code>start</code> rule, which defines the starting point of the grammar, i.e. the rule with which a program must start. This full grammars allows us to parse programs such as:</p> <pre><code>c red yellow\n    fill { repeat 36 {\n        f200 l170\n    }}\n</code></pre> <p>The result of the parse, the parse tree, can then easily be translated into a Python program that uses the <code>turtle</code> library to draw a pattern.</p>"},{"location":"reference/generation/cfg/#next-steps","title":"Next steps","text":"<p>This section provides a very brief overview of grammars and their possibilities. Check out the Lark documentation for more thorough explanations and more examples.</p>"},{"location":"reference/generation/choices/","title":"Multiple choices","text":"<p>Oultines allows you to make sure the generated text is chosen between different options:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.choice(model, [\"skirt\", \"dress\", \"pen\", \"jacket\"])\nanswer = generator(\"Pick the odd word out: skirt, dress, pen, jacket\")\n</code></pre> <p>Performance</p> <p><code>generation.choice</code> computes an index that helps Outlines guide generation. This can take some time, but only needs to be done once. If you want to generate from the same list of choices several times make sure that you only call <code>generate.choice</code> once.</p>"},{"location":"reference/generation/creating_grammars/","title":"Overview","text":"<p>Outlines allows the use of Lark grammars to guide generation. These grammars are used to construct parsers that filter out incompatible tokens during the generation process The result is a generation that adheres to the grammar's production rules.</p>"},{"location":"reference/generation/creating_grammars/#primer-on-creating-grammars","title":"Primer on Creating Grammars","text":"<p>To create grammars for Outlines, a solid understanding of Lark grammars is necessary. Here's how you can get started:</p> <ul> <li>Read Lark's grammars documentations here.</li> <li>Review Outlines' existing grammars here.</li> </ul>"},{"location":"reference/generation/creating_grammars/#compatibility-with-outlines","title":"Compatibility With Outlines","text":"<p>It's important to note that not all Lark grammars work with Outlines. Changes may be necessary to ensure compatability.</p>"},{"location":"reference/generation/creating_grammars/#lalr1-parser","title":"LALR(1) Parser","text":"<p>Outlines utilizes Larks LALR(1) parser, meaning the grammar must be unambiguous at least up to the next token (one token lookahead). Read Lark's official LALR(1) parser documentation here.</p> <p>If your grammar is ambiguous, you will recieve the following error at runtime:</p> <pre><code>GrammarError: Reduce/Reduce collision in Terminal('B') between the following rules:\n</code></pre>"},{"location":"reference/generation/creating_grammars/#regex-terminal-restrictions","title":"Regex Terminal Restrictions","text":"<p>Outlines converts terminals to finite state machines using the Interegular library. Not all regular expressions work with Interegular, mitigation is described in the subsections which follow.</p>"},{"location":"reference/generation/creating_grammars/#avoid-lookarounds","title":"Avoid Lookarounds","text":"<p>Examples of removing lookaround while maintaining the same functionality</p>"},{"location":"reference/generation/creating_grammars/#example-escaped-string","title":"Example: Escaped String","text":"<p>From Outlines' modified <code>ESCAPED_STRING</code> in common.lark.</p> <p>Before: <pre><code>_STRING_INNER: /.*?/\n_STRING_ESC_INNER: _STRING_INNER /(?&lt;!\\\\)(\\\\\\\\)*?/\n\nESCAPED_STRING : \"\\\"\" _STRING_ESC_INNER \"\\\"\"\n</code></pre></p> <p>After: <pre><code>_NON_CONTROL_CHAR: /([^\"\\\\\\x00-\\x1F\\x7F-\\x9F])/\n_ESCAPED_CHAR: /\\\\/ (_NON_CONTROL_CHAR | /\\\\/ | /\"/)\nESCAPED_STRING_INNER: _NON_CONTROL_CHAR | _ESCAPED_CHAR\nESCAPED_STRING: /\"/ ESCAPED_STRING_INNER* /\"/\n</code></pre></p>"},{"location":"reference/generation/creating_grammars/#avoid-backreferences","title":"Avoid Backreferences","text":"<p>Backreferences, for example <code>([ab]^*)\\1</code>, cannot be simulated by a finite state machine, and will result in an error if used.</p>"},{"location":"reference/generation/creating_grammars/#creating-a-valid-grammar","title":"Creating a Valid Grammar","text":"<p>You can use Outlines' test suite to verify your grammar.</p>"},{"location":"reference/generation/creating_grammars/#1-create-your-grammar","title":"1) Create Your Grammar","text":"<p>Create your grammar file named <code>your_new_grammar.lark</code>, adhering to the guidelines provided above. Add it to <code>outlines/grammars/</code> (ensure attribution is included and license is compatible).</p> <p>Update <code>outlines/grammars.py</code> with a line including your grammar.</p>"},{"location":"reference/generation/creating_grammars/#2-test-your-grammar","title":"2) Test Your Grammar","text":"<p>Test grammar for false negatives, ensure sample grammars can be generated: - Add valid example outputs which are compliant with the grammar to <code>tests/benchmark/cfg_samples/your_new_grammar/</code> - Run the tests for your grammar via <code>pytest -s tests/fsm/test_cfg_guide.py::test_cfg_grammar_sample -k \"your_new_grammar\"</code></p> <p>Test grammar for false positives, ensure invalid outputs aren't generated.</p> <p>Currently there isn't a builtin false positive testing utility. It is recommended you smoke test via <pre><code>from outlines import models, generate, grammars\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.cfg(model, grammars.your_new_grammar)\nresult = generator(&lt;your prompt to generate output for your grammar&gt;)\nprint(result)\n</code></pre></p>"},{"location":"reference/generation/creating_grammars/#converting","title":"Converting","text":"<p>There are a few tools available for converting from other grammars to lark. These tools serve as a starting point. However, you will typically need to make additional adjustments to ensure full compatibility and proper functioning within Outlines.</p> <p>Tools: - Larks built in \"Nearley-to-Lark\" converter https://lark-parser.readthedocs.io/en/latest/tools.html - Convert ANTLR4 to Lark (Note, most antlr4 grammars are not LALR(1) compatible, so will require additional tweaking) https://github.com/kaby76/Domemtech.Trash/blob/main/src/trconvert/readme.md - Extract EBNF from Yacc files https://www.bottlecaps.de/rr/ui</p> <p>Reference Grammars: - Github Lark Grammars https://github.com/search?q=path%3A.lark&amp;type=code - Github Nearley Grammars https://github.com/search?q=path%3A.ne+%22-%3E%22&amp;type=code - Antlr4 grammars https://github.com/antlr/grammars-v4/ - Grammar zoo https://slebok.github.io/zoo/index.html#html</p>"},{"location":"reference/generation/custom_fsm_ops/","title":"Custom FSM Operations","text":"<p>Outlines is fast because it compiles regular expressions into an index ahead of inference. To do so we use the equivalence between regular expressions and Finite State Machines (FSMs), and the library interegular to perform the translation.</p> <p>Alternatively, one can pass a FSM built using <code>integular</code> directly to structure the generation.</p>"},{"location":"reference/generation/custom_fsm_ops/#example","title":"Example","text":""},{"location":"reference/generation/custom_fsm_ops/#using-the-difference-operation","title":"Using the <code>difference</code> operation","text":"<p>In the following example we build a fsm which recognizes only the strings valid to the first regular expression but not the second. In particular, it will prevent the words \"pink\" and \"elephant\" from being generated:</p> <pre><code>import interegular\nfrom outlines import models, generate\n\n\nlist_of_strings_pattern = \"\"\"\\[\"[^\"\\s]*\"(?:,\"[^\"\\s]*\")*\\]\"\"\"\npink_elephant_pattern = \"\"\".*(pink|elephant).*\"\"\"\n\nlist_of_strings_fsm = interegular.parse_pattern(list_of_strings_pattern).to_fsm()\npink_elephant_fsm = interegular.parse_pattern(pink_elephant_pattern).to_fsm()\n\ndifference_fsm = list_of_strings_fsm - pink_elephant_fsm\n\ndifference_fsm_fsm.accepts('[\"a\",\"pink\",\"elephant\"]')\n# False\ndifference_fsm_fsm.accepts('[\"a\",\"blue\",\"donkey\"]')\n# True\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.fsm(model, difference_fsm)\nresponse = generator(\"Don't talk about pink elephants\")\n</code></pre> <p>To see the other operations available, consult interegular's documentation.</p>"},{"location":"reference/generation/format/","title":"Type constraints","text":"<p>We can ask completions to be restricted to valid python types:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.format(model, int)\nanswer = generator(\"When I was 6 my sister was half my age. Now I\u2019m 70 how old is my sister?\")\nprint(answer)\n# 67\n</code></pre> <p>The following types are currently available:</p> <ul> <li>int</li> <li>float</li> <li>bool</li> <li>datetime.date</li> <li>datetime.time</li> <li>datetime.datetime</li> <li>We also provide custom types</li> </ul>"},{"location":"reference/generation/generation/","title":"Generation","text":"<p>Once an Outlines model is constructed you can use <code>outlines.generate</code> to generate text. Standard LLM generation is possible via <code>outlines.generate.text</code>, along with a variety of structured generation methods described below. (For a detailed technical explanation of how structured generation works, you may review the Structured Generation Explanation page)</p> <p>Before generating text, you must construct an <code>outlines.model</code>. Example:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda\")\n</code></pre>"},{"location":"reference/generation/generation/#text-generator","title":"Text generator","text":"<pre><code>generator = outlines.generate.text(model)\n\nresult = generator(\"Question: What's 2+2? Answer:\", max_tokens=100)\nprint(result)\n# The answer is 4\n\n# Outlines also supports streaming output\nstream = generator.stream(\"What's 2+2?\", max_tokens=4)\nfor i in range(5):\n    token = next(stream)\n    print(repr(token))\n# '2'\n# '+'\n# '2'\n# ' equals'\n# '4'\n</code></pre>"},{"location":"reference/generation/generation/#multi-label-classification","title":"Multi-label classification","text":"<p>Outlines allows you to do multi-label classification by guiding the model so it can only output either of the specified choices:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = outlines.generate.choice(model, [\"Blue\", \"Red\", \"Yellow\"])\n\ncolor = generator(\"What is the closest color to Indigo? \")\nprint(color)\n# Blue\n</code></pre>"},{"location":"reference/generation/generation/#json-structured-generation","title":"JSON-structured generation","text":"<p>Outlines can guide models so that they output valid JSON 100% of the time. You can either specify the structure using Pydantic or a string that contains a JSON Schema:</p> PydanticJSON Schema <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr, conint\n\nimport outlines\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: conint(gt=18, lt=99)\n    armor: Armor\n    strength: conint(gt=1, lt=100)\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = outlines.generate.json(model, Character)\n\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# name='Orla' age=21 armor=&lt;Armor.plate: 'plate'&gt; strength=8\n</code></pre> <pre><code>import outlines\n\nschema = \"\"\"{\n    \"$defs\": {\n        \"Armor\": {\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"title\": \"Armor\",\n            \"type\": \"string\"\n        }\n    },\n    \"properties\": {\n        \"name\": {\"maxLength\": 10, \"title\": \"Name\", \"type\": \"string\"},\n        \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\n        \"armor\": {\"$ref\": \"#/$defs/Armor\"},\n        \"strength\": {\"title\": \"Strength\", \"type\": \"integer\"}\\\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"strength\"],\n    \"title\": \"Character\",\n    \"type\": \"object\"\n}\"\"\"\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = outlines.generate.json(model, schema)\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# {'name': 'Yuki', 'age': 24, 'armor': 'plate', 'strength': 3}\n</code></pre> <p>Note</p> <p>We advise you to constrain the length of the strings fields when first testing your schema, especially with small models.</p>"},{"location":"reference/generation/generation/#grammar-structured-generation","title":"Grammar-structured generation","text":"<p>Outlines also allows to generate text that is valid to any context-free grammar (CFG) in the EBNF format. Grammars can be intimidating, but they are a very powerful tool! Indeed, they determine the syntax of every programming language, valid chess moves, molecule structure, can help with procedural graphics generation, etc.</p> <p>Here we show a simple example of a grammar that defines arithmetic operations:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = \"\"\"\n    ?start: sum\n\n    ?sum: product\n        | sum \"+\" product   -&gt; add\n        | sum \"-\" product   -&gt; sub\n\n    ?product: atom\n        | product \"*\" atom  -&gt; mul\n        | product \"/\" atom  -&gt; div\n\n    ?atom: NUMBER           -&gt; number\n         | \"-\" atom         -&gt; neg\n         | \"(\" sum \")\"\n\n    %import common.NUMBER\n    %import common.WS_INLINE\n\n    %ignore WS_INLINE\n\"\"\"\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = generate.cfg(model, arithmetic_grammar, max_tokens=100)\n\nresult = generator(\"Question: How can you write 5*5 using addition?\\nAnswer:\")\nprint(result)\n# 5+5+5+5+5\n</code></pre> <p>EBNF grammars can be cumbersome to write. This is why Outlines provides grammar definitions in the <code>outlines.grammars.</code> module</p> <pre><code>from outlines import models, generate, grammars\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = generate.cfg(model, grammars.arithmetic, max_tokens=100)\n\nresult = generator(\"Question: How can you write 5*5 using addition?\\nAnswer:\")\nprint(result)\n# 5+5+5+5+5\n</code></pre> <p>The available grammars are listed here.</p>"},{"location":"reference/generation/generation/#regex-structured-generation","title":"Regex-structured generation","text":"<p>Slightly simpler, but no less useful, Outlines can generate text that is in the language of a regular expression. For instance to force the model to generate IP addresses:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\n\nregex_str = r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\"\ngenerator = generate.regex(model, regex_str)\n\nresult = generator(\"What is the IP address of localhost?\\nIP: \")\nprint(result)\n# 127.0.0.100\n</code></pre>"},{"location":"reference/generation/generation/#generate-a-given-python-type","title":"Generate a given Python type","text":"<p>We provide a shortcut to regex-structured generation for simple use cases. Pass a Python type to the <code>outlines.generate.format</code> function and the LLM will output text that matches this type:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\ngenerator = generate.format(model, int)\n\nresult = generator(\"What is 2+2?\")\nprint(result)\n# 4\n</code></pre>"},{"location":"reference/generation/json/","title":"JSON structured generation","text":"<p>Outlines can make any open source model return a JSON object that follows a structure that is specified by the user. This is useful whenever we want the output of the model to be processed by code downstream: code does not understand natural language but rather the structured language it has been programmed to understand.</p> <p>There are mostly two reasons why someone would want to get an output formatted as JSON from a LLM:</p> <ol> <li>Parse the answer (e.g. with Pydantic), store it somewhere, return it to a user, etc.</li> <li>Call a function with the result</li> </ol> <p>Outlines has you covered in both cases! Indeed, to define the structure of the JSON you want the model to follow you can either provide a Pydantic model, or a function. No need to duplicate code!</p>"},{"location":"reference/generation/json/#using-pydantic","title":"Using Pydantic","text":"<p>Outlines can infer the structure of the output from a Pydantic model. The result is an instance of the model that contains the values returned by the LLM:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models, generate\n\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.json(model, User)\nresult = generator(\n    \"Create a user profile with the fields name, last_name and id\"\n)\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre> <p>JSON and whitespaces</p> <p>By default Outlines prevents the model from generating json with syntactic newlines, tabs, or multiple spaces. The default <code>whitespace_pattern</code> is <code>r\"[ ]?\"</code>. Small models tend to enter an infinite repetition loop if the <code>whitespace_pattern</code> allows infinite spacing. If you would like to allow the model to generate multiple tabs, newlines, and spaces, you can set the whitespace pattern as follows:</p> <pre><code>generator = generate.json(model, User, whitespace_pattern=r\"[\\n\\t ]*\")\n</code></pre> <p>Performance</p> <p><code>generation.json</code> computes an index that helps Outlines guide generation. This can take some time, but only needs to be done once. If you want to generate several times with the same schema make sure that you only call <code>generate.json</code> once.</p> <p>Custom types</p> <p>Outlines provides custom Pydantic types so you do not have to write regular expressions for common types, such as phone numbers or zip codes.</p>"},{"location":"reference/generation/json/#using-a-json-schema","title":"Using a JSON Schema","text":"<p>Instead of a Pydantic model you can pass a string that represents a JSON Schema specification to <code>generate.json</code>:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models\nfrom outlines import generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\nschema = \"\"\"\n{\n  \"title\": \"User\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": {\"type\": \"string\"},\n    \"last_name\": {\"type\": \"string\"},\n    \"id\": {\"type\": \"integer\"}\n  },\n  \"required\": [\"name\", \"last_name\", \"id\"]\n}\n\"\"\"\n\ngenerator = generate.json(model, schema)\nresult = generator(\n    \"Create a user profile with the fields name, last_name and id\"\n)\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre>"},{"location":"reference/generation/json/#from-a-functions-signature","title":"From a function's signature","text":"<p>Outlines can infer the structure of the output from the signature of a function. The result is a dictionary, and can be passed directly to the function using the usual dictionary expansion syntax <code>**</code>:</p> <pre><code>from outlines import models\nfrom outlines import generate\n\ndef add(a: int, b: int):\n    return a + b\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.json(model, add)\nresult = generator(\"Return two integers named a and b respectively. a is odd and b even.\")\n\nprint(add(**result))\n# 3\n</code></pre> <p>A great advantage of passing functions directly to specify the structure is that the structure of the LLM will change with the function's definition. No need to change the code at several places!</p>"},{"location":"reference/generation/regex/","title":"Regular expressions","text":"<p>Outlines can guarantee that the text generated by the LLM will be valid to a regular expression:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\n\ngenerator = generate.regex(\n    model,\n    r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\",\n)\n\nprompt = \"What is the IP address of the Google DNS servers? \"\nanswer = generator(prompt, max_tokens=30)\n\nprint(answer)\n# What is the IP address of the Google DNS servers?\n# 2.2.6.1\n</code></pre> <p>If you find yourself using <code>generate.regex</code> to restrict the answers' type you can take a look at type-structured generation instead.</p> <p>Performance</p> <p><code>generate.regex</code> computes an index that helps Outlines guide generation. This can take some time, but only needs to be done once. If you want to generate several times using the same regular expression make sure that you only call <code>generate.regex</code> once.</p>"},{"location":"reference/generation/structured_generation_explanation/","title":"How does Outlines work?","text":"<p>Language models generate text token by token, using the previous token sequence as input and sampled logits as output. This document explains the structured generation process, where only legal tokens are considered for the next step based on a predefined automata, e.g. a regex-defined finite-state machine (FSM) or Lark grammar.`</p>"},{"location":"reference/generation/structured_generation_explanation/#worked-example","title":"Worked Example","text":"<p>Let's consider a worked example with a pattern for whole and decimal numbers:</p> <p><code>^\\d*(\\.\\d+)?$</code>.</p>"},{"location":"reference/generation/structured_generation_explanation/#creating-automata","title":"Creating Automata","text":"<p>The pattern is first converted into an automata. Below is a brief explanation of the automata conversion and its representation.</p> <p>Automata Diagram:</p> <pre><code>graph LR\n    node0(\"1-9\") --&gt; node1(\"1-9\")\n    node1 --&gt; node1\n    node1 --&gt; nodeEND{{END}}\n    node1 --&gt; nodePeriod(\".\")\n    nodePeriod --&gt; node2(\"1-9\")\n    node2 --&gt; node2\n    node2 --&gt; nodeEND{{END}}</code></pre>"},{"location":"reference/generation/structured_generation_explanation/#generating-a-token","title":"Generating a Token","text":"<p>Let's assume that we're in the middle of generation, and so far \"748\" has been generated. Here is the automata with the current state highlighted in green, with the legal next characters being another number (1-9), a dot (.), or end of sequence.</p> <pre><code>graph LR\n    node0(\"1-9\") --&gt; node1(\"1-9\")\n    node1 --&gt; node1\n    node1 --&gt; nodeEND{{END}}\n    node1 --&gt; nodePeriod(\".\")\n    nodePeriod --&gt; node2(\"1-9\")\n    node2 --&gt; node2\n    node2 --&gt; nodeEND{{END}}\n\n    style node1 fill:#090</code></pre> <p>Generating a token requires the following steps:</p> <ul> <li>Feed the previous input sequence (\"748\") into the language model.</li> <li>Language model runs a forward pass and produces token logits.</li> <li>Outlines logits processor sets the probability of illegal tokens to 0%.</li> <li>A token is sampled from the set of legal tokens.</li> </ul> <p></p>"},{"location":"reference/generation/types/","title":"Custom types","text":"<p>Outlines provides custom Pydantic types so you can focus on your use case rather than on writing regular expressions:</p> Category Type Import Description ISBN 10 &amp; 13 <code>outlines.types.ISBN</code> There is no guarantee that the check digit will be correct Airport IATA <code>outlines.types.airports.IATA</code> Valid airport IATA codes Country alpha-2 code <code>outlines.types.airports.Alpha2</code> Valid country alpha-2 codes alpha-3 code <code>outlines.types.countries.Alpha3</code> Valid country alpha-3 codes numeric code <code>outlines.types.countries.Numeric</code> Valid country numeric codes name <code>outlines.types.countries.Name</code> Valid country names flag <code>outlines.types.countries.Flag</code> Valid flag emojis email <code>outlines.types.Email</code> Valid email address <p>Some types require localization. We currently only support US types, but please don't hesitate to create localized versions of the different types and open a Pull Request. Localized types are specified using <code>types.locale</code> in the following way:</p> <pre><code>from outlines import types\n\ntypes.locale(\"us\").ZipCode\ntypes.locale(\"us\").PhoneNumber\n</code></pre> <p>Here are the localized types that are currently available:</p> Category Locale Import Description Zip code US <code>ZipCode</code> Generate US Zip(+4) codes Phone number US <code>PhoneNumber</code> Generate valid US phone numbers <p>You can use these types in Pydantic schemas for JSON-structured generation:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models, generate, types\n\n# Specify the locale for types\nlocale = types.locale(\"us\")\n\nclass Client(BaseModel):\n    name: str\n    phone_number: locale.PhoneNumber\n    zip_code: locale.ZipCode\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.json(model, Client)\nresult = generator(\n    \"Create a client profile with the fields name, phone_number and zip_code\"\n)\nprint(result)\n# name='Tommy' phone_number='129-896-5501' zip_code='50766'\n</code></pre> <p>Or simply with <code>outlines.generate.format</code>:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models, generate, types\n\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.format(model, types.locale(\"us\").PhoneNumber)\nresult = generator(\n    \"Return a US Phone number: \"\n)\nprint(result)\n# 334-253-2630\n</code></pre> <p>We plan on adding many more custom types. If you have found yourself writing regular expressions to generate fields of a given type, or if you could benefit from more specific types don't hesite to submit a PR or open an issue.</p>"},{"location":"reference/models/exllamav2/","title":"ExllamaV2","text":"<pre><code>pip install exllamav2 transformers torch\n</code></pre> <p>Coming soon</p>"},{"location":"reference/models/llamacpp/","title":"Llama.cpp","text":"<p>Outlines provides an integration with Llama.cpp using the llama-cpp-python library. Llamacpp allows to run quantized models on machines with limited compute.</p> <p>Installation</p> <p>You need to install the <code>llama-cpp-python</code> library to use the llama.cpp integration. See the installation section for instructions to install <code>llama-cpp-python</code> with CUDA, Metal, ROCm and other backends.</p>"},{"location":"reference/models/llamacpp/#load-the-model","title":"Load the model","text":"<p>You can initialize the model by passing the name of the repository on the HuggingFace Hub, and the filenames (or glob pattern):</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\n</code></pre> <p>This will download the model files to the hub cache folder and load the weights in memory.</p> <p>You can also initialize the model by passing the path to the weights on your machine. Assuming Phi2's weights are in the current directory:</p> <pre><code>from outlines import models\nfrom llama_cpp import Llama\n\nllm = Llama(\"./phi-2.Q4_K_M.gguf\")\nmodel = models.LlamaCpp(llm)\n</code></pre> <p>If you need more control, you can pass the same keyword arguments to the model as you would pass in the llama-ccp-library:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\n    \"TheBloke/phi-2-GGUF\",\n    \"phi-2.Q4_K_M.gguf\"\n    n_ctx=512,  # to set the context length value\n)\n</code></pre> <p>Main parameters:</p> Parameters Type Description Default <code>n_gpu_layers</code> <code>int</code> Number of layers to offload to GPU. If -1, all layers are offloaded <code>0</code> <code>split_mode</code> <code>int</code> How to split the model across GPUs. <code>1</code> for layer-wise split, <code>2</code> for row-wise split <code>1</code> <code>main_gpu</code> <code>int</code> Main GPU <code>0</code> <code>tensor_split</code> <code>Optional[List[float]]</code> How split tensors should be distributed accross GPUs. If <code>None</code> the model is not split. <code>None</code> <code>n_ctx</code> <code>int</code> Text context. Inference from the model if set to <code>0</code> <code>0</code> <code>n_threads</code> <code>Optional[int]</code> Number of threads to use for generation. All available threads if set to <code>None</code>. <code>None</code> <code>verbose</code> <code>bool</code> Print verbose outputs to <code>stderr</code> <code>False</code> <p>See the llama-cpp-python documentation for the full list of parameters.</p>"},{"location":"reference/models/llamacpp/#load-the-model-on-gpu","title":"Load the model on GPU","text":"<p>Note</p> <p>Make sure that you installed <code>llama-cpp-python</code> with GPU support.</p> <p>To load the model on GPU, pass <code>n_gpu_layers=-1</code>:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\n    \"TheBloke/phi-2-GGUF\",\n    \"phi-2.Q4_K_M.gguf\"\n    n_gpu_layers=-1,  # to use GPU acceleration\n)\n</code></pre> <p>This also works with generators built with <code>generate.regex</code>, <code>generate.json</code>, <code>generate.cfg</code>, <code>generate.format</code> and <code>generate.choice</code>.</p>"},{"location":"reference/models/llamacpp/#load-lora-adapters","title":"Load LoRA adapters","text":"<p>You can load LoRA adapters dynamically:</p> <pre><code>from outlines import models, generate\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\ngenerator = generate.text(model)\nanswer_1 = generator(\"prompt\")\n\nmodel.load_lora(\"./path/to/adapter.gguf\")\nanswer_2 = generator(\"prompt\")\n</code></pre> <p>To load another adapter you need to re-initialize the model. Otherwise the adapter will be added on top of the previous one:</p> <pre><code>from outlines import models\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\nmodel.load_lora(\"./path/to/adapter1.gguf\")  # Load first adapter\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\nmodel.load_lora(\"./path/to/adapter2.gguf\")  # Load second adapter\n</code></pre>"},{"location":"reference/models/llamacpp/#generate-text","title":"Generate text","text":"<p>In addition to the parameters described in the text generation section you can pass extra keyword arguments, for instance to set sampling parameters not exposed in Outlines' public API:</p> <pre><code>from outlines import models, generate\n\n\nmodel = models.llamacpp(\"TheBloke/phi-2-GGUF\", \"phi-2.Q4_K_M.gguf\")\ngenerator = generate.text(model)\n\nanswer = generator(\"A prompt\", presence_penalty=0.8)\n</code></pre> <p>Extra keyword arguments:</p> <p>The value of the keyword arguments you pass to the generator suspersede the values set when initializing the sampler or generator. All extra sampling methods and repetition penalties are disabled by default.</p> Parameters Type Description Default <code>suffix</code> <code>Optional[str]</code> A suffix to append to the generated text. If <code>None</code> no suffix is added. <code>None</code> <code>echo</code> <code>bool</code> Whether to preprend the prompt to the completion. <code>False</code> <code>seed</code> <code>int</code> The random seed to use for sampling. <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> The maximum number of tokens to generate. If <code>None</code> the maximum number of tokens depends on <code>n_ctx</code>. <code>16</code> <code>frequence_penalty</code> <code>float</code> The penalty to apply to tokens based on their frequency in the past 64 tokens. <code>0.0</code> <code>presence_penalty</code> <code>float</code> The penalty to apply to tokens based on their presence in the past 64 tokens. <code>0.0</code> <code>repeat_penalty</code> <code>float</code> The penalty to apply to repeated tokens in the past 64 tokens. <code>1.</code> <code>stopping_criteria</code> <code>Optional[StoppingCriteriaList]</code> A list of stopping criteria to use. <code>None</code> <code>logits_processor</code> <code>Optional[LogitsProcessorList]</code> A list of logits processors to use. The logits processor used for structured generation will be added to this list. <code>None</code> <code>temperature</code> <code>float</code> The temperature to use for sampling <code>1.0</code> <code>top_p</code> <code>float</code> The top-p value to use for nucleus sampling. <code>1.</code> <code>min_p</code> <code>float</code> The min-p value to use for minimum-p sampling. <code>0.</code> <code>typical_p</code> <code>float</code> The p value to use for locally typical sampling. <code>1.0</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> A list of strings that stop generation when encountered. <code>[]</code> <code>top_k</code> <code>int</code> The top-k value used for top-k sampling. Negative value to consider all logit values. <code>-1.</code> <code>tfs_z</code> <code>float</code> The tail-free sampling parameter. <code>1.0</code> <code>mirostat_mode</code> <code>int</code> The mirostat sampling mode. <code>0</code> <code>mirostat_tau</code> <code>float</code> The target cross-entropy for mirostat sampling. <code>5.0</code> <code>mirostat_eta</code> <code>float</code> The learning rate used to update <code>mu</code> in mirostat sampling. <code>0.1</code> <p>See the llama-cpp-python documentation for the full and up-to-date list of parameters and the llama.cpp code for the default values of other sampling parameters.</p>"},{"location":"reference/models/llamacpp/#streaming","title":"Streaming","text":""},{"location":"reference/models/llamacpp/#installation","title":"Installation","text":"<p>You need to install the <code>llama-cpp-python</code> library to use the llama.cpp integration.</p>"},{"location":"reference/models/llamacpp/#cpu","title":"CPU","text":"<p>For a CPU-only installation run:</p> <pre><code>pip install llama-cpp-python\n</code></pre> <p>Warning</p> <p>Do not run this command if you want support for BLAS, Metal or CUDA. Follow the instructions below instead.</p>"},{"location":"reference/models/llamacpp/#cuda","title":"CUDA","text":"<pre><code>CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n</code></pre> <p>It is also possible to install pre-built wheels with CUDA support (Python 3.10 and above):</p> <pre><code>pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/&lt;cuda-version&gt;\n</code></pre> <p>Where <code>&lt;cuda-version&gt;</code> is one of the following, depending on the version of CUDA installed on your system:</p> <ul> <li><code>cu121</code> for CUDA 12.1</li> <li><code>cu122</code> for CUDA 12.2</li> <li><code>cu123</code> CUDA 12.3</li> </ul>"},{"location":"reference/models/llamacpp/#metal","title":"Metal","text":"<pre><code>CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n</code></pre> <p>It is also possible to install pre-build wheels with Metal support (Python 3.10 or above, MacOS 11.0 and above):</p> <pre><code>pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal\n</code></pre>"},{"location":"reference/models/llamacpp/#openblas","title":"OpenBLAS","text":"<pre><code>CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n</code></pre>"},{"location":"reference/models/llamacpp/#other-backend","title":"Other backend","text":"<p><code>llama.cpp</code> supports many other backends. Refer to the llama.cpp documentation to use the following backends:</p> <ul> <li>CLBast (OpenCL)</li> <li>hipBLAS (ROCm)</li> <li>Vulkan</li> <li>Kompute</li> <li>SYCL</li> </ul>"},{"location":"reference/models/mlxlm/","title":"mlx-lm","text":"<p>Outlines provides an integration with mlx-lm, allowing models to be run quickly on Apple Silicon via the mlx library.</p> <p>Installation</p> <p>You need to install the <code>mlx</code> and <code>mlx-lm</code> libraries on a device which supports Metal to use the mlx-lm integration.</p>"},{"location":"reference/models/mlxlm/#load-the-model","title":"Load the model","text":"<p>You can initialize the model by passing the name of the repository on the HuggingFace Hub. The official repository for mlx-lm supported models is mlx-community.</p> <pre><code>from outlines import models\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\n</code></pre> <p>This will download the model files to the hub cache folder and load the weights in memory.</p> <p>The arguments <code>model_config</code> and <code>tokenizer_config</code> are available to modify loading behavior. For example, per the <code>mlx-lm</code> documentation, you must set an eos_token for <code>qwen/Qwen-7B</code>. In outlines you may do so via</p> <pre><code>model = models.mlxlm(\n    \"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\",\n    tokenizer_config={\"eos_token\": \"&lt;|endoftext|&gt;\", \"trust_remote_code\": True},\n)\n</code></pre> <p>Main parameters:</p> <p>(Subject to change. Table based on mlx-lm.load docstring)</p> Parameters Type Description Default <code>tokenizer_config</code> <code>dict</code> Configuration parameters specifically for the tokenizer. Defaults to an empty dictionary. <code>{}</code> <code>model_config</code> <code>dict</code> Configuration parameters specifically for the model. Defaults to an empty dictionary. <code>{}</code> <code>adapter_path</code> <code>str</code> Path to the LoRA adapters. If provided, applies LoRA layers to the model. <code>None</code> <code>lazy</code> <code>bool</code> If False, evaluate the model parameters to make sure they are loaded in memory before returning. <code>False</code>"},{"location":"reference/models/mlxlm/#generate-text","title":"Generate text","text":"<p>You may generate text using the parameters described in the text generation documentation.</p> <p>With the loaded model, you can generate text or perform structured generation, e.g.</p> <pre><code>from outlines import models, generate\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\ngenerator = generate.text(model)\n\nanswer = generator(\"A prompt\", temperature=2.0)\n</code></pre>"},{"location":"reference/models/mlxlm/#streaming","title":"Streaming","text":"<p>You may creating a streaming iterable with minimal changes</p> <pre><code>from outlines import models, generate\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\ngenerator = generate.text(model)\n\nfor token_str in generator.text(\"A prompt\", temperature=2.0):\n    print(token_str)\n</code></pre>"},{"location":"reference/models/mlxlm/#structured","title":"Structured","text":"<p>You may perform structured generation with mlxlm to guarantee your output will match a regex pattern, json schema, or lark grammar.</p> <p>Example: Phone number generation with pattern <code>\"\\\\+?[1-9][0-9]{7,14}\"</code>:</p> <pre><code>from outlines import models, generate\n\nmodel = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\n\nphone_number_pattern = \"\\\\+?[1-9][0-9]{7,14}\"\ngenerator = generate.regex(model, phone_number_pattern)\n\nmodel_output = generator(\"What's Jennys Number?\\n\")\nprint(model_output)\n# '8675309'\n</code></pre>"},{"location":"reference/models/models/","title":"Models","text":"<p>Outlines supports generation using a number of inference engines (<code>outlines.models</code>). Loading a model using outlines follows a similar interface between inference engines:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-128k-instruct\")\nmodel = outlines.models.transformers_vision(\"llava-hf/llava-v1.6-mistral-7b-hf\")\nmodel = outlines.models.vllm(\"microsoft/Phi-3-mini-128k-instruct\")\nmodel = outlines.models.llamacpp(\n    \"microsoft/Phi-3-mini-4k-instruct-gguf\", \"Phi-3-mini-4k-instruct-q4.gguf\"\n)\nmodel = outlines.models.exllamav2(\"bartowski/Phi-3-mini-128k-instruct-exl2\")\nmodel = outlines.models.mlxlm(\"mlx-community/Phi-3-mini-4k-instruct-4bit\")\n\nmodel = outlines.models.openai(\n    \"gpt-4o-mini\",\n    api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n</code></pre>"},{"location":"reference/models/models/#feature-matrix","title":"Feature Matrix","text":"Transformers Transformers Vision vLLM llama.cpp ExLlamaV2 MLXLM OpenAI* Device Cuda \u2705 \u2705 \u2705 \u2705 \u2705 \u274c N/A Apple Silicon \u2705 \u2705 \u274c \u2705 \u2705 \u2705 N/A x86 / AMD64 \u2705 \u2705 \u274c \u2705 \u2705 \u274c N/A Sampling Greedy \u2705 \u2705 \u2705 \u2705* \u2705 \u2705 \u274c Multinomial \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Multiple Samples \u2705 \u2705 \u274c \u274c \u2705 Beam Search \u2705 \u2705 \u2705 \u274c \u2705 \u274c \u274c Generation Batch \u2705 \u2705 \u2705 \u274c ? \u274c \u274c Stream \u2705 \u274c \u274c \u2705 ? \u2705 \u274c <code>outlines.generate</code> Text \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Structured \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 JSON Schema \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Choice \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Regex \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c Grammar \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c"},{"location":"reference/models/models/#caveats","title":"Caveats","text":"<ul> <li>OpenAI doesn't support structured generation due to limitations in their API and server implementation.</li> <li><code>outlines.generate</code> \"Structured\" includes methods such as <code>outlines.generate.regex</code>, <code>outlines.generate.json</code>, <code>outlines.generate.cfg</code>, etc.</li> <li>MLXLM only supports Apple Silicon.</li> <li>llama.cpp greedy sampling available via multinomial with <code>temperature = 0.0</code>.</li> </ul>"},{"location":"reference/models/openai/","title":"OpenAI and compatible APIs","text":"<p>Installation</p> <p>You need to install the <code>openai</code> library to be able to use the OpenAI API in Outlines.</p>"},{"location":"reference/models/openai/#openai-models","title":"OpenAI models","text":"<p>Outlines supports models available via the OpenAI Chat API, e.g. GPT-4o, ChatGPT and GPT-4. You can initialize the model by passing the model name to <code>outlines.models.openai</code>:</p> <pre><code>from outlines import models\n\n\nmodel = models.openai(\"gpt-4o-mini\")\nmodel = models.openai(\"gpt-4o\")\n</code></pre> <p>Check the OpenAI documentation for an up-to-date list of available models. You can pass any parameter you would pass to <code>openai.AsyncOpenAI</code> as keyword arguments:</p> <pre><code>import os\nfrom outlines import models\n\n\nmodel = models.openai(\n    \"gpt-4o-mini\",\n    api_key=os.environ[\"OPENAI_API_KEY\"]\n)\n</code></pre> <p>The following table enumerates the possible parameters. Refer to the OpenAI SDK's code for an up-to-date list.</p> <p>Parameters:</p> Parameters Type Description Default <code>api_key</code> <code>str</code> OpenAI API key. Infered from <code>OPENAI_API_KEY</code> if not specified <code>None</code> <code>organization</code> <code>str</code> OpenAI organization id. Infered from <code>OPENAI_ORG_ID</code> if not specified <code>None</code> <code>project</code> <code>str</code> OpenAI project id. Infered from <code>OPENAI_PROJECT_ID</code> if not specified. <code>None</code> <code>base_url</code> <code>str | https.URL</code> Base URL for the endpoint. Infered from <code>OPENAI_BASE_URL</code> if no specified. <code>None</code> <code>timeout</code> <code>float</code> Request timeout. <code>NOT_GIVEN</code> <code>max_retries</code> <code>int</code> Maximum number of retries for failing requests <code>2</code> <code>default_headers</code> <code>Mapping[str, str]</code> Default HTTP headers <code>None</code> <code>default_query</code> <code>Mapping[str, str]</code> Custom parameters added to the HTTP queries <code>None</code> <code>http_client</code> <code>https.AsyncClient</code> User-specified <code>httpx</code> client <code>None</code>"},{"location":"reference/models/openai/#azure-openai-models","title":"Azure OpenAI models","text":"<p>Outlines also supports Azure OpenAI models:</p> <pre><code>from outlines import models\n\n\nmodel = models.azure_openai(\n    \"azure-deployment-name\",\n    \"gpt-4o-mini\",\n    api_version=\"2024-07-18\",\n    azure_endpoint=\"https://example-endpoint.openai.azure.com\",\n)\n</code></pre> <p>Why do I need to specify model and deployment name?</p> <p>The model name is needed to load the correct tokenizer for the model. The tokenizer is necessary for structured generation.</p> <p>You can pass any parameter you would pass to <code>openai.AsyncAzureOpenAI</code>. You can consult the OpenAI SDK's code for an up-to-date list.</p> <p>Parameters:</p> Parameters Type Description Default <code>azure_endpoint</code> <code>str</code> Azure endpoint, including the resource. Infered from <code>AZURE_OPENAI_ENDPOINT</code> if not specified <code>None</code> <code>api_version</code> <code>str</code> API version. Infered from <code>AZURE_OPENAI_API_KEY</code> if not specified <code>None</code> <code>api_key</code> <code>str</code> OpenAI API key. Infered from <code>OPENAI_API_KEY</code> if not specified <code>None</code> <code>azure_ad_token</code> <code>str</code> Azure active directory token. Inference from <code>AZURE_OPENAI_AD_TOKEN</code> if not specified <code>None</code> <code>azure_ad_token_provider</code> <code>AzureADTokenProvider</code> A function that returns an Azure Active Directory token <code>None</code> <code>organization</code> <code>str</code> OpenAI organization id. Infered from <code>OPENAI_ORG_ID</code> if not specified <code>None</code> <code>project</code> <code>str</code> OpenAI project id. Infered from <code>OPENAI_PROJECT_ID</code> if not specified. <code>None</code> <code>base_url</code> <code>str | https.URL</code> Base URL for the endpoint. Infered from <code>OPENAI_BASE_URL</code> if not specified. <code>None</code> <code>timeout</code> <code>float</code> Request timeout. <code>NOT_GIVEN</code> <code>max_retries</code> <code>int</code> Maximum number of retries for failing requests <code>2</code> <code>default_headers</code> <code>Mapping[str, str]</code> Default HTTP headers <code>None</code> <code>default_query</code> <code>Mapping[str, str]</code> Custom parameters added to the HTTP queries <code>None</code> <code>http_client</code> <code>https.AsyncClient</code> User-specified <code>httpx</code> client <code>None</code>"},{"location":"reference/models/openai/#models-that-follow-the-openai-standard","title":"Models that follow the OpenAI standard","text":"<p>Outlines supports models that follow the OpenAI standard. You will need to initialize the OpenAI client properly configured and pass it to <code>outlines.models.openai</code></p> <pre><code>import os\nfrom openai import AsyncOpenAI\nfrom outlines import models\nfrom outlines.models.openai import OpenAIConfig\n\n\nclient = AsyncOpenAI(\n    api_key=os.environ.get(\"PROVIDER_KEY\"),\n    base_url=\"http://other.provider.server.com\"\n)\nconfig = OpenAIConfig(\"model_name\")\nmodel = models.openai(client, config)\n</code></pre> <p>Warning</p> <p>You need to pass the async client to be able to do batch inference.</p>"},{"location":"reference/models/openai/#structured-generation-support","title":"Structured Generation Support","text":"<p>Outlines provides support for OpenAI Structured Outputs via <code>outlines.generate.json</code>, <code>outlines.generate.choice</code></p> <pre><code>from pydantic import BaseModel, ConfigDict\nimport outlines.models as models\nfrom outlines import generate\n\nmodel = models.openai(\"gpt-4o-mini\")\n\nclass Person(BaseModel):\n    model_config = ConfigDict(extra='forbid')  # required for openai\n    first_name: str\n    last_name: str\n    age: int\n\ngenerate.json(model, Person)\ngenerator(\"current indian prime minister on january 1st 2023\")\n# Person(first_name='Narendra', last_name='Modi', age=72)\n\ngenerator = generate.choice(model, [\"Chicken\", \"Egg\"])\nprint(generator(\"Which came first?\"))\n# Chicken\n</code></pre> <p>Warning</p> <p>Structured generation support only provided to OpenAI-compatible endpoints which conform to OpenAI's standard. Additionally, <code>generate.regex</code> and <code>generate.cfg</code> are not supported.</p>"},{"location":"reference/models/openai/#advanced-configuration","title":"Advanced configuration","text":"<p>For more advanced configuration option, such as support proxy, please consult the OpenAI SDK's documentation:</p> <pre><code>from openai import AsyncOpenAI, DefaultHttpxClient\nfrom outlines import models\nfrom outlines.models.openai import OpenAIConfig\n\n\nclient = AsyncOpenAI(\n    base_url=\"http://my.test.server.example.com:8083\",\n    http_client=DefaultHttpxClient(\n        proxies=\"http://my.test.proxy.example.com\",\n        transport=httpx.HTTPTransport(local_address=\"0.0.0.0\"),\n    ),\n)\nconfig = OpenAIConfig(\"model_name\")\nmodel = models.openai(client, config)\n</code></pre> <p>It is possible to specify the values for <code>seed</code>, <code>presence_penalty</code>, <code>frequence_penalty</code>, <code>top_p</code> by passing an instance of <code>OpenAIConfig</code> when initializing the model:</p> <pre><code>from outlines.models.openai import OpenAIConfig\nfrom outlines import models\n\n\nconfig = OpenAIConfig(\n    presence_penalty=1.,\n    frequency_penalty=1.,\n    top_p=.95,\n    seed=0,\n)\nmodel = models.openai(\"gpt-4o-mini\", config)\n</code></pre>"},{"location":"reference/models/openai/#monitoring-api-use","title":"Monitoring API use","text":"<p>It is important to be able to track your API usage when working with OpenAI's API. The number of prompt tokens and completion tokens is directly accessible via the model instance:</p> <pre><code>from openai import AsyncOpenAI\nimport outlines.models\n\n\nmodel = models.openai(\"gpt-4o\")\n\nprint(model.prompt_tokens)\n# 0\n\nprint(model.completion_tokens)\n# 0\n</code></pre> <p>These numbers are updated every time you call the model.</p>"},{"location":"reference/models/tgi/","title":"Text-generation-inference (TGI)","text":"<p>TGI uses Outlines to provide structured generation, see their documentation.</p>"},{"location":"reference/models/transformers/","title":"transformers","text":"<p>Installation</p> <p>You need to install the <code>transformer</code>, <code>datasets</code> and <code>torch</code> libraries to be able to use these models in Outlines:</p> <pre><code>pip install torch transformers datasets\n</code></pre> <p>Outlines provides an integration with the <code>torch</code> implementation of causal models in the transformers library. You can initialize the model by passing its name:</p> <pre><code>from outlines import models\n\nmodel = models.transformers(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda\")\n</code></pre> <p>If you need more fine-grained control you can also initialize the model and tokenizer separately:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom outlines import models\n\nllm = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_attentions=True)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = models.Transformers(llm, tokenizer)\n</code></pre>"},{"location":"reference/models/transformers/#using-logits-processors","title":"Using Logits Processors","text":"<p>There are two ways to use Outlines Structured Generation with HuggingFace Transformers:</p> <ol> <li>Use Outlines generation wrapper, <code>outlines.models.transformers</code></li> <li>Use <code>OutlinesLogitsProcessor</code> with <code>transformers.AutoModelForCausalLM</code></li> </ol> <p>Outlines supports a myriad of logits processors for structured generation. In these example, we will use the <code>RegexLogitsProcessor</code> which guarantees generated text matches the specified pattern.</p>"},{"location":"reference/models/transformers/#using-outlinesmodelstransformers","title":"Using <code>outlines.models.transformers</code>","text":"<pre><code>import outlines\n\ntime_regex_pattern = r\"(0?[1-9]|1[0-2]):[0-5]\\d\\s?(am|pm)?\"\n\nmodel = outlines.models.transformers(\"microsoft/Phi-3-mini-4k-instruct\", device=\"cuda\")\ngenerator = outlines.generate.regex(model, time_regex_pattern)\n\noutput = generator(\"The the best time to visit a dentist is at \")\nprint(output)\n# 2:30 pm\n</code></pre>"},{"location":"reference/models/transformers/#using-models-initialized-via-the-transformers-library","title":"Using models initialized via the <code>transformers</code>  library","text":"<pre><code>import outlines\nimport transformers\n\n\nmodel_uri = \"microsoft/Phi-3-mini-4k-instruct\"\n\noutlines_tokenizer = outlines.models.TransformerTokenizer(\n    transformers.AutoTokenizer.from_pretrained(model_uri)\n)\nphone_number_logits_processor = outlines.processors.RegexLogitsProcessor(\n    \"\\\\+?[1-9][0-9]{7,14}\",  # phone number pattern\n    outlines_tokenizer,\n)\n\ngenerator = transformers.pipeline('text-generation', model=model_uri)\n\noutput = generator(\n    \"Jenny gave me her number it's \",\n    logits_processor=transformers.LogitsProcessorList([phone_number_logits_processor])\n)\nprint(output)\n# [{'generated_text': \"Jenny gave me her number it's 2125550182\"}]\n# not quite 8675309 what we expected, but it is a valid phone number\n</code></pre>"},{"location":"reference/models/transformers/#alternative-model-classes","title":"Alternative Model Classes","text":"<p><code>outlines.models.transformers</code> defaults to <code>transformers.AutoModelForCausalLM</code>, which is the appropriate class for most standard large language models, including Llama 3, Mistral, Phi-3, etc.</p> <p>However other variants with unique behavior can be used as well by passing the appropriate class.</p>"},{"location":"reference/models/transformers/#mamba","title":"Mamba","text":"<p>Mamba is a transformers alternative which employs memory efficient, linear-time decoding.</p> <p>To use Mamba with outlines you must first install the necessary requirements: <pre><code>pip install causal-conv1d&gt;=1.2.0 mamba-ssm torch transformers\n</code></pre></p> <p>Then you can either create an Mamba-2 Outlines model via <pre><code>import outlines\n\nmodel = outlines.models.mamba(\"state-spaces/mamba-2.8b-hf\")\n</code></pre></p> <p>or explicitly with <pre><code>import outlines\nfrom transformers import MambaForCausalLM\n\nmodel = outlines.models.transformers(\n    \"state-spaces/mamba-2.8b-hf\",\n    model_class=MambaForCausalLM\n)\n</code></pre></p> <p>Read <code>transformers</code>'s documentation for more information.</p>"},{"location":"reference/models/transformers/#encoder-decoder-models","title":"Encoder-Decoder Models","text":"<p>You can use encoder-decoder (seq2seq) models like T5 and BART with Outlines.</p> <p>Be cautious with model selection though, some models such as <code>t5-base</code> don't include certain characters (<code>{</code>) and you may get an error when trying to perform structured generation.</p> <p>T5 Example: <pre><code>import outlines\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel_pile_t5 = models.transformers(\n    model_name=\"EleutherAI/pile-t5-large\",\n    model_class=AutoModelForSeq2SeqLM,\n)\n</code></pre></p> <p>Bart Example: <pre><code>model_bart = models.transformers(\n    model_name=\"facebook/bart-large\",\n    model_class=AutoModelForSeq2SeqLM,\n)\n</code></pre></p>"},{"location":"reference/models/transformers_vision/","title":"Transformers Vision","text":"<p>Outlines allows seamless use of vision models.</p> <p><code>outlines.models.transformers_vision</code> has shares interfaces with, and is based on outlines.models.transformers.</p> <p>Tasks supported include</p> <ul> <li>image + text -&gt; text</li> <li>video + text -&gt; text</li> </ul>"},{"location":"reference/models/transformers_vision/#example-using-llava-next-vision-models","title":"Example: Using Llava-Next Vision Models","text":"<p>Install dependencies <code>pip install torchvision pillow flash-attn</code></p> <p>Create the model <pre><code>import outlines\nfrom transformers import LlavaNextForConditionalGeneration\n\nmodel = outlines.models.transformers_vision(\n    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n    model_class=LlavaNextForConditionalGeneration,\n    device=\"cuda\",\n)\n</code></pre></p> <p>Create convenience function to load a <code>PIL.Image</code> from URL <pre><code>from PIL import Image\nfrom io import BytesIO\nfrom urllib.request import urlopen\n\ndef img_from_url(url):\n    img_byte_stream = BytesIO(urlopen(url).read())\n    return Image.open(img_byte_stream).convert(\"RGB\")\n</code></pre></p>"},{"location":"reference/models/transformers_vision/#describing-an-image","title":"Describing an image","text":"<pre><code>description_generator = outlines.generate.text(model)\ndescription_generator(\n    \"&lt;image&gt; detailed description:\",\n    [img_from_url(\"https://upload.wikimedia.org/wikipedia/commons/2/25/Siam_lilacpoint.jpg\")]\n)\n</code></pre> <p>This is a color photograph featuring a Siamese cat with striking blue eyes. The cat has a creamy coat and a light eye color, which is typical for the Siamese breed. Its features include elongated ears, a long, thin tail, and a striking coat pattern. The cat is sitting in an indoor setting, possibly on a cat tower or a similar raised platform, which is covered with a beige fabric, providing a comfortable and soft surface for the cat to rest or perch. The surface of the wall behind the cat appears to be a light-colored stucco or plaster.</p>"},{"location":"reference/models/transformers_vision/#multiple-images","title":"Multiple Images","text":"<p>To include multiple images in your prompt you simply add more <code>&lt;image&gt;</code> tokens to the prompt</p> <pre><code>image_urls = [\n    \"https://cdn1.byjus.com/wp-content/uploads/2020/08/ShapeArtboard-1-copy-3.png\",  # triangle\n    \"https://cdn1.byjus.com/wp-content/uploads/2020/08/ShapeArtboard-1-copy-11.png\",  # hexagon\n]\ndescription_generator = outlines.generate.text(model)\ndescription_generator(\n    \"&lt;image&gt;&lt;image&gt;&lt;image&gt;What shapes are present?\",\n    list(map(img_from_url, image_urls)),\n)\n</code></pre> <p>There are two shapes present. One shape is a hexagon and the other shape is an triangle. '</p>"},{"location":"reference/models/transformers_vision/#classifying-an-image","title":"Classifying an Image","text":"<pre><code>pattern = \"Mercury|Venus|Earth|Mars|Saturn|Jupiter|Neptune|Uranus|Pluto\"\nplanet_generator = outlines.generate.regex(model, pattern)\n\nplanet_generator(\n    \"What planet is this: &lt;image&gt;\",\n    [img_from_url(\"https://upload.wikimedia.org/wikipedia/commons/e/e3/Saturn_from_Cassini_Orbiter_%282004-10-06%29.jpg\")]\n)\n</code></pre> <p>Saturn</p>"},{"location":"reference/models/transformers_vision/#extracting-structured-image-data","title":"Extracting Structured Image data","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\n\nclass ImageData(BaseModel):\n    caption: str\n    tags_list: List[str]\n    object_list: List[str]\n    is_photo: bool\n\nimage_data_generator = outlines.generate.json(model, ImageData)\n\nimage_data_generator(\n    \"&lt;image&gt; detailed JSON metadata:\",\n    [img_from_url(\"https://upload.wikimedia.org/wikipedia/commons/9/98/Aldrin_Apollo_11_original.jpg\")]\n)\n</code></pre> <p><code>ImageData(caption='An astronaut on the moon', tags_list=['moon', 'space', 'nasa', 'americanflag'], object_list=['moon', 'moon_surface', 'space_suit', 'americanflag'], is_photo=True)</code></p>"},{"location":"reference/models/transformers_vision/#resources","title":"Resources","text":""},{"location":"reference/models/transformers_vision/#chosing-a-model","title":"Chosing a model","text":"<ul> <li>https://mmbench.opencompass.org.cn/leaderboard</li> <li>https://huggingface.co/spaces/WildVision/vision-arena</li> </ul>"},{"location":"reference/models/vllm/","title":"vLLM","text":"<p>Installation</p> <p>You need to install the <code>vllm</code> library to use the vLLM integration. See the installation section for instructions to install vLLM for CPU or ROCm.</p>"},{"location":"reference/models/vllm/#load-the-model","title":"Load the model","text":"<p>Outlines supports models available via vLLM's offline batched inference interface. You can load a model using:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\"microsoft/Phi-3-mini-4k-instruct\")\n</code></pre> <p>Or alternatively:</p> <pre><code>import vllm\nfrom outlines import models\n\nllm = vllm.LLM(\"microsoft/Phi-3-mini-4k-instruct\")\nmodel = models.VLLM(llm)\n</code></pre> <p>Models are loaded from the HuggingFace hub.</p> <p>Device</p> <p>The default installation of vLLM only allows to load models on GPU. See the installation instructions to run models on CPU.</p> <p>You can pass any parameter that you would normally pass to <code>vllm.LLM</code>, as keyword arguments:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    trust_remote_code=True,\n    gpu_memory_utilization=0.7\n)\n</code></pre> <p>Main parameters:</p> Parameters Type Description Default <code>tokenizer_mode</code> <code>str</code> \"auto\" will use the fast tokenizer if available and \"slow\" will always use the slow tokenizer. <code>auto</code> <code>trust_remote_code</code> <code>bool</code> Trust remote code when downloading the model and tokenizer. <code>False</code> <code>tensor_parallel_size</code> <code>int</code> The number of GPUs to use for distributed execution with tensor parallelism. <code>1</code> <code>dtype</code> <code>str</code> The data type for the model weights and activations. Currently, we support <code>float32</code>, <code>float16</code>, and <code>bfloat16</code>. If <code>auto</code>, we use the <code>torch_dtype</code> attribute specified in the model config file. However, if the <code>torch_dtype</code> in the config is <code>float32</code>, we will use <code>float16</code> instead. <code>auto</code> <code>quantization</code> <code>Optional[str]</code> The method used to quantize the model weights. Currently, we support \"awq\", \"gptq\" and \"squeezellm\". If None, we first check the <code>quantization_config</code> attribute in the model config file. If that is None, we assume the model weights are not quantized and use <code>dtype</code> to determine the data type of the weights. <code>None</code> <code>revision</code> <code>Optional[str]</code> The specific model version to use. It can be a branch name, a tag name, or a commit id. <code>None</code> <code>tokenizer_revision</code> <code>Optional[str]</code> The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. <code>None</code> <code>gpu_memory_utilization</code> <code>float</code> The ratio (between 0 and 1) of GPU memory to reserve for the model weights, activations, and KV cache. Higher values will increase the KV cache size and thus improve the model's throughput. However, if the value is too high, it may cause out-of-memory (OOM) errors. <code>0.9</code> <code>swap_space</code> <code>int</code> The size (GiB) of CPU memory per GPU to use as swap space. This can be used for temporarily storing the states of the requests when their <code>best_of</code> sampling parameters are larger than 1. If all requests will have <code>best_of=1</code>, you can safely set this to 0. Otherwise, too small values may cause out-of-memory (OOM) errors. 4 <code>enforce_eager</code> <code>bool</code> Whether to enforce eager execution. If True, we will disable CUDA graph and always execute the model in eager mode. If False, we will use CUDA graph and eager execution in hybrid. <code>False</code> <code>enable_lora</code> <code>bool</code> Whether to enable loading LoRA adapters <code>False</code> <p>See the vLLM code for a list of all the available parameters.</p>"},{"location":"reference/models/vllm/#use-quantized-models","title":"Use quantized models","text":"<p>vLLM supports AWQ, GPTQ and SqueezeLLM quantized models:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\"TheBloke/Llama-2-7B-Chat-AWQ\", quantization=\"awq\")\nmodel = models.vllm(\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\", quantization=\"gptq\")\nmodel = models.vllm(\"https://huggingface.co/squeeze-ai-lab/sq-llama-30b-w4-s5\", quantization=\"squeezellm\")\n</code></pre> <p>Dependencies</p> <p>To use AWQ model you need to install the autoawq library <code>pip install autoawq</code>.</p> <p>To use GPTQ models you need to install the autoGTPQ and optimum libraries <code>pip install auto-gptq optimum</code>.</p>"},{"location":"reference/models/vllm/#multi-gpu-usage","title":"Multi-GPU usage","text":"<p>To run multi-GPU inference with vLLM you need to set the <code>tensor_parallel_size</code> argument to the number of GPUs available when initializing the model. For instance to run inference on 2 GPUs:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\n    \"microsoft/Phi-3-mini-4k-instruct\"\n    tensor_parallel_size=2\n)\n</code></pre>"},{"location":"reference/models/vllm/#load-lora-adapters","title":"Load LoRA adapters","text":"<p>You can load LoRA adapters and alternate between them dynamically:</p> <pre><code>from outlines import models\n\nmodel = models.vllm(\"facebook/opt-350m\", enable_lora=True)\nmodel.load_lora(\"ybelkaa/opt-350m-lora\")  # Load LoRA adapter\nmodel.load_lora(None)  # Unload LoRA adapter\n</code></pre>"},{"location":"reference/models/vllm/#generate-text","title":"Generate text","text":"<p>In addition to the parameters described in the text generation section you can pass an instance of <code>SamplingParams</code> directly to any generator via the <code>sampling_params</code> keyword argument:</p> <pre><code>from vllm.sampling_params import SamplingParams\nfrom outlines import models, generate\n\n\nmodel = models.vllm(\"microsoft/Phi-3-mini-4k-instruct\")\ngenerator = generate.text(model)\n\nparams = SamplingParams(n=2, frequency_penalty=1., min_tokens=2)\nanswer = generator(\"A prompt\", sampling_params=params)\n</code></pre> <p>This also works with generators built with <code>generate.regex</code>, <code>generate.json</code>, <code>generate.cfg</code>, <code>generate.format</code> and <code>generate.choice</code>.</p> <p>Note</p> <p>The values passed via the <code>SamplingParams</code> instance supersede the other arguments to the generator or the samplers.</p> <p><code>SamplingParams</code> attributes:</p> Parameters Type Description Default <code>n</code> <code>int</code> Number of output sequences to return for the given prompt. <code>1</code> <code>best_of</code> <code>Optional[int]</code> Number of output sequences that are generated from the prompt. From these <code>best_of</code> sequences, the top <code>n</code> sequences are returned. <code>best_of</code> must be greater than or equal to <code>n</code>. This is treated as the beam width when <code>use_beam_search</code> is True. By default, <code>best_of</code> is set to <code>n</code>. <code>None</code> <code>presence_penalty</code> <code>float</code> Float that penalizes new tokens based on whether they appear in the generated text so far. Values &gt; 0 encourage the model to use new tokens, while values &lt; 0 encourage the model to repeat tokens. <code>0.0</code> <code>frequency_penalty</code> <code>float</code> Float that penalizes new tokens based on their frequency in the generated text so far. Values &gt; 0 encourage the model to use new tokens, while values &lt; 0 encourage the model to repeat tokens. <code>0.0</code> <code>repetition_penalty</code> <code>float</code> Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values &gt; 1 encourage the model to use new tokens, while values &lt; 1 encourage the model to repeat tokens. <code>1.0</code> <code>temperature</code> <code>float</code> Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling. <code>1.0</code> <code>top_p</code> <code>float</code> Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens. <code>1.0</code> <code>top_k</code> <code>int</code> Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens. <code>-1</code> <code>min_p</code> <code>float</code> Float that represents the minimum probability for a token to be considered, relative to the probability of the most likely token. Must be in [0, 1]. Set to 0 to disable this. <code>0.0</code> <code>seed</code> <code>Optional[int]</code> Random seed to use for the generation. <code>None</code> <code>use_beam_search</code> <code>bool</code> Whether to use beam search instead of sampling. <code>False</code> <code>length_penalty</code> <code>float</code> Float that penalizes sequences based on their length. Used in beam search. <code>1.0</code> <code>early_stopping</code> <code>Union[bool, str]</code> Controls the stopping condition for beam search. It accepts the following values: <code>True</code>, where the generation stops as soon as there are <code>best_of</code> complete candidates; <code>False</code>, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates; <code>\"never\"</code>, where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm). <code>False</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> List of strings that stop the generation when they are generated. The returned output will not contain the stop strings. <code>None</code> <code>stop_token_ids</code> <code>Optional[List[int]]</code> List of tokens that stop the generation when they are generated. The returned output will contain the stop tokens unless the stop tokens are special tokens. <code>None</code> <code>include_stop_str_in_output</code> <code>bool</code> Whether to include the stop strings in output text. Defaults to False. <code>False</code> <code>ignore_eos</code> <code>bool</code> Whether to ignore the EOS token and continue generating tokens after the EOS token is generated. <code>False</code> <code>max_tokens</code> <code>int</code> Maximum number of tokens to generate per output sequence. <code>16</code> <code>min_tokens</code> <code>int</code> Minimum number of tokens to generate per output sequence before EOS or stop_token_ids can be generated <code>0</code> <code>skip_special_tokens</code> <code>bool</code> Whether to skip special tokens in the output. <code>True</code> <code>spaces_between_special_tokens</code> <code>bool</code> Whether to add spaces between special tokens in the output.  Defaults to True. <code>True</code>"},{"location":"reference/models/vllm/#streaming","title":"Streaming","text":"<p>Warning</p> <p>Streaming is not available for the offline vLLM integration.</p>"},{"location":"reference/models/vllm/#installation","title":"Installation","text":"<p>By default the vLLM library is installed with pre-commpiled C++ and CUDA binaries and will only run on GPU:</p> <pre><code>pip install vllm\n</code></pre>"},{"location":"reference/models/vllm/#cpu","title":"CPU","text":"<p>You need to have the <code>gcc</code> compiler installed on your system. Then you will need to install vLLM from source. First clone the repository:</p> <pre><code>git clone https://github.com/vllm-project/vllm.git\ncd vllm\n</code></pre> <p>Install the Python packages needed for the installation:</p> <pre><code>pip install --upgrade pip\npip install wheel packaging ninja setuptools&gt;=49.4.0 numpy\npip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n</code></pre> <p>and finally run:</p> <pre><code>VLLM_TARGET_DEVICE=cpu python setup.py install\n</code></pre> <p>See the vLLM documentation for more details, alternative installation methods (Docker) and performance tips.</p>"},{"location":"reference/models/vllm/#rocm","title":"ROCm","text":"<p>You will need to install vLLM from source. First install Pytorch on ROCm:</p> <pre><code>pip install torch==2.2.0.dev20231206+rocm5.7 --index-url https://download.pytorch.org/whl/nightly/rocm5.7 # tested version\n</code></pre> <p>You will then need to install flash attention for ROCm following these instructions. You can then install <code>xformers=0.0.23</code> and apply the patches needed to adapt Flash Attention for ROCm:</p> <pre><code>pip install xformers==0.0.23 --no-deps\nbash patch_xformers.rocm.sh\n</code></pre> <p>And finally build vLLM:</p> <pre><code>cd vllm\npip install -U -r requirements-rocm.txt\npython setup.py install # This may take 5-10 minutes.\n</code></pre> <p>See the vLLM documentation for alternative installation methods (Docker).</p>"},{"location":"reference/serve/vllm/","title":"Serve with vLLM","text":"<p>Would rather not self-host?</p> <p>If you want to get started quickly with JSON-structured generation you can call instead .json, a .txt API that guarantees valid JSON.</p> <p>Outlines can be deployed as an LLM service using the vLLM inference engine and a FastAPI server. vLLM is not installed by default so will need to install Outlines with:</p> <pre><code>pip install outlines[serve]\n</code></pre> <p>You can then start the server with:</p> <pre><code>python -m outlines.serve.serve --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre> <p>This will by default start a server at <code>http://127.0.0.1:8000</code> (check what the console says, though). Without the <code>--model</code> argument set, the OPT-125M model is used. The <code>--model</code> argument allows you to specify any model of your choosing.</p> <p>To run inference on multiple GPUs you must pass the <code>--tensor-parallel-size</code> argument when initializing the server. For instance, to run inference on 2 GPUs:</p> <pre><code>python -m outlines.serve.serve --model=\"microsoft/Phi-3-mini-4k-instruct\" --tensor-parallel-size 2\n</code></pre>"},{"location":"reference/serve/vllm/#alternative-method-via-docker","title":"Alternative Method: Via Docker","text":"<p>You can install and run the server with Outlines' official Docker image using the command</p> <pre><code>docker run -p 8000:8000 outlinesdev/outlines --model=\"microsoft/Phi-3-mini-4k-instruct\"\n</code></pre>"},{"location":"reference/serve/vllm/#querying-endpoint","title":"Querying Endpoint","text":"<p>You can then query the model in shell by passing a prompt and either</p> <ol> <li>a JSON Schema specification or</li> <li>a Regex pattern</li> </ol> <p>with the <code>schema</code> or <code>regex</code> parameters, respectively, to the <code>/generate</code> endpoint. If both are specified, the schema will be used. If neither is specified, the generated text will be unconstrained.</p> <p>For example, to generate a string that matches the schema <code>{\"type\": \"string\"}</code> (any string):</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"What is the capital of France?\",\n        \"schema\": {\"type\": \"string\", \"maxLength\": 5}\n        }'\n</code></pre> <p>To generate a string that matches the regex <code>(-)?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-][0-9]+)?</code> (a number):</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"What is Pi? Give me the first 15 digits: \",\n        \"regex\": \"(-)?(0|[1-9][0-9]*)(\\\\.[0-9]+)?([eE][+-][0-9]+)?\"\n        }'\n</code></pre> <p>Instead of <code>curl</code>, you can also use the requests library from another python program.</p> <p>Please consult the vLLM documentation for details on additional request parameters. You can also read the code in case you need to customize the solution to your needs.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/roadmap/","title":"Roadmap","text":""}]}